{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLABFOLD STAGE of GAIN-GRN\n",
    "In this stage, we start with the collection of GAIN domain PDBs and their sequences collected in a single large FASTA file.\n",
    "The goal here is to filter only valid GAIN domains, that have a GPS triad (or residues there) and a helical subdomain A, since the N-terminal GAIN boundaries are not annotated in current databases.\n",
    "\n",
    "**The corresponding RAW PDB files are not deposited in the ZENODO repository. Please contact www.researchgate.net/profile/Florian-Seufert-4 for the raw data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "from gaingrn.scripts.gain_classes import GainDomain, FilterCollection\n",
    "import gaingrn.scripts.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Run STRIDE on all PDB files of best RANK (1) in the folded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdbs = glob.glob(\"/home/hildilab/projects/agpcr_nom/*output/**/*_rank_1_*.pdb\")\n",
    "print(f\"Found {len(pdbs)} best ranked models in target directories.\")\n",
    "#print(len(celsr_pdbs))\n",
    "\n",
    "stride_folder = f\"/home/hildilab/projects/agpcr_nom/all_gps_stride\"\n",
    "stride_bin = \"/home/hildilab/lib/stride/stride\"\n",
    "           \n",
    "def compile_stride_mp_list(pdbs, stride_folder,stride_bin):\n",
    "    stride_mp_list = []\n",
    "    \n",
    "    for pdb in pdbs:\n",
    "        pdb_name = pdb.split(\"/\")[-1]\n",
    "        name = pdb_name.split(\"_unrelaxed_\")[0]\n",
    "        out_file = f\"{stride_folder}/{name}.stride\"\n",
    "        arg = [pdb, out_file, stride_bin]\n",
    "        \n",
    "        stride_mp_list.append(arg)\n",
    "        \n",
    "    return stride_mp_list\n",
    "\n",
    "def run_stride(arg):\n",
    "    pdb_file, out_file, stride_bin = arg\n",
    "    stride_command = f\"{stride_bin} {pdb_file} -f{out_file}\"\n",
    "    gaingrn.scripts.io.run_command(stride_command)\n",
    "\n",
    "def execute_stride_mp(stride_mp_list, n_threads=10):\n",
    "        # multiprocessed variant wrapper\n",
    "        stride_pool = mp.Pool(n_threads)\n",
    "        stride_pool.map(run_stride, stride_mp_list)\n",
    "        print(\"Completed mutithreaded creation of STRIDE files!\")\n",
    "\n",
    "stride_mp_list = compile_stride_mp_list(pdbs, stride_folder, stride_bin)\n",
    "print(len(stride_mp_list))\n",
    "# MP execution of STRIDE\n",
    "execute_stride_mp(stride_mp_list, n_threads=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Eliminate doublet entries (we are dealing with multiple folding runs) from the PDBs, by UniProtKB identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate double entries (both in the original run and the added small runs)\n",
    "# Form the \"pdbs\" list\n",
    "\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/all_gps_stride/*\")\n",
    "print(len(stride_files))\n",
    "accessions = [f.split(\".strid\")[0].split(\"/\")[-1].split(\"-\")[0] for f in stride_files]\n",
    "pdb_accessions = np.array([p.split(\"_unrelaxed_\")[0].split(\"/\")[-1].split(\"-\")[0] for p in pdbs])\n",
    "\n",
    "# Find duplicate in the original pdbs list and indicate them via > is_duplicate = True <\n",
    "is_duplicate=np.zeros([len(pdbs)], dtype=bool)\n",
    "sort_pdb_ac = np.sort(pdb_accessions)\n",
    "duplicate_list = []\n",
    "\n",
    "for i, pdb in enumerate(sort_pdb_ac):\n",
    "    if i+1 == len(sort_pdb_ac):\n",
    "        break\n",
    "    if pdb == sort_pdb_ac[i+1]:\n",
    "        duplicate_list.append(pdb)\n",
    "        multi_indices = np.where(pdb == pdb_accessions)[0]\n",
    "        is_duplicate[multi_indices[0]] = True\n",
    "\n",
    "np_pdbs = np.array(pdbs)\n",
    "singlet_pdbs = np_pdbs[is_duplicate == False] # This is the reduced list with ONLY UNIQUE PDBs\n",
    "print(f\"Reduced the initial set of {len(pdbs)} PDB files down to {len(singlet_pdbs)} files.\")\n",
    "\n",
    "# This is a check routine if there are PDBs in the reduced list which have NOT a STRIDE file\n",
    "singlet_pdb_accessions = np.array([p.split(\"_unrelaxed_\")[0].split(\"/\")[-1].split(\"-\")[0] for p in singlet_pdbs])\n",
    "\n",
    "counter = 0\n",
    "for ac in singlet_pdb_accessions:\n",
    "    if ac not in accessions:\n",
    "        print(ac)\n",
    "    else:\n",
    "        counter += 1\n",
    "print(f\"Found {counter}/{len(singlet_pdb_accessions)} accessions in the accession list.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Create a FilterCollection object for assessing all GAIN domain models\n",
    "This is done by transforming the initial MAFFT alignment + quality + FASTA to a collection of GainDomain objects to detect their C- and N-terminal boundaries and check their validity via:\n",
    "- assessing presence of a helical Subdomain A\n",
    "- checking the presence of two C-terminal strands and a loop in between (either in the alignment as GPS triad or alternatively via structure segment search)\n",
    "\n",
    "We write all valid GAIN domain model sequences to a FASTA file. This will be the basis, along with a MAFFT alignment of this, for the template search and the statistical analysis of the GAIN domain dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE THREADED VERSION, this is very slow\n",
    "fasta_file = \"/home/hildilab/projects/GPS_massif/uniprot_query/all_celsr_trunc.fa\"\n",
    "alignment_file = \"/home/hildilab/projects/agpcr_nom/appended_big_mafft.fa\" # This is a combined alignment of ALL sequences in ALL queries!\n",
    "quality_file = \"/home/hildilab/projects/agpcr_nom/appended_big_mafft.jal\"  # ^ corresponding quality file\n",
    "stride_folder = \"/home/hildilab/projects/agpcr_nom/all_gps_stride\" \n",
    "quality = gaingrn.scripts.io.read_quality(quality_file)                              # extract BLOSUM62-based scores from the JAL file\n",
    "gps_minus_one = 21160  # 19258\n",
    "aln_cutoff = 21813 # 19822\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/all_gps_stride/*\")\n",
    "alignment_dict = gaingrn.scripts.io.read_alignment(alignment_file, aln_cutoff)\n",
    "\n",
    "sequences = gaingrn.scripts.io.read_multi_seq(fasta_file)\n",
    "print(len(sequences))\n",
    "print(len(stride_files))\n",
    "\n",
    "filterCollection = FilterCollection(alignment_file,\n",
    "                                   aln_cutoff = 19822,\n",
    "                                   quality = quality,\n",
    "                                   gps_index = gps_minus_one,\n",
    "                                   stride_files = stride_files,\n",
    "                                   sequences = sequences)\n",
    "\n",
    "filterCollection.write_filtered(savename=\"../data/all_valid_gain.fa\", bool_mask = filterCollection.valid_gps, write_mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTIPROCESSING VERSION FOR SPEED UP, we collect individual valid models and cat them into a single file.\n",
    "# FUNCTION BLOCK\n",
    "\n",
    "def batch_filter_seqs(arg_item):\n",
    "                   # [sequences,      # A number of sequences as tuple instances\n",
    "                   #  stride_folder,  # A folder containing ALL stride files\n",
    "                   #   output_prefix,  # A prefix for individual file identification\n",
    "                   #   alignment_file, # The big (initial) alignment file\n",
    "                   #   quality,        # The corr. parsed quality for BLOSUM62 score\n",
    "                   #   aln_cutoff,     # the left-most column (19822 for big_mafft.fa)\n",
    "                   #   gps_minus_one,  # The column index of GPS-1 (zero-indexed! 19258 big_mafft)\n",
    "                   #   ]\n",
    "    sequences, stride_folder, output_prefix, alignment_file, quality, aln_cutoff, gps_minus_one, alignment_dict = arg_item\n",
    "    # Parallelizable version of filtering sequences and models via FilterCollection\n",
    "    # This should create separate files for each valid, fragment and no-SD group\n",
    "    # These files should then be grouped together\n",
    "    # The batch size is arbitrary and is considered the number of sequences passed\n",
    "    \n",
    "    # Output: Profiles; 4 Text files matching individual criteria (valid, fragment, invalidGPS, invalid)\n",
    "    filteredBatch = FilterCollection(alignment_file,\n",
    "                                   aln_cutoff = aln_cutoff,\n",
    "                                   quality = quality,\n",
    "                                   gps_index = gps_minus_one,\n",
    "                                   stride_files = stride_files,\n",
    "                                   sequences = sequences,\n",
    "                                   alignment_dict = alignment_dict)\n",
    "    outpath = \"/home/hildilab/projects/agpcr_nom/all_gps_profiles_001\" # autoproduce the GPS profiles as image here\n",
    "    \n",
    "    for Gain in filteredBatch.collection:\n",
    "        if Gain:\n",
    "            Gain.plot_profile(outdir=outpath, noshow=True)\n",
    "            if Gain.hasSubdomain:\n",
    "                Gain.plot_helicality(coil_weight=0.01, savename=f'{outpath}/{Gain.name}.hel.png', debug=False, noshow=True)\n",
    "        \n",
    "    suffixes = [\"gain\", \n",
    "                \"fragments\", \n",
    "                \"noncons_gps\", \n",
    "                \"invalid\"]\n",
    "    masks = [np.logical_and(filteredBatch.valid_gps, filteredBatch.valid_subdomain),\n",
    "              np.logical_not(filteredBatch.valid_subdomain),\n",
    "              np.logical_not(filteredBatch.valid_gps),\n",
    "              np.logical_not(np.logical_and(filteredBatch.valid_gps, filteredBatch.valid_subdomain))]\n",
    "    # write four separate files, matching each criterion\n",
    "    for k in range(4):\n",
    "        filteredBatch.write_filtered(savename=f\"{outpath}/{output_prefix}_{suffixes[k]}.fa\", \n",
    "                                     bool_mask = masks[k],\n",
    "                                     write_mode = 'w')\n",
    "    del filteredBatch\n",
    "    return None \n",
    "\n",
    "def run_mp_collection(arg_list, n_threads=10):\n",
    "    pool = mp.Pool(n_threads)\n",
    "    pool.map(batch_filter_seqs, arg_list)\n",
    "    print(\"Completed mutithreaded filtering.\")\n",
    "\n",
    "def construct_arg_list(batch_sequence_files, \n",
    "                       output_folder,\n",
    "                       stride_folder, \n",
    "                       quality, \n",
    "                       alignment_file, \n",
    "                       aln_cutoff, \n",
    "                       gps_minus_one,\n",
    "                       alignment_dict = None):\n",
    "    \"\"\" each item looks like this:\n",
    "        sequences, \\ \n",
    "        stride_folder, \\\n",
    "        output_prefix, \\\n",
    "        alignment_file, \\\n",
    "        quality, \\\n",
    "        aln_cutoff, \\\n",
    "        gps_minus_one = arg_item\"\"\"\n",
    "    # static : stride_folder, quality, alignment_file, aln_cutoff, gps_minus_one\n",
    "    # flexible : sequences, output_prefix\n",
    "    arg_list = []\n",
    "    #\n",
    "    for idx, sequence_file in enumerate(batch_sequence_files):\n",
    "        \n",
    "        index_string = str(idx)\n",
    "        sequences = gaingrn.scripts.io.read_multi_seq(sequence_file)\n",
    "        output_prefix = f\"{output_folder}_{index_string.zfill(3)}\"\n",
    "        item = [sequences, \n",
    "                stride_folder, \n",
    "                output_prefix, \n",
    "                alignment_file, \n",
    "                quality, \n",
    "                aln_cutoff, \n",
    "                gps_minus_one,\n",
    "                alignment_dict]\n",
    "        \n",
    "        arg_list.append(item)\n",
    "    \n",
    "    print(f\"[NOTE] : Compiled list of arguments for multithreaded filtering\"\n",
    "          f\" containing {len(arg_list)} items.\")\n",
    "    return arg_list\n",
    "\n",
    "\n",
    "def compile_fastas(prefix, out_prefix):\n",
    "    # Compiles the fasta files together to construct one large file containing the sequences\n",
    "    # satisfying each criterion in the 2x2 matrix\n",
    "    # we want to have the GAIN sequence only that is output by the write_filtered() func.\n",
    "    \n",
    "    # Gather all files:\n",
    "    suffixes = [\"gain\", \n",
    "                \"fragments\", \n",
    "                \"noncons\", \n",
    "                \"invalid\"]\n",
    "    all_files = np.asarray(glob.glob(f\"{prefix}*fa\"))\n",
    "    print(len(all_files))\n",
    "    for suffix in suffixes:\n",
    "        sub_list = sorted([f for f in all_files if suffix in f.split(\"_\")[-1]])\n",
    "        print(f\"Sublist constructed for {suffix = } containing {len(sub_list)} files.\")\n",
    "        with open(f\"{out_prefix}_{suffix}.fa\", \"w\") as all_file:\n",
    "            all_seqs = []\n",
    "            for file in sub_list:\n",
    "                seqs = sse_func.read_multi_seq(file)\n",
    "                for j in seqs:\n",
    "                    if j in all_seqs:\n",
    "                        print(j[0], \"doublet\")\n",
    "                        continue\n",
    "                    all_seqs.append(j)\n",
    "                    all_file.write(f\">{j[0]}\\n{j[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE MULTIPROCESSED VARIANT OF FILTER_COLLECTION\n",
    "\n",
    "batch_sequence_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/*output/batch_*.fa\")\n",
    "print(len(batch_sequence_files))\n",
    "output_folder = \"app_gain_domains_001\"\n",
    "\n",
    "alignment_file = \"/home/hildilab/projects/agpcr_nom/appended_big_mafft.fa\" # This is a combined alignment of ALL sequences in ALL queries!\n",
    "quality_file = \"/home/hildilab/projects/agpcr_nom/appended_big_mafft.jal\"  # ^ corresponding quality file\n",
    "stride_folder = \"/home/hildilab/projects/agpcr_nom/all_gps_stride\" \n",
    "quality = gaingrn.scripts.io.read_quality(quality_file)\n",
    "gps_minus_one = 21160  # 19258\n",
    "aln_cutoff = 21813 # 19822\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/all_gps_stride/*\")\n",
    "alignment_dict = gaingrn.scripts.io.read_alignment(alignment_file, aln_cutoff)\n",
    "print(len(stride_files))\n",
    "print(len(batch_sequence_files))\n",
    "\n",
    "arg_list = construct_arg_list(batch_sequence_files, \n",
    "                       output_folder,\n",
    "                       stride_folder, \n",
    "                       quality, \n",
    "                       alignment_file, \n",
    "                       aln_cutoff, \n",
    "                       gps_minus_one,\n",
    "                       alignment_dict)\n",
    "\n",
    "run_mp_collection(arg_list, n_threads=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_fastas(\"/home/hildilab/projects/agpcr_nom/all_gps_profiles/app_gain_domains\",\n",
    "              out_prefix = \"/home/hildilab/projects/agpcr_nom/app_gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We proceed with the compiled FASTA file containing only VALID GAIN domain sequences and their respective accessions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad450da4e6f5b8cdd3942a14692e4e36ff09ff1e9c8df5cfaea1622af0db4001"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
