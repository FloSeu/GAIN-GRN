{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS FOR BRANCH PKD - poycystins without alignment\n",
    "# DEPENDENCIES\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FixedLocator)\n",
    "#import logomaker\n",
    "# LOCAL IMPORTS\n",
    "from indexing_classes import GPCRDBIndexing\n",
    "from gain_classes import GainDomain, GainCollection, GainDomainNoAln\n",
    "import sse_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_receptor(sequences, selection):\n",
    "    new_list = []\n",
    "    for seq_tup in sequences:\n",
    "        if selection in seq_tup[0]:\n",
    "            new_list.append(seq_tup)\n",
    "    return new_list\n",
    "\n",
    "def filter_by_list(sequences, selection): # selection list\n",
    "    new_list = []\n",
    "    for seq_tup in sequences:\n",
    "        for it in selection:\n",
    "            if it in seq_tup[0]:\n",
    "                new_list.append(seq_tup)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seqs = sse_func.read_multi_seq(\"../data/pkd/all_pkd_gain.fa\")\n",
    "stride_files = glob.glob(\"../data/pkd/pkds_stride/*\")\n",
    "# This only contains the sigma files for truncated (?) PDBs.\n",
    "print(len(stride_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-offset the sequences to match the exact PDB indexing\n",
    "f_seqs = sse_func.read_alignment(\"../all_pkds.fa\")\n",
    "full_seqs = {k.split(\"-\")[0]:v for k,v in f_seqs.items()}\n",
    "print(full_seqs.keys())\n",
    "valid_adj_seqs = []\n",
    "for tup in valid_seqs:\n",
    "    name = tup[0].split(\"-\")[0]\n",
    "    x = sse_func.find_the_start(longseq=full_seqs[name], shortseq=tup[1])\n",
    "    if x == 0:\n",
    "        print(\"already 1st res.\\n\", name, tup[1][:10], tup[1][-10:])\n",
    "        valid_adj_seqs.append( (name,full_seqs[name][:len(tup[1])-1]) )\n",
    "        print(full_seqs[name][:len(tup[1])-1])\n",
    "    else:\n",
    "        valid_adj_seqs.append( (name,full_seqs[name][x-1:x+len(tup[1])-1]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_collection = GainCollection(  \n",
    "                                    stride_files = stride_files,\n",
    "                                    sequence_files=None,\n",
    "                                    sequences=valid_adj_seqs,#valid_seqs,\n",
    "                                    is_truncated = True,\n",
    "                                    coil_weight=0.00, # TESTING\n",
    "                                    #domain_threshold=20, # TESTING\n",
    "                                    stride_outlier_mode=True,\n",
    "                                    no_alignment=True,\n",
    "                                    debug=False)\n",
    "#valid_collection = pd.read_pickle(\"../valid_collection.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(valid_collection, open('../pkd_collection.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_pdb(start, end, oldpdb, newpdb):\n",
    "    with open(oldpdb) as p:\n",
    "        data = p.readlines()\n",
    "    newdata = []\n",
    "    for line in data:\n",
    "        if line.startswith('ATOM'):\n",
    "            resid = int(line[22:26])\n",
    "            if start > resid or end < resid:\n",
    "                continue\n",
    "        newdata.append(line)\n",
    "    with open(newpdb, 'w') as new:\n",
    "        new.write(''.join(newdata))\n",
    "\n",
    "pdbs = glob.glob(\"../pkd_pdbs/*_rank_1_*.pdb\")\n",
    "print(f\"Found {len(pdbs)} best ranked models in target directories.\")\n",
    "valid_ct = 0\n",
    "\n",
    "for gain in valid_collection.collection:\n",
    "    valid_ct +=1\n",
    "    name = gain.name\n",
    "    tar_pdb = [p for p in pdbs if name.split(\"_\")[0] in p][0]\n",
    "    new_pdb_path = f'../trunc_pkd_pdbs/{gain.name.split(\"-\")[0]}.pdb'\n",
    "    truncate_pdb(gain.start,gain.end, tar_pdb, new_pdb_path)\n",
    "print(\"Copied and truncated\", valid_ct, \"GAIN domains.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_collection.plot_sse_hist(title=f\"Polycystins (Total: {len(valid_adj_seqs)})\",\n",
    "                               n_max=26,\n",
    "                               #savename=\"../fig/hists/%s.adj\"%(out_names[i]))\n",
    "                               savename=\"../pkd_data/pkd_sse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    parse_string = \"_\"\n",
    "    print(parse_string)\n",
    "    filtered_sequences = filter_by_receptor(valid_seqs, parse_string)\n",
    "    #if len(filtered_sequences) == 0:\n",
    "    print(f\"Parsed with {parse_string = }: Result : {len(filtered_sequences)} Sequences\")\n",
    "    parsed_collection = GainCollection( alignment_file = alignment_file,\n",
    "                                        aln_cutoff = aln_cutoff,\n",
    "                                        quality = quality,\n",
    "                                        gps_index = gps_minus_one,\n",
    "                                        stride_files = stride_files,\n",
    "                                        sequence_files=None,\n",
    "                                        sequences=filtered_sequences,\n",
    "                                        alignment_dict = alignment_dict,\n",
    "                                        is_truncated = True\n",
    "                                         )\n",
    "    parsed_collection.plot_sse_hist(title=f\"Receptor group: {parse_string} (Total: {len(filtered_sequences)})\",\n",
    "                                   n_max=16,\n",
    "                                   savename=\"hists/%s\"%(str(group)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Parsing out specific Files from the overall dataset based on selection\n",
    "def grab_selection(parse_string, stride_path, pdb_list, sequences, profile_path, target_dir, seqs=None):\n",
    "    # grabs PDB file, stride file, profiles, sequence from FASTA and copies to target dir.\n",
    "    if seqs is None:\n",
    "        sub_seqs = [seq for seq in sequences if parse_string.lower() in seq[0].lower()]\n",
    "    else: sub_seqs = seqs\n",
    "    print(f\"Found {len(sub_seqs)} sequences.\")\n",
    "    strides = glob.glob(stride_path+\"*.stride\")#\n",
    "    profiles = glob.glob(profile_path+\"*.png\")\n",
    "    \n",
    "    sub_strides = []\n",
    "    sub_profiles = []\n",
    "    sub_pdbs = []\n",
    "    \n",
    "    for seq in sub_seqs:\n",
    "        ac = seq[0].split(\"-\")[0]\n",
    "        [sub_profiles.append(prof) for prof in profiles if ac in prof]\n",
    "        [sub_strides.append(stride) for stride in strides if ac in stride]\n",
    "        [sub_pdbs.append(pdb) for pdb in pdb_list if ac in pdb]\n",
    "    \n",
    "    for prof in sub_profiles:\n",
    "        name = prof.split(\"/\")[-1]\n",
    "        copyfile(prof, target_dir+\"profiles/\"+name)\n",
    "    \n",
    "    for stride in sub_strides:\n",
    "        name = stride.split(\"/\")[-1]\n",
    "        copyfile(stride, target_dir+\"strides/\"+name)\n",
    "    \n",
    "    for pdb in sub_pdbs:\n",
    "        name = pdb.split(\"/\")[-1]\n",
    "        copyfile(pdb, target_dir+\"pdbs/\"+name)\n",
    "        \n",
    "    for seq in sub_seqs:\n",
    "        sse_func.write2fasta(seq[1]+\"\\n\", seq[0], target_dir+\"seqs/\"+seq[0]+\".fa\")\n",
    "        \n",
    "    print(f\"Copied {len(sub_pdbs)} PDB files, {len(sub_strides)} STRIDE files,\",\n",
    "          f\" {len(sub_profiles)} Profiles and {len(sub_seqs)} Sequences\",\n",
    "          f\"for Selection {parse_string}\")\n",
    "    \n",
    "root_path = \"/home/hildilab/projects/agpcr_nom/\"\n",
    "profile_path = root_path+\"all_gps_profiles/\"\n",
    "pdb_list = glob.glob(f\"{root_path}all_gps*/batch*/*rank_1_*.pdb\")\n",
    "print(len(pdb_list))\n",
    "#valid_seqs\n",
    "target_dir = root_path+\"human/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"grab_selection(parse_string='HUMAN',\n",
    "              stride_path = root_path+\"all_gps_stride/\",\n",
    "              pdb_list = pdb_list,\n",
    "              sequences = valid_seqs,\n",
    "              profile_path = profile_path,\n",
    "              target_dir = target_dir)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_seqs = [\"Q9HBW9\",\"O60241\",\"Q6QNK2\",\"Q9UHX3\",\"Q5T601\",\"Q96PE1\",\"O60242\",\"Q86SQ4\",\n",
    "                \"O94910\",\"Q8IWK6\",\"Q8IZP9\",\"Q8WXG9\",\"Q86Y34\",\"O95490\",\"Q14246\",\"Q9BY15\",\n",
    "                \"Q8IZF2\",\"Q86SQ3\",\"Q8IZF6\",\"Q96K78\",\"Q8IZF3\",\"Q8IZF7\",\"Q8IZF5\",\"Q7Z7M1\",\n",
    "                \"Q8IZF4\",\"Q9HCU4\",\"Q9NYQ6\",\"Q9NYQ7\",\"Q9HAR2\",\"O14514\",\"P48960\",\n",
    "                \"Q9Y653\"]\n",
    "list_31 = filter_by_list(valid_seqs, human_seqs)\n",
    "sigma_2_strides = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigma_2*/*.stride\")\n",
    "\n",
    "\n",
    "human_collection = GainCollection( alignment_file = alignment_file,\n",
    "                                        aln_cutoff = aln_cutoff,\n",
    "                                        quality = quality,\n",
    "                                        gps_index = gps_minus_one,\n",
    "                                        stride_files =  sigma_2_strides,\n",
    "                                        sequence_files=None,\n",
    "                                        sequences=list_31,\n",
    "                                        alignment_dict = alignment_dict,\n",
    "                                        is_truncated = True,\n",
    "                                        coil_weight = 0.08, # TESTING\n",
    "                                        stride_outlier_mode = True,\n",
    "                                        debug=False\n",
    "                                         )\n",
    "\n",
    "pickle.dump(human_collection, open('../human_collection.pkl', 'wb'))\n",
    "#print(len(human_collection.collection))\n",
    "\"\"\"for gain in human_collection.collection:\n",
    "    #print(gain.name, gain.start, gain.end, gain.sequence, gain.index, gain.subdomain_boundary)\n",
    "    pdb_out = root_path+\"human/trunc_pdbs/\"+gain.name+\"_gain.pdb\"\n",
    "    ac = gain.name.split(\"-\")[0]\n",
    "    found_pdb = [pdb for pdb in pdb_list if ac in pdb]\n",
    "    target_pdb = found_pdb[0]\n",
    "    gain.write_gain_pdb(target_pdb, pdb_out)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plt.plot(human_collection.anchor_hist)\n",
    "human_collection.plot_sse_hist(title=f\"Receptor group: HUMAN_31 (Total: 31)\",\n",
    "                                   n_max=16,\n",
    "                                   savename=f\"hists/human_31.s2_newAnch.{enum}\")\"\"\"\n",
    "def find_pdb(name, pdb_folder):\n",
    "    identifier = name.split(\"-\")[0]\n",
    "    target_pdb = glob.glob(f\"{pdb_folder}/*{identifier}*.pdb\")[0]\n",
    "    return target_pdb\n",
    "l = []\n",
    "for gain in human_collection.collection:\n",
    "    l.append(find_pdb(gain.name, \"../all_pdbs/\"))\n",
    "print(\"pymol\", \" \".join(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy = np.zeros([aln_cutoff],dtype=int)\n",
    "sse_matrix = np.zeros([len(valid_collection.collection), aln_cutoff])\n",
    "print(f\"{occupancy.shape = }\\n{sse_matrix.shape = }\")\n",
    "for i, gain in enumerate(valid_collection.collection):\n",
    "    \n",
    "    #print(gain.sda_helices, gain.sdb_sheets)\n",
    "#    for res_id in range(gain.start,gain.end+1):\n",
    "    occupancy[gain.alignment_indices] += 1\n",
    "    #print(\"_\"*30, \"\\n\",gain.name, \"\\n\",gain.sda_helices,\"\\n\", gain.sdb_sheets)\n",
    "    #print(len(gain.alignment_indices))\n",
    "    for helix in gain.sda_helices:\n",
    "        for res_id in range(helix[0]-1,helix[1]): # Residue 1 means that the index is zero.\n",
    "            #print(gain.alignment_indices[res_id], res_id)\n",
    "            gain.alignment_indices[res_id]\n",
    "            sse_matrix[i, gain.alignment_indices[res_id]] = -1\n",
    "    for sheet in gain.sdb_sheets:\n",
    "        for res_id in range(sheet[0]-1,sheet[1]):\n",
    "            sse_matrix[i, gain.alignment_indices[res_id]] = 1\n",
    "\n",
    "anchors , anchor_occupation = valid_collection.find_anchors(cutoff=3000)\n",
    "anchor_dict = sse_func.make_anchor_dict(anchors, valid_collection.alignment_subdomain_boundary)\n",
    "#dir(human_collection.collection[0])\n",
    "#human_collection.collection[0].create_indexing(anchors, anchor_occupation, anchor_dict)\n",
    "print(anchor_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"all_base\" Indexing and the \"validCollection\" Collection, we can query the data to give us statistics about the anchors and individual SSE composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TSV file with the individual pLDDT values.\n",
    "import re, json\n",
    "\n",
    "jsons = glob.glob('/home/hildilab/projects/agpcr_nom/*_output/batch*/*rank_1*scores.json')\n",
    "\n",
    "identifiers = []\n",
    "with open('all_plddt.tsv', 'w') as c:\n",
    "    c.write('Identifier\\tplddt_values\\n')\n",
    "    for j in jsons:\n",
    "        identifier = re.findall(r'\\/[\\w]+-', j)[0][1:-1]\n",
    "        if identifier in identifiers:\n",
    "            continue\n",
    "        identifiers.append(identifier)\n",
    "        with open(j) as jx:\n",
    "            data = json.load(jx)\n",
    "        c.write(f\"{identifier}\\t{','.join(['{:.2f}'.format(k) for k in data['plddt']])}\\n\")\n",
    "#print(len(identifiers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file where all pLDDT values are contained within, with each identifier\n",
    "import json\n",
    "\n",
    "jsons = glob.glob('/home/hildilab/projects/agpcr_nom/*_output/batch*/*rank_1*scores.json')\n",
    "\n",
    "# construct a dictionary where for each identifier (i.e. H6.49), the total occ. is plottef\n",
    "\n",
    "def construct_identifiers(indexing_dir, center_dir, plddt_values, max_id_dir, name, seq=None):\n",
    "    id_dir = {}\n",
    "    plddts = {}\n",
    "    sse_seq = {}\n",
    "    for sse in indexing_dir.keys():\n",
    "        if sse == 'GPS' :\n",
    "            continue\n",
    "        start = indexing_dir[sse][0]\n",
    "        end = indexing_dir[sse][1]\n",
    "        if end-start > 45:\n",
    "            print(f\"NOTE: SKIPPDING TOO LONG SSE WITH LENGTH {end-start}\\n{name}: {sse}\")\n",
    "            continue\n",
    "        center_res = center_dir[f\"{sse}.50\"]\n",
    "        first_res = 50 - center_res + start\n",
    "        for k in range(end-start+1):\n",
    "            if sse not in max_id_dir.keys():\n",
    "                max_id_dir[sse] = []\n",
    "            if first_res+k not in max_id_dir[sse]:\n",
    "                max_id_dir[sse].append(first_res+k)\n",
    "        id_dir[sse] = [first_res+k for k in range(end-start+1)]\n",
    "        plddts[sse] = [plddt_values[k] for k in range(start, end+1)]\n",
    "        if seq is not None:\n",
    "            sse_seq[sse] = [seq[k] for k in range(start, end+1)]\n",
    "    if seq is None:\n",
    "        sse_seq = None\n",
    "    return max_id_dir, id_dir, plddts, sse_seq\n",
    "\n",
    "def get_plddt_dir(file='all_plddt.tsv'):\n",
    "    plddt_dir = {}\n",
    "    with open(file) as f:\n",
    "        data = [l.strip() for l in f.readlines()[1:]]\n",
    "        for l in data:\n",
    "            i,v  = tuple(l.split(\"\\t\"))\n",
    "            plddt_dir[i] = [float(val) for val in v.split(\",\")]\n",
    "    return plddt_dir\n",
    "\n",
    "def make_id_list(id_dir):\n",
    "    id_list = []\n",
    "    for sse in id_dir.keys():\n",
    "        for res in id_dir[sse]:\n",
    "            id_list.append(f\"{sse}.{res}\")\n",
    "    return id_list #np.array(id_list)\n",
    "\n",
    "def compact_label_positions(id_collection, plddt_collection, sse_keys):\n",
    "    label_plddts = {}\n",
    "    for sse in sse_keys:\n",
    "        label_plddts[sse] = {}\n",
    "\n",
    "    for i in range(len(id_collection)):\n",
    "        gain_positions = id_collection[i]\n",
    "        plddt_positions = plddt_collection[i]\n",
    "        \n",
    "        for sse in gain_positions.keys():\n",
    "            \n",
    "            for j, pos in enumerate(gain_positions[sse]):\n",
    "                pos = int(pos)\n",
    "\n",
    "                if pos not in label_plddts[sse].keys():\n",
    "                    label_plddts[sse][pos] = [plddt_positions[sse][j]]\n",
    "                else:\n",
    "                    label_plddts[sse][pos].append(plddt_positions[sse][j])\n",
    "\n",
    "    return label_plddts\n",
    "\n",
    "def construct_id_occupancy(indexing_dirs, center_dirs, length, plddt_dir, names, seqs):\n",
    "    newkeys = ['H1','H2','H3','H4','H5','H6','H7','H8','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13']\n",
    "    id_collection = []\n",
    "    plddt_collection = []\n",
    "    seq_collection = []\n",
    "    all_id_dir = {x:[] for x in newkeys}\n",
    "    for k in range(length):\n",
    "        identifier = names[k].split(\"-\")[0]\n",
    "        plddt_values = plddt_dir[identifier]\n",
    "        all_id_dir, id_dir, plddts, sse_seq = construct_identifiers(indexing_dirs[k], center_dirs[k], plddt_values, all_id_dir, names[k], seqs[k])\n",
    "        id_collection.append(id_dir)\n",
    "        #print(id_dir)\n",
    "        plddt_collection.append(plddts)\n",
    "        seq_collection.append(sse_seq)\n",
    "    print(\"Completed creating value collection.\")\n",
    "    print(id_collection[0])\n",
    "    print(plddt_collection[0])\n",
    "\n",
    "    # Here, parse through the id_dirs to count the occurrence of positions per SSE\n",
    "    # Dictionary to map any label identifier to a respective position.\n",
    "    id_map = {}\n",
    "    i = 0\n",
    "    for sse in newkeys:\n",
    "        for res in all_id_dir[sse]:\n",
    "            id_map[f'{sse}.{res}'] = i \n",
    "            i += 1\n",
    "    \n",
    "    max_id_list = []\n",
    "    for i, id_dict in enumerate(id_collection):\n",
    "        max_id_list.append(make_id_list(id_dict))\n",
    "    flat_id_list = np.array([item for sublist in max_id_list for item in sublist])\n",
    "    print(\"Finished constructing flat_id_list.\")\n",
    "    labels, occ = np.unique(flat_id_list, return_counts=True)\n",
    "    # Parse through labels, occ to generate the sse-specific data\n",
    "    occ_dict = {labels[u]:occ[u] for u in range(len(labels))}\n",
    "    # Transform occ_dict to the same format as label_plddts (one dict per sse):\n",
    "    label_occ = {}\n",
    "    for sse in newkeys:\n",
    "        label_occ[sse] = {int(k[-2:]):v for k,v in occ_dict.items() if sse in k}\n",
    "    #print(labels, occ)\n",
    "    label_plddts = compact_label_positions(id_collection, plddt_collection, newkeys)\n",
    "    label_seq = compact_label_positions(id_collection, seq_collection, newkeys)\n",
    "    #print(labels)\n",
    "    return label_plddts, label_occ, label_seq\n",
    "    #[print(k, len(v)) for k,v in label_plddts.items()]\n",
    "\n",
    "plddt_dir = get_plddt_dir()\n",
    "#print(list(plddt_dir.keys())[:10])\n",
    "plddt_values, occ_values, label_seq = construct_id_occupancy(all_base.indexing_dirs, all_base.center_dirs, all_base.length, plddt_dir, all_base.names, seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT THE POSITION OCCUPANCY AND THE AVERAGE PLDDT PER POSITION. with plddt_values, occ_values\n",
    "newkeys = ['H1','H2','H3','H4','H5','H6','H7','H8','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13']\n",
    "for sse in newkeys:\n",
    "    # Transform the values first\n",
    "    pp = plddt_values[sse]\n",
    "    #print(occ_values[sse])\n",
    "    av_pp = {k:np.average(np.array(v))/100 for k,v in pp.items()}\n",
    "    #print(av_pp)\n",
    "    norm_occ = {k:v/14435 for k,v in occ_values[sse].items()}\n",
    "    xax = sorted(av_pp.keys())\n",
    "    y_pp = [av_pp[x] for x in xax]\n",
    "    y_occ = [norm_occ[x] for x in xax]\n",
    "    norm_pp = np.array(y_pp)*np.array(y_occ)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[5,2])\n",
    "    fig.set_facecolor('w')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1)) #AutoMinorLocator())\n",
    "    ax.xaxis.set_major_locator(FixedLocator([a for a in range(2,100,3)]))#MultipleLocator(3)))\n",
    "    ax.tick_params(which='both', width=2)\n",
    "    ax.tick_params(which='major', length=8)\n",
    "    ax.tick_params(which='minor', length=6)\n",
    "    plt.bar(xax,y_pp, color='silver', alpha=0.7)\n",
    "    plt.plot(xax, y_occ, color='dodgerblue')\n",
    "    plt.bar(xax, norm_pp, color='xkcd:lightish red', alpha=0.1)\n",
    "    plt.title(f'Element Composition ({sse})')\n",
    "    plt.yticks(ticks = [0, 0.2, 0.4, 0.6, 0.8, 1], labels = ['0%', '20%', '40%', '60%', '80%', '100%'])\n",
    "    #plt.ylabel('')\n",
    "    ax.set_xticklabels([f'{sse}.{str(int(v))}' for v in ax.get_xticks()], rotation=90)\n",
    "    #plt.savefig(f'../fig/{sse}_stats.svg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the occupancy of certain positions:\n",
    "enriched_positions = ['H1.50','H1.54','H1.57','H1.61','H2.56','H2.57','H2.60','H2.61','H3.36','H3.43',\n",
    "'H3.44','H3.51','H3.53','H3.56','H4.38','H4.41','H4.51','H5.37','H5.38','H5.42','H5.44',\n",
    "'H5.48','H5.50','H5.59','H6.42','H6.54','H6.56','H7.40','H7.51','H8.46','H8.58','H8.60',\n",
    "'S1.48','S2.47','S2.51','S2.53','S2.58','S3.53','S3.55','S4.56','S5.48','S5.52','S5.55',\n",
    "'S6.50','S7.45','S7.52','S7.57','S9.53','S10.50','S11.54','S11.55','S12.47','S12.50','S12.52','S13.48','S13.50']\n",
    "for sse in newkeys:\n",
    "    sub_positions = [k for k in enriched_positions if f'{sse}.' in k]\n",
    "    # Transform the values first\n",
    "    #pp = plddt_values[sse]\n",
    "    #print(occ_values[sse])\n",
    "    #av_pp = {k:np.average(np.array(v))/100 for k,v in pp.items()}\n",
    "    #print(av_pp)\n",
    "    norm_occ = {f'{sse}.{k}':v/14435 for k,v in occ_values[sse].items()}\n",
    "    #print(norm_occ)\n",
    "    for k in sub_positions:\n",
    "        print(round(norm_occ[k],2),k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE A FULL DATAFRAME FOR THE LABELED POSITIONS AND THEIR RESPECTIVE AA FREQUENCIES FOR LOGOPLOTS\n",
    "sse_aa_freqs = {}\n",
    "aastr = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "cols = {aa:i for i,aa in enumerate(aastr)}\n",
    "for sse in newkeys:\n",
    "    sse_dict = label_seq[sse]\n",
    "    aafreqs = np.zeros(shape=(len(sse_dict.keys()), 21))\n",
    "    for p_index, pos in enumerate(sorted(sse_dict.keys())):\n",
    "        aas, freq = np.unique(np.array(sse_dict[pos]), return_counts=True)\n",
    "        for i, aa in enumerate(aas):\n",
    "            aafreqs[p_index, cols[aa]] = freq[i]/14435\n",
    "    sse_aa_freqs[sse] = aafreqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the sequence composition for each cancer-enriched-position\n",
    "\n",
    "for sse in newkeys:\n",
    "    sub_positions = [k for k in enriched_positions if f'{sse}.' in k]\n",
    "    lframe = pd.DataFrame(data=sse_aa_freqs[sse], columns=[c for c in aastr], index = sorted(plddt_values[sse].keys()))\n",
    "    #print(lframe)\n",
    "    for pos in sub_positions:\n",
    "        idx = int(pos[-2:]) # get the row number in the SSE\n",
    "        res_data = lframe.loc[[idx]]\n",
    "        total_freqs = res_data.sum(axis=1).to_list()[0]\n",
    "        #print(f\"{total_freqs = }, {type(total_freqs)}\")\n",
    "\n",
    "        norm_freq_dict = {round(freq.to_list()[0]/total_freqs, 5) : aa for aa, freq in res_data.items()}\n",
    "        sorted_norm_freqs = sorted(norm_freq_dict.keys())[::-1]\n",
    "        #print(pos)\n",
    "        xstring = ''\n",
    "        for k in sorted_norm_freqs[:3]: \n",
    "            xstring = xstring +f'{norm_freq_dict[k]}:{round(k*100)}%,'\n",
    "        print(pos, xstring)\n",
    "\n",
    "            # normalize frequency with the total sum\n",
    "\n",
    "\"\"\"for sse in newkeys:\n",
    "    for pos in enriched_positions:\n",
    "        datarow = center_res[pos]\n",
    "        occ, residues = parse_conservation(datarow, all_base.length)\n",
    "        print(f\"{sse}\\t{occ}%\\t{residues}\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGOPLOTS FOR THE ELEMENTS\n",
    "\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "for sse in newkeys:\n",
    "\n",
    "    lframe = pd.DataFrame(data=sse_aa_freqs[sse], columns=[c for c in aastr], index = sorted(plddt_values[sse].keys()))\n",
    "\n",
    "    # Note down the first and last row where the occupation threshold is met.\n",
    "    firstval = None\n",
    "    for i, r in lframe.iterrows():\n",
    "        if np.sum(r) > 0.05: \n",
    "            if firstval is None:\n",
    "                firstval = i\n",
    "            lastval = i\n",
    "    print(firstval, lastval)\n",
    "    subframe = lframe.truncate(before=firstval, after=lastval)\n",
    "    #x_offset = sorted(plddt_values[sse].keys())[0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[5,2])\n",
    "    cons_logo = logomaker.Logo(subframe,\n",
    "                                ax=ax,\n",
    "                                color_scheme='chemistry',\n",
    "                                show_spines=False,\n",
    "                                font_name='FreeSans')\n",
    "\n",
    "    fig.set_facecolor('w')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1)) #AutoMinorLocator())\n",
    "    ax.xaxis.set_major_locator(FixedLocator([a for a in range(2,100,3)]))#MultipleLocator(3))\n",
    "    ax.tick_params(which='both', width=2)\n",
    "    ax.tick_params(which='major', length=8)\n",
    "    ax.tick_params(which='minor', length=6)\n",
    "    ax.set_xticklabels([f'{sse}.{str(int(v))}' for v in ax.get_xticks()], rotation=90)\n",
    "    cons_logo.draw()\n",
    "    fig.tight_layout()\n",
    "    fig.set_facecolor('w')\n",
    "    #plt.savefig(f\"../fig/conslogo_{sse}.svg\", bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gain in enumerate(human_collection.collection):\n",
    "    #if \"Q6QNK2\" in gain.name:\n",
    "        x1, x2,_,_ = gain.create_indexing(anchors, anchor_occupation, anchor_dict, \n",
    "                    outdir = \"/home/hildilab/projects/agpcr_nom/human_31/indexing_files_s2_dsp\",\n",
    "                    #offset = fasta_offsets[i]-gain.start+1,\n",
    "                    split_mode='double')\n",
    "        \n",
    "        print(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A CELL FOR SPECIFICALLY EVALUATING INDIVIDUAL GAIN DOMAINS.\n",
    "# MOSTLY FOR DEBUGGING OR CHECKING OUT ANOMALITIES\n",
    "#[('G1SHT5-G1SHT5_RABIT-AGRG6-Oryctolagus_cuniculus', 'REANEVASEILNLTADGQNLTSANITSIVEQVKRIVNKEENIDVTLGSTLMNIFSNILSNSDSDLLESSSEALKTIDELAFKIDLNSTPHVNIATRNLALGVSSVSPGTNVISNFSIGLPSNNESYFQMDFESGQVDPLASVILPPNLLENLSQEDSILVRRAQFTFFNKTGLFQDVGPQRKTLVSYVMACSIGNITIQDLKDPVQIKIKHTRTQEVHHPICAFWDLNKNKSFGGWNTSGCIAHRDSDASETICLCNHFTHFGVLMD')\n",
    "# ('G1SQ79-G1SQ79_RABIT-AGRG4-Oryctolagus_cuniculus', 'LQGLPDKILDLANITVSDENANDVAEHILNLLNESPPLDEEETKIIVSKVSDISLCEKISMNLTQLMLQIINAVLEKQNDSASGLHEVSNEILRLIERAGHKMEFWGRTANLMVARLALAMLRVDHKFEGVTFSIRSYTEGTDPEIYLGDVPAGKVLASIYLPKSLKKRLRVNNLQTILFNFFGQTSLFKVKNVSKALTTYVVSASISDLSIQNLADPVVITLQHVEGSQKYDQVHCAFWDFEKNNGLGGWNSSGCKVKETNVNYTICQCDHLTHFGVLMDL')\n",
    "# ('G1T5U9-G1T5U9_RABIT-AGRL2-Oryctolagus_cuniculus'\n",
    "tar = [\"A0A151X191\"]\n",
    "tar_seqs = filter_by_list(valid_seqs, tar)\n",
    "print(tar_seqs)\n",
    "Xalignment_file = \"/home/hildilab/projects/agpcr_nom/big_mafft14792.fa\" # This is a combined alignment of ALL sequences in ALL queries!\n",
    "Xquality_file = \"/home/hildilab/projects/agpcr_nom/big_mafft14792.jal\"  # ^ corresponding quality file\n",
    "Xquality = sse_func.read_quality(Xquality_file)\n",
    "Xgps_minus_one = 19258 #appended_big_mafft: 21160  #big_mafft14792: 19258\n",
    "Xaln_cutoff = 19822 #appended_big_mafft: 21813 #big_mafft14792: 19822\n",
    "Xstride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/all_gps_stride/*\")\n",
    "Xalignment_dict = sse_func.read_alignment(alignment_file, aln_cutoff)\n",
    "Xname, Xseq = tar_seqs[0]\n",
    "tar_stride = \"/home/hildilab/projects/agpcr_nom/all_gps_stride/A0A369RSM9-A0A369RSM9_9METZ-AGRL3-Trichoplax_sp._H2.stride\"\n",
    "tar_gain = GainDomain(alignment_file = Xalignment_file,\n",
    "                                        aln_cutoff = Xaln_cutoff,\n",
    "                                        quality = Xquality,\n",
    "                                        gps_index = Xgps_minus_one,\n",
    "                                        name=Xname,\n",
    "                                        #stride_files = Xstride_files,\n",
    "                                        sequence = Xseq,\n",
    "                                        alignment_dict = Xalignment_dict,\n",
    "                                        explicit_stride_file=tar_stride,\n",
    "                                        is_truncated = True,\n",
    "                                        coil_weight = 0.01, # TESTING\n",
    "                                        stride_outlier_mode = True,\n",
    "                                        without_anchors=True)\n",
    "#print(tar_gain.isValid)\n",
    "#if tar_gain.isValid: \n",
    "#    tar_gain.plot_helicality(savename=f\"{tar_gain.name.split('-')[0]}_helicality.full.svg\", debug=True)\n",
    "#    tar_gain.plot_profile()\n",
    "#print(tar_gain.subdomain_boundary, tar_gain.start, tar_gain.end)\n",
    "    #print(gain.name, gain.start, gain.end, gain.sequence, gain.index, gain.subdomain_boundary)\n",
    "    #pdb_out = root_path+\"human_32/trunc_pdbs/\"+gain.name+\"_gain.pdb\"\n",
    "    #ac = gain.name.split(\"-\")[0]\n",
    "    #found_pdb = [pdb for pdb in pdb_list if ac in pdb]\n",
    "    #target_pdb = found_pdb[0]\n",
    "    #gain.write_gain_pdb(target_pdb, pdb_out) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln_col = []\n",
    "\n",
    "for idx, idx_dir in enumerate(all_base.indexing_dirs):\n",
    "    try:\n",
    "        l = idx_dir['H6']\n",
    "    except:\n",
    "        continue\n",
    "    print(l)\n",
    "    \n",
    "    for i in range(l[0], l[1]+1):\n",
    "        if valid_collection.collection[idx].sequence[i] == 'W':\n",
    "            aln_col.append(valid_collection.collection[idx].alignment_indices[i])\n",
    "            print(valid_collection.collection[idx].sequence[i],valid_collection.collection[idx].alignment_indices[i])\n",
    "    \n",
    "print(len(aln_col), np.unique(np.array(aln_col), return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATISTICS FOR ELEMENT-CONNECTING LOOPS\n",
    "[print(v, len(k), round(np.average(k),2), round(np.std(k),2)) for v,k in loop_lengths.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loop_stats(indexing_dir, sequence):\n",
    "    # Returns a named dict with loop lengths, i.e. {\"H1-H2\":13, \"H8-S1\":12}\n",
    "    inverted_dir = {sse[0] : (sse[1],ki) for ki, sse in indexing_dir.items() if \"GPS\" not in ki} # The begin of each sse is here {0:(13, \"H2\")}\n",
    "    loop_loc = {}\n",
    "    loop_dir = {}\n",
    "    ordered_starts = sorted(inverted_dir.keys())\n",
    "    for i, sse_start in enumerate(ordered_starts):\n",
    "        if i == 0: \n",
    "            continue # Skip the first and go from the second SSE onwards, looking in N-terminal direction.\n",
    "        c_label = inverted_dir[sse_start][1]\n",
    "        n_end, n_label = inverted_dir[ordered_starts[i-1]]\n",
    "        loop_loc[f\"{n_label}-{c_label}\"] = (n_end, sse_start-1)\n",
    "        loop_dir[f\"{n_label}-{c_label}\"] = sequence[n_end+1:sse_start] # The one-letter-coded seqeuence. Will be a list of lists\n",
    "    return loop_loc, loop_dir\n",
    "\n",
    "loop_lengths = {}\n",
    "loop_seqs = {}\n",
    "loop_seq = {}\n",
    "\n",
    "loop_info = {}\n",
    "#[loop_info[loop] = {} for loop in loop_seqs.keys()] # into each of these keys, any entry is composed of \"name\":$name, \"sequence\":$seq\n",
    "\n",
    "for idx in range(all_base.length):\n",
    "    curr_name = all_base.names[idx]\n",
    "    start = valid_collection.collection[idx].start\n",
    "    i_loc, i_dir = get_loop_stats(all_base.indexing_dirs[idx], valid_collection.collection[idx].sequence)\n",
    "    for k, seq in i_dir.items():\n",
    "        if k not in loop_info.keys():\n",
    "            loop_info[k] = []\n",
    "        loop_info[k].append({'name':f'{all_base.names[idx]}_{i_loc[k][0]+start}-{i_loc[k][1]+start}', 'sequence':''.join(seq)})\n",
    "    #print(i_len)\n",
    "    #loop_lengths = match_dirs(i_len, loop_lengths)\n",
    "    #loop_seqs = match_dirs(i_dir, loop_seqs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the collected loop sequences to a FASTA file for later alignment.\n",
    "def loop2fasta(outfile, itemlist):\n",
    "    with open(outfile, 'w') as out:\n",
    "        for subdict in itemlist:\n",
    "            out.write(f\">{subdict['name']}\\n{subdict['sequence']}\\n\")\n",
    "    print(\"Done with\", outfile)\n",
    "\n",
    "for loop in loop_info.keys():\n",
    "    loop2fasta(f\"../loops/{loop}.fa\", loop_info[loop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all structures containing 7 Helices in Subdomain A.\n",
    "\n",
    "mystring = ''\n",
    "no=0\n",
    "for gain in valid_collection.collection:\n",
    "        try:sse_labl, _, _, _ = gain.create_indexing(silent=True,anchors=anchors, anchor_occupation=anchor_occupation, anchor_dict=anchor_dict, debug=False)\n",
    "        except:no+=1;continue\n",
    "        kk = len([k for k in sse_labl.keys() if \"H\" in k])\n",
    "        if kk >= 7: mystring += gain.name+\"\\n\"#+str(kk)+\"\\n\"\n",
    "print(no)\n",
    "print(mystring)\n",
    "with open(\"7helix.txt\", 'w') as seven:\n",
    "        seven.write(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mystring.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all structures containing 14+ Strands in Subdomain A.\n",
    "\n",
    "mystring = ''\n",
    "no=0\n",
    "num_sheets = np.zeros(shape=(14432))\n",
    "total_strand = np.zeros(shape=(14432))\n",
    "for i, gain in enumerate(valid_collection.collection):\n",
    "        #try:sse_labl, _, _, _ = gain.create_indexing(silent=True,anchors=anchors, anchor_occupation=anchor_occupation, anchor_dict=anchor_dict, debug=False)\n",
    "        #except:no+=1;continue\n",
    "        num_sheets[i] = len(gain.sdb_sheets)\n",
    "        if len(gain.sdb_sheets) > 14 and len(gain.sdb_sheets) < 17:\n",
    "                mystring += gain.name+\"\\n\"#+str(kk)+\"\\n\"\n",
    "\n",
    "        total_strand[i] = np.sum([strand[1]-strand[0] for strand in gain.sdb_sheets])\n",
    "        #k = len([k for k in sse_labl.keys() if \"S\" in k])\n",
    "        #if kk >= 13: mystring += gain.name+\"\\n\"#+str(kk)+\"\\n\"\n",
    "print(no)\n",
    "print(np.unique(num_sheets, return_counts=True))\n",
    "\n",
    "#print(np.unique(total_strand, return_counts=True, return_index=True))\n",
    "with open(\"14plussheet.txt\", 'w') as seven:\n",
    "        seven.write(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extents = {}\n",
    "for gain in valid_collection.collection:\n",
    "    extents[gain.name.split(\"-\")[0]] = [str(gain.start+1), str(gain.subdomain_boundary+1), str(gain.end+1)] # make it compatible with ONE-indexed PDBs.\n",
    "\n",
    "import json\n",
    "with open('domain_extents.json', 'w') as j:\n",
    "    dump = json.dumps(extents)\n",
    "    j.write(dump)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  },
  "vscode": {
   "interpreter": {
    "hash": "efcc3436bf700bf51081b251413b556e30c22be82f452601745119c8a669a2f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
