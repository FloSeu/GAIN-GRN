{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Assign GAIN-GRN\n",
    "#### This is a notebook to construct the complete GAIN GRN indexing on a collection of GAIN domains.\n",
    "\n",
    "The completed indexing objects are present in the ../data folder as stal_indexing.pkl, human_indexing.pkl and pkd_indexing.pkl, respectively.\n",
    "\n",
    "Requirements:\n",
    "> - GESAMT binary\n",
    "> - STRIDE set of files (one for each entry in the dataset, here we use float-modified STRIDE files for the outliers)\n",
    "> - A Folder of template PDBs\n",
    "> - template_data.json with all information about the template elements and centers\n",
    "\n",
    "The main challenge in constructing a good indexing on each GAIN-Domain is the detection of the start and end of each segment. Often, segments are continuously indexed as Helix or Strand, despite it being two actual segments (i.e. a kink between Helix 4,5 and 6, but all residues are in a single helical segment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logomaker\n",
    "import pickle \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FixedLocator)\n",
    "\n",
    "# LOCAL IMPORTS\n",
    "import gaingrn.scripts.io\n",
    "import gaingrn.scripts.assign\n",
    "import gaingrn.scripts.alignment_utils\n",
    "import gaingrn.scripts.bb_angle_tools\n",
    "import gaingrn.scripts.indexing_utils\n",
    "from gaingrn.scripts.indexing_classes import StAlIndexing\n",
    "from gaingrn.scripts.gain_classes import GainDomainNoAln\n",
    "\n",
    "\n",
    "try: \n",
    "    GESAMT_BIN = os.environ.get('GESAMT_BIN')\n",
    "except:\n",
    "    GESAMT_BIN = \"/home/hildilab/lib/xtal/ccp4-8.0/ccp4-8.0/bin/gesamt\"\n",
    "if GESAMT_BIN is None:\n",
    "    GESAMT_BIN = \"/home/hildilab/lib/xtal/ccp4-8.0/ccp4-8.0/bin/gesamt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle the issue of a very broad assignment of some elements, and to help with splitting them into their respective elements, there is a two-fold modification of STRIDE files in place:\n",
    "\n",
    "1. Every residue outside of 2 Sigmas of the mean (keep in mind, this is circular statistics) gets assigned a lower-case letter as the SSE descriptor, enabling resolving element ambiguities\n",
    "2. The multiple of sigmas for outliers is written into columns 66-70 of the stride file. If this exceeds a defines threshold (usually 5.0), the element is truncated here always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is already done in your set, just for further reference.\n",
    "\n",
    "# import gaingrn.scripts.bb_angle_tools\n",
    "# gaingrn.scripts.bb_angle_tools.stride_file_processing(stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2/*\"), outfolder = \"../data/gain_strides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the GainCollection objects to be indexed. Here, we have the whole 14435 structure set (valid_collection) and the 31 structure set (human_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_collection = pd.read_pickle(\"../data/valid_collection.pkl\")\n",
    "human_collection = pd.read_pickle(\"../data/human_collection.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For testing, in this cell an individual indexing can be constructed. \n",
    "\n",
    "We have implemented a heirarchy of \"split\" modes, which will disambiguate continuous SSE where multiple segment centers are contained --> **split_modes**\n",
    "\n",
    "Setting _debug=True_ will result in a large amount of information being printed, enabling the tracing of errors and irregularities during the assignment process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a Uniprot identifier here.\n",
    "uniprot = \"Q8IZF6\"\n",
    "\n",
    "for i, gain in enumerate(human_collection.collection):\n",
    "    if unprot not in gain.name: \n",
    "        continue\n",
    "    file_prefix = f\"../test_stal_indexing/f3_test{gain.name}\" # a temp folder where calculations and outputs will be stored.\n",
    "    print(\"_\"*30, f\"\\n{i} {gain.name}\")\n",
    "    element_intervals, element_centers, residue_labels, unindexed_elements, params = gaingrn.scripts.assign.assign_indexing(gain, \n",
    "                                                                                                file_prefix=file_prefix, \n",
    "                                                                                                gain_pdb=gaingrn.scripts.io.find_pdb(gain.name, '../../all_pdbs'), \n",
    "                                                                                                template_dir='../data/template_pdbs/',\n",
    "                                                                                                template_json='../data/template_data.json',\n",
    "                                                                                                outlier_cutoff=5.0,\n",
    "                                                                                                gesamt_bin=GESAMT_BIN,\n",
    "                                                                                                debug=False, # If you want ALL that is happening\n",
    "                                                                                                create_pdb=False,\n",
    "                                                                                                hard_cut={\"S2\":7,\"S7\":3,\"H5\":3},\n",
    "                                                                                                patch_gps=True\n",
    "                                                                                                )\n",
    "    # This dictionary denotes the priority line of splitting. Only the lowest-heirarchy split is indicated, i.e. the higher the number, the \"worse\" the split.\n",
    "    split_modes = {\n",
    "        0:\"No Split.\",\n",
    "        1:\"Split by coiled residue.\",\n",
    "        2:\"Split by disordered residue.\",\n",
    "        3:\"Split by Proline/Glycine\",\n",
    "        4:\"Split by hard cut.\",\n",
    "        5:\"Overwrite by anchor priority.\"\n",
    "    }\n",
    "    print(gain.name, gain.subdomain_boundary)\n",
    "    if params[\"split_mode\"] > 0:\n",
    "        print(params[\"split_mode\"], split_modes[params[\"split_mode\"]])\n",
    "    #print(element_intervals, element_centers, residue_labels, unindexed_elements, sep=\"\\n\")\n",
    "    print(unindexed_elements, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, the full __StAlIndexing__ may be constructed test-wise, or by default the pickle of the Indexing is loaded. \n",
    "Keep in mind that within this jupyter notebook - due to its handling of multiprocessig.Pool - the number of threads is limited to 1 and this takes a while for the full set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessions = [gain.name.split(\"-\")[0].split(\"_\")[0] for gain in valid_collection.collection]\n",
    "sequences = [\"\".join(gain.sequence) for gain in valid_collection.collection]\n",
    "\n",
    "fasta_offsets = gaingrn.scripts.alignment_utils.find_offsets(\"../data/all_query_sequences.fasta\",\n",
    "                                 accessions, \n",
    "                                 sequences)\n",
    "\n",
    "# Pseudocenter cases: cases, where the segment center is NOT part of the segment in question. Therefore, an alternative indexing method is applied,\n",
    "# using a \"pseudocenter\", a residue which matches the segment, but is not the .50 residue.\n",
    "ps_file = \"../data/pseudocenters.csv\"\n",
    "open(ps_file,\"w\").write(f\"GAIN,res,elem\\n\")\n",
    "\n",
    "# Careful when running, this takes a lot of time to calculate on single-thread. use run_indexing.py for fast multithreaded calculation.\n",
    "stal_indexing = StAlIndexing(valid_collection.collection, \n",
    "                             prefix=\"../test_stal_indexing/test20\", \n",
    "                             pdb_dir='../../all_pdbs/',\n",
    "                             template_json='../data/template_data.json',\n",
    "                             gesamt_bin=GESAMT_BIN, \n",
    "                             template_dir='../data/template_pdbs/', \n",
    "                             fasta_offsets=fasta_offsets,\n",
    "                             n_threads=1,\n",
    "                             #pseudocenters=ps_file,\n",
    "                             debug=False)\n",
    "#with open(\"../data/stal_indexing.pkl\",\"wb\") as save:\n",
    "#    pickle.dump(stal_indexing, save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a file contaning all segment starts, ends and center residues for each GAIN domain. This is the basis for the GPCRdb implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accession': 0, 'Name': 1, 'Species': 2, 'type': 3, 'GPS-2': 4, 'GPS-1': 5, 'GPS+1': 6, 'H1.start': 7, 'H1.anchor': 8, 'H1.end': 9, 'H2.start': 10, 'H2.anchor': 11, 'H2.end': 12, 'H3.start': 13, 'H3.anchor': 14, 'H3.end': 15, 'H4.start': 16, 'H4.anchor': 17, 'H4.end': 18, 'H5.start': 19, 'H5.anchor': 20, 'H5.end': 21, 'H6.start': 22, 'H6.anchor': 23, 'H6.end': 24, 'S1.start': 25, 'S1.anchor': 26, 'S1.end': 27, 'S2.start': 28, 'S2.anchor': 29, 'S2.end': 30, 'S3.start': 31, 'S3.anchor': 32, 'S3.end': 33, 'S4.start': 34, 'S4.anchor': 35, 'S4.end': 36, 'S5.start': 37, 'S5.anchor': 38, 'S5.end': 39, 'S6.start': 40, 'S6.anchor': 41, 'S6.end': 42, 'S7.start': 43, 'S7.anchor': 44, 'S7.end': 45, 'S8.start': 46, 'S8.anchor': 47, 'S8.end': 48, 'S9.start': 49, 'S9.anchor': 50, 'S9.end': 51, 'S10.start': 52, 'S10.anchor': 53, 'S10.end': 54, 'S11.start': 55, 'S11.anchor': 56, 'S11.end': 57, 'S12.start': 58, 'S12.anchor': 59, 'S12.end': 60, 'S13.start': 61, 'S13.anchor': 62, 'S13.end': 63, 'S14.start': 64, 'S14.anchor': 65, 'S14.end': 66}\n",
      "Completed file ../data/gaingrn_indexing.csv .\n",
      "[DEBUG] construct_data_matrix: Found the following unique headers:\n",
      "\tindividual_headers = array(['H1.D1', 'H1.E1', 'H1.F4'], dtype='<U5')\n",
      "{'Accession': 0, 'Name': 1, 'Species': 2, 'type': 3, 'GPS-2': 4, 'GPS-1': 5, 'GPS+1': 6, 'H1.start': 7, 'H1.anchor': 8, 'H1.end': 9, 'H2.start': 10, 'H2.anchor': 11, 'H2.end': 12, 'H3.start': 13, 'H3.anchor': 14, 'H3.end': 15, 'H4.start': 16, 'H4.anchor': 17, 'H4.end': 18, 'H5.start': 19, 'H5.anchor': 20, 'H5.end': 21, 'H6.start': 22, 'H6.anchor': 23, 'H6.end': 24, 'S1.start': 25, 'S1.anchor': 26, 'S1.end': 27, 'S2.start': 28, 'S2.anchor': 29, 'S2.end': 30, 'S3.start': 31, 'S3.anchor': 32, 'S3.end': 33, 'S4.start': 34, 'S4.anchor': 35, 'S4.end': 36, 'S5.start': 37, 'S5.anchor': 38, 'S5.end': 39, 'S6.start': 40, 'S6.anchor': 41, 'S6.end': 42, 'S7.start': 43, 'S7.anchor': 44, 'S7.end': 45, 'S8.start': 46, 'S8.anchor': 47, 'S8.end': 48, 'S9.start': 49, 'S9.anchor': 50, 'S9.end': 51, 'S10.start': 52, 'S10.anchor': 53, 'S10.end': 54, 'S11.start': 55, 'S11.anchor': 56, 'S11.end': 57, 'S12.start': 58, 'S12.anchor': 59, 'S12.end': 60, 'S13.start': 61, 'S13.anchor': 62, 'S13.end': 63, 'S14.start': 64, 'S14.anchor': 65, 'S14.end': 66, 'H1.D1.start': 67, 'H1.D1.anchor': 68, 'H1.D1.end': 69, 'H1.E1.start': 70, 'H1.E1.anchor': 71, 'H1.E1.end': 72, 'H1.F4.start': 73, 'H1.F4.anchor': 74, 'H1.F4.end': 75}\n",
      "Completed file ../data/gaingrn_indexing.unique.csv .\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-calculated STAL indexing, which saves some time.\n",
    "stal_indexing = pd.read_pickle(\"../data/stal_indexing.pkl\")\n",
    "\n",
    "header, matrix = stal_indexing.construct_data_matrix(unique_sse=False)\n",
    "stal_indexing.data2csv(header, matrix, \"../data/gaingrn_indexing.csv\")\n",
    "header, matrix = stal_indexing.construct_data_matrix(unique_sse=True)\n",
    "stal_indexing.data2csv(header, matrix, \"../data/gaingrn_indexing.unique.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we construct the indexing for the human set with the modified STRIDE files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_collection = pd.read_pickle(\"../data/human_collection.pkl\")\n",
    "\n",
    "human_accessions = [gain.name.split(\"-\")[0].split(\"_\")[0] for gain in human_collection.collection]\n",
    "human_sequences = [\"\".join(gain.sequence) for gain in human_collection.collection]\n",
    "\n",
    "human_fasta_offsets = gaingrn.scripts.alignment_utils.find_offsets(\"../data/all_query_sequences.fasta\", \n",
    "                                 human_accessions, \n",
    "                                 human_sequences)\n",
    "\n",
    "for i, gain in enumerate(human_collection.collection):\n",
    "    element_intervals, element_centers, residue_labels, unindexed_elements, params = gaingrn.scripts.assign.assign_indexing(gain, \n",
    "                                                                                                file_prefix=f\"../test_stal_indexing/human_{i}_{gain.name.split('-')[0]}\", \n",
    "                                                                                                gain_pdb=gaingrn.scripts.io.find_pdb(gain.name, '../../all_pdbs'), \n",
    "                                                                                                template_dir='../data/template_pdbs/',\n",
    "                                                                                                gesamt_bin=GESAMT_BIN,\n",
    "                                                                                                debug=True, \n",
    "                                                                                                create_pdb=True,\n",
    "                                                                                                template_json='../data/template_data.json',\n",
    "                                                                                                hard_cut={\"S2\":7,\"S6\":3,\"H5\":3},\n",
    "                                                                                                patch_gps=True\n",
    "                                                                                                )\n",
    "stal_human_indexing = stal_indexing = StAlIndexing(human_collection.collection, \n",
    "                             prefix=\"../test_stal_indexing/test\", \n",
    "                             pdb_dir='../all_pdbs/',  \n",
    "                             template_dir='../r4_template_pdbs/', \n",
    "                             template_json = 'template_data.json',\n",
    "                             outlier_cutoff=5.0,\n",
    "                             fasta_offsets=human_fasta_offsets,\n",
    "                             gesamt_bin=GESAMT_BIN,\n",
    "                             n_threads=1,\n",
    "                             debug=True)\n",
    "\n",
    "header, matrix = stal_human_indexing.construct_data_matrix(overwrite_gps=True, unique_sse=False)\n",
    "stal_human_indexing.data2csv(header, matrix, \"../data/human_indexing.csv\")\n",
    "\n",
    "# Also include unique Helices in a separate file.\n",
    "header, matrix = stal_human_indexing.construct_data_matrix(overwrite_gps=True, unique_sse=True)\n",
    "stal_human_indexing.data2csv(header, matrix, \"..data/human_indexing.unique.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Indexing for the whole set is best constructed via multithreaded exection by _stal\\_indexing.py_ and saved in a pickle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
