{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Assign GAIN-GRN\n",
    "#### This is a notebook to construct the complete GAIN GRN indexing on a collection of GAIN domains.\n",
    "\n",
    "The completed indexing objects are present in the ../data folder as stal_indexing.pkl, human_indexing.pkl and pkd_indexing.pkl, respectively.\n",
    "\n",
    "Requirements:\n",
    "> - GESAMT binary\n",
    "> - STRIDE set of files (one for each entry in the dataset, here we use float-modified STRIDE files for the outliers)\n",
    "> - A Folder of template PDBs\n",
    "> - template_data.json with all information about the template elements and centers\n",
    "\n",
    "The main challenge in constructing a good indexing on each GAIN-Domain is the detection of the start and end of each segment. Often, segments are continuously indexed as Helix or Strand, despite it being two actual segments (i.e. a kink between Helix 4,5 and 6, but all residues are in a single helical segment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logomaker\n",
    "import pickle \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FixedLocator)\n",
    "\n",
    "# LOCAL IMPORTS\n",
    "import gaingrn.scripts.io\n",
    "import gaingrn.scripts.assign\n",
    "import gaingrn.scripts.alignment_utils\n",
    "import gaingrn.scripts.bb_angle_tools\n",
    "import gaingrn.scripts.indexing_utils\n",
    "from gaingrn.scripts.indexing_classes import StAlIndexing\n",
    "from gaingrn.scripts.gain_classes import GainDomainNoAln\n",
    "\n",
    "\n",
    "try: \n",
    "    GESAMT_BIN = os.environ.get('GESAMT_BIN')\n",
    "except:\n",
    "    GESAMT_BIN = \"/home/hildilab/lib/xtal/ccp4-8.0/ccp4-8.0/bin/gesamt\"\n",
    "if GESAMT_BIN is None:\n",
    "    GESAMT_BIN = \"/home/hildilab/lib/xtal/ccp4-8.0/ccp4-8.0/bin/gesamt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle the issue of a very broad assignment of some elements, and to help with splitting them into their respective elements, there is a two-fold modification of STRIDE files in place:\n",
    "\n",
    "1. Every residue outside of 2 Sigmas of the mean (keep in mind, this is circular statistics) gets assigned a lower-case letter as the SSE descriptor, enabling resolving element ambiguities\n",
    "2. The multiple of sigmas for outliers is written into columns 66-70 of the stride file. If this exceeds a defines threshold (usually 5.0), the element is truncated here always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is already done in your set, just for further reference.\n",
    "\n",
    "# import gaingrn.scripts.bb_angle_tools\n",
    "# gaingrn.scripts.bb_angle_tools.stride_file_processing(stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2/*\"), outfolder = \"../data/gain_strides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the GainCollection objects to be indexed. Here, we have the whole 14435 structure set (valid_collection) and the 31 structure set (human_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_collection = pd.read_pickle(\"../data/valid_collection.pkl\")\n",
    "human_collection = pd.read_pickle(\"../data/human_collection.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For testing, in this cell an individual indexing can be constructed. \n",
    "\n",
    "We have implemented a heirarchy of \"split\" modes, which will disambiguate continuous SSE where multiple segment centers are contained --> **split_modes**\n",
    "\n",
    "Setting _debug=True_ will result in a large amount of information being printed, enabling the tracing of errors and irregularities during the assignment process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a Uniprot identifier here.\n",
    "uniprot = \"Q8IZF6\"\n",
    "\n",
    "for i, gain in enumerate(human_collection.collection):\n",
    "    if uniprot not in gain.name: \n",
    "        continue\n",
    "    file_prefix = f\"../test_stal_indexing/f3_test{gain.name}\" # a temp folder where calculations and outputs will be stored.\n",
    "    print(\"_\"*30, f\"\\n{i} {gain.name}\")\n",
    "    element_intervals, element_centers, residue_labels, unindexed_elements, params = gaingrn.scripts.assign.assign_indexing(gain, \n",
    "                                                                                                file_prefix=file_prefix, \n",
    "                                                                                                gain_pdb=gaingrn.scripts.io.find_pdb(gain.name, '../../all_pdbs'), \n",
    "                                                                                                template_dir='../data/template_pdbs/',\n",
    "                                                                                                template_json='../data/template_data.json',\n",
    "                                                                                                outlier_cutoff=5.0,\n",
    "                                                                                                gesamt_bin=GESAMT_BIN,\n",
    "                                                                                                debug=False, # If you want ALL that is happening\n",
    "                                                                                                create_pdb=False,\n",
    "                                                                                                hard_cut={\"S2\":7,\"S7\":3,\"H5\":3},\n",
    "                                                                                                patch_gps=True\n",
    "                                                                                                )\n",
    "    # This dictionary denotes the priority line of splitting. Only the lowest-heirarchy split is indicated, i.e. the higher the number, the \"worse\" the split.\n",
    "    split_modes = {\n",
    "        0:\"No Split.\",\n",
    "        1:\"Split by coiled residue.\",\n",
    "        2:\"Split by disordered residue.\",\n",
    "        3:\"Split by Proline/Glycine\",\n",
    "        4:\"Split by hard cut.\",\n",
    "        5:\"Overwrite by anchor priority.\"\n",
    "    }\n",
    "    print(gain.name, gain.subdomain_boundary)\n",
    "    if params[\"split_mode\"] > 0:\n",
    "        print(params[\"split_mode\"], split_modes[params[\"split_mode\"]])\n",
    "    #print(element_intervals, element_centers, residue_labels, unindexed_elements, sep=\"\\n\")\n",
    "    print(unindexed_elements, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, the full __StAlIndexing__ may be constructed test-wise, or by default the pickle of the Indexing is loaded. \n",
    "Keep in mind that within this jupyter notebook - due to its handling of multiprocessig.Pool - the number of threads is limited to 1 and this takes a while for the full set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessions = [gain.name.split(\"-\")[0].split(\"_\")[0] for gain in valid_collection.collection]\n",
    "sequences = [\"\".join(gain.sequence) for gain in valid_collection.collection]\n",
    "\n",
    "fasta_offsets = gaingrn.scripts.alignment_utils.find_offsets(\"../data/all_query_sequences.fasta\",\n",
    "                                 accessions, \n",
    "                                 sequences)\n",
    "\n",
    "# Pseudocenter cases: cases, where the segment center is NOT part of the segment in question. Therefore, an alternative indexing method is applied,\n",
    "# using a \"pseudocenter\", a residue which matches the segment, but is not the .50 residue.\n",
    "ps_file = \"../data/pseudocenters.csv\"\n",
    "open(ps_file,\"w\").write(f\"GAIN,res,elem\\n\")\n",
    "\n",
    "# Careful when running, this takes a lot of time to calculate on single-thread. use run_indexing.py for fast multithreaded calculation.\n",
    "stal_indexing = StAlIndexing(valid_collection.collection, \n",
    "                             prefix=\"../test_stal_indexing/test20\", \n",
    "                             pdb_dir='../../all_pdbs/',\n",
    "                             template_json='../data/template_data.json',\n",
    "                             gesamt_bin=GESAMT_BIN, \n",
    "                             template_dir='../data/template_pdbs/', \n",
    "                             fasta_offsets=fasta_offsets,\n",
    "                             n_threads=1,\n",
    "                             #pseudocenters=ps_file,\n",
    "                             debug=False)\n",
    "#with open(\"../data/stal_indexing.pkl\",\"wb\") as save:\n",
    "#    pickle.dump(stal_indexing, save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a file contaning all segment starts, ends and center residues for each GAIN domain. This is the basis for the GPCRdb implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accession': 0, 'Name': 1, 'Species': 2, 'type': 3, 'GPS-2': 4, 'GPS-1': 5, 'GPS+1': 6, 'H1.start': 7, 'H1.anchor': 8, 'H1.end': 9, 'H2.start': 10, 'H2.anchor': 11, 'H2.end': 12, 'H3.start': 13, 'H3.anchor': 14, 'H3.end': 15, 'H4.start': 16, 'H4.anchor': 17, 'H4.end': 18, 'H5.start': 19, 'H5.anchor': 20, 'H5.end': 21, 'H6.start': 22, 'H6.anchor': 23, 'H6.end': 24, 'S1.start': 25, 'S1.anchor': 26, 'S1.end': 27, 'S2.start': 28, 'S2.anchor': 29, 'S2.end': 30, 'S3.start': 31, 'S3.anchor': 32, 'S3.end': 33, 'S4.start': 34, 'S4.anchor': 35, 'S4.end': 36, 'S5.start': 37, 'S5.anchor': 38, 'S5.end': 39, 'S6.start': 40, 'S6.anchor': 41, 'S6.end': 42, 'S7.start': 43, 'S7.anchor': 44, 'S7.end': 45, 'S8.start': 46, 'S8.anchor': 47, 'S8.end': 48, 'S9.start': 49, 'S9.anchor': 50, 'S9.end': 51, 'S10.start': 52, 'S10.anchor': 53, 'S10.end': 54, 'S11.start': 55, 'S11.anchor': 56, 'S11.end': 57, 'S12.start': 58, 'S12.anchor': 59, 'S12.end': 60, 'S13.start': 61, 'S13.anchor': 62, 'S13.end': 63, 'S14.start': 64, 'S14.anchor': 65, 'S14.end': 66}\n",
      "Completed file ../data/gaingrn_indexing.csv .\n",
      "[DEBUG] construct_data_matrix: Found the following unique headers:\n",
      "\tindividual_headers = array(['H1.D1', 'H1.E1', 'H1.F4'], dtype='<U5')\n",
      "{'Accession': 0, 'Name': 1, 'Species': 2, 'type': 3, 'GPS-2': 4, 'GPS-1': 5, 'GPS+1': 6, 'H1.start': 7, 'H1.anchor': 8, 'H1.end': 9, 'H2.start': 10, 'H2.anchor': 11, 'H2.end': 12, 'H3.start': 13, 'H3.anchor': 14, 'H3.end': 15, 'H4.start': 16, 'H4.anchor': 17, 'H4.end': 18, 'H5.start': 19, 'H5.anchor': 20, 'H5.end': 21, 'H6.start': 22, 'H6.anchor': 23, 'H6.end': 24, 'S1.start': 25, 'S1.anchor': 26, 'S1.end': 27, 'S2.start': 28, 'S2.anchor': 29, 'S2.end': 30, 'S3.start': 31, 'S3.anchor': 32, 'S3.end': 33, 'S4.start': 34, 'S4.anchor': 35, 'S4.end': 36, 'S5.start': 37, 'S5.anchor': 38, 'S5.end': 39, 'S6.start': 40, 'S6.anchor': 41, 'S6.end': 42, 'S7.start': 43, 'S7.anchor': 44, 'S7.end': 45, 'S8.start': 46, 'S8.anchor': 47, 'S8.end': 48, 'S9.start': 49, 'S9.anchor': 50, 'S9.end': 51, 'S10.start': 52, 'S10.anchor': 53, 'S10.end': 54, 'S11.start': 55, 'S11.anchor': 56, 'S11.end': 57, 'S12.start': 58, 'S12.anchor': 59, 'S12.end': 60, 'S13.start': 61, 'S13.anchor': 62, 'S13.end': 63, 'S14.start': 64, 'S14.anchor': 65, 'S14.end': 66, 'H1.D1.start': 67, 'H1.D1.anchor': 68, 'H1.D1.end': 69, 'H1.E1.start': 70, 'H1.E1.anchor': 71, 'H1.E1.end': 72, 'H1.F4.start': 73, 'H1.F4.anchor': 74, 'H1.F4.end': 75}\n",
      "Completed file ../data/gaingrn_indexing.unique.csv .\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-calculated STAL indexing, which saves some time.\n",
    "stal_indexing = pd.read_pickle(\"../data/stal_indexing.pkl\")\n",
    "\n",
    "header, matrix = stal_indexing.construct_data_matrix(unique_sse=False)\n",
    "stal_indexing.data2csv(header, matrix, \"../data/gaingrn_indexing.csv\")\n",
    "header, matrix = stal_indexing.construct_data_matrix(unique_sse=True)\n",
    "stal_indexing.data2csv(header, matrix, \"../data/gaingrn_indexing.unique.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we construct the indexing for the human set with the modified STRIDE files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StAlIndexing: Assigning indexing via single process.\n",
      "Total of keys found in the dictionaries:\n",
      " ['GPS.+1', 'GPS.-1', 'GPS.-2', 'H1.37', 'H1.38', 'H1.39', 'H1.40', 'H1.41', 'H1.42', 'H1.43', 'H1.44', 'H1.45', 'H1.46', 'H1.47', 'H1.48', 'H1.49', 'H1.50', 'H1.51', 'H1.52', 'H1.53', 'H1.54', 'H1.55', 'H1.56', 'H1.57', 'H1.58', 'H1.59', 'H1.60', 'H1.E1.50', 'H1.E1.51', 'H1.E1.52', 'H1.E1.53', 'H1.E1.54', 'H1.F4.47', 'H1.F4.48', 'H1.F4.49', 'H1.F4.50', 'H1.F4.51', 'H1.F4.52', 'H1.F4.53', 'H1.F4.54', 'H2.36', 'H2.37', 'H2.38', 'H2.39', 'H2.40', 'H2.41', 'H2.42', 'H2.43', 'H2.44', 'H2.45', 'H2.46', 'H2.47', 'H2.48', 'H2.49', 'H2.50', 'H2.51', 'H2.52', 'H2.53', 'H2.54', 'H2.55', 'H2.56', 'H2.57', 'H2.58', 'H2.59', 'H2.60', 'H2.61', 'H2.62', 'H3.43', 'H3.44', 'H3.45', 'H3.46', 'H3.47', 'H3.48', 'H3.49', 'H3.50', 'H3.51', 'H3.52', 'H3.53', 'H3.54', 'H3.55', 'H3.56', 'H3.57', 'H3.58', 'H3.59', 'H3.60', 'H3.61', 'H3.62', 'H3.63', 'H3.64', 'H3.65', 'H3.66', 'H3.67', 'H3.68', 'H3.69', 'H3.70', 'H3.71', 'H4.22', 'H4.23', 'H4.24', 'H4.25', 'H4.26', 'H4.27', 'H4.28', 'H4.29', 'H4.30', 'H4.31', 'H4.32', 'H4.33', 'H4.34', 'H4.35', 'H4.36', 'H4.37', 'H4.38', 'H4.39', 'H4.40', 'H4.41', 'H4.42', 'H4.43', 'H4.44', 'H4.45', 'H4.46', 'H4.47', 'H4.48', 'H4.49', 'H4.50', 'H4.51', 'H4.52', 'H4.53', 'H4.54', 'H4.55', 'H4.56', 'H4.57', 'H4.58', 'H4.59', 'H4.60', 'H5.44', 'H5.45', 'H5.46', 'H5.47', 'H5.48', 'H5.49', 'H5.50', 'H5.51', 'H5.52', 'H5.53', 'H5.54', 'H5.55', 'H5.56', 'H5.57', 'H6.35', 'H6.36', 'H6.37', 'H6.38', 'H6.39', 'H6.40', 'H6.41', 'H6.42', 'H6.43', 'H6.44', 'H6.45', 'H6.46', 'H6.47', 'H6.48', 'H6.49', 'H6.50', 'H6.51', 'H6.52', 'H6.53', 'H6.54', 'H6.55', 'H6.56', 'H6.57', 'H6.58', 'H6.59', 'H6.60', 'H6.61', 'H6.62', 'H6.63', 'S1.46', 'S1.47', 'S1.48', 'S1.49', 'S1.50', 'S1.51', 'S1.52', 'S10.41', 'S10.42', 'S10.43', 'S10.44', 'S10.45', 'S10.46', 'S10.47', 'S10.48', 'S10.49', 'S10.50', 'S10.51', 'S10.52', 'S10.53', 'S11.47', 'S11.48', 'S11.49', 'S11.50', 'S11.51', 'S12.50', 'S12.51', 'S12.52', 'S12.53', 'S12.54', 'S12.55', 'S12.56', 'S13.45', 'S13.46', 'S13.47', 'S13.48', 'S13.49', 'S13.50', 'S13.51', 'S13.52', 'S13.53', 'S14.44', 'S14.45', 'S14.46', 'S14.47', 'S14.48', 'S14.49', 'S14.50', 'S14.51', 'S14.52', 'S14.53', 'S14.54', 'S14.55', 'S2.48', 'S2.49', 'S2.50', 'S2.51', 'S2.52', 'S2.53', 'S2.54', 'S2.55', 'S2.56', 'S3.47', 'S3.48', 'S3.49', 'S3.50', 'S3.51', 'S3.52', 'S3.53', 'S3.54', 'S3.55', 'S4.47', 'S4.48', 'S4.49', 'S4.50', 'S4.51', 'S4.52', 'S4.53', 'S5.43', 'S5.44', 'S5.45', 'S5.46', 'S5.47', 'S5.48', 'S5.49', 'S5.50', 'S5.51', 'S5.52', 'S5.53', 'S5.54', 'S6.42', 'S6.43', 'S6.44', 'S6.45', 'S6.46', 'S6.47', 'S6.48', 'S6.49', 'S6.50', 'S6.51', 'S7.45', 'S7.46', 'S7.47', 'S7.48', 'S7.49', 'S7.50', 'S7.51', 'S8.48', 'S8.49', 'S8.50', 'S8.51', 'S8.52', 'S8.53', 'S8.54', 'S8.55', 'S8.56', 'S8.57', 'S8.58', 'S9.47', 'S9.48', 'S9.49', 'S9.50', 'S9.51', 'S9.52', 'S9.53', 'S9.54', 'S9.55', 'S9.56'] ['H1.50', 'H2.50', 'H3.50', 'H4.50', 'H5.50', 'H6.50', 'S1.50', 'S2.50', 'S3.50', 'S4.50', 'S5.50', 'S6.50', 'S7.50', 'S8.50', 'S9.50', 'S10.50', 'S11.50', 'S12.50', 'S13.50', 'S14.50']\n",
      "First entry {'H1.39': 460, 'H1.40': 461, 'H1.41': 462, 'H1.42': 463, 'H1.43': 464, 'H1.44': 465, 'H1.45': 466, 'H1.46': 467, 'H1.47': 468, 'H1.48': 469, 'H1.49': 470, 'H1.50': 471, 'H1.51': 472, 'H1.52': 473, 'H1.53': 474, 'H2.40': 481, 'H2.41': 482, 'H2.42': 483, 'H2.43': 484, 'H2.44': 485, 'H2.45': 486, 'H2.46': 487, 'H2.47': 488, 'H2.48': 489, 'H2.49': 490, 'H2.50': 491, 'H2.51': 492, 'H2.52': 493, 'H2.53': 494, 'H2.54': 495, 'H2.55': 496, 'H2.56': 497, 'H3.48': 503, 'H3.49': 504, 'H3.50': 505, 'H3.51': 506, 'H3.52': 507, 'H3.53': 508, 'H3.54': 509, 'H3.55': 510, 'H3.56': 511, 'H3.57': 512, 'H3.58': 513, 'H3.59': 514, 'H3.60': 515, 'H3.61': 516, 'H3.62': 517, 'H3.63': 518, 'H3.64': 519, 'H3.65': 520, 'H3.66': 521, 'H4.44': 535, 'H4.45': 536, 'H4.46': 537, 'H4.47': 538, 'H4.48': 539, 'H4.49': 540, 'H4.50': 541, 'H4.51': 542, 'H4.52': 543, 'H4.53': 544, 'H4.54': 545, 'H4.55': 546, 'H4.56': 547, 'H4.57': 548, 'H5.44': 550, 'H5.45': 551, 'H5.46': 552, 'H5.47': 553, 'H5.48': 554, 'H5.49': 555, 'H5.50': 556, 'H5.51': 557, 'H5.52': 558, 'H5.53': 559, 'H5.54': 560, 'H5.55': 561, 'H5.56': 562, 'H6.40': 564, 'H6.41': 565, 'H6.42': 566, 'H6.43': 567, 'H6.44': 568, 'H6.45': 569, 'H6.46': 570, 'H6.47': 571, 'H6.48': 572, 'H6.49': 573, 'H6.50': 574, 'H6.51': 575, 'H6.52': 576, 'H6.53': 577, 'H6.54': 578, 'H6.55': 579, 'H6.56': 580, 'H6.57': 581, 'H6.58': 582, 'H6.59': 583, 'H6.60': 584, 'H6.61': 585, 'H6.62': 586, 'S1.48': 592, 'S1.49': 593, 'S1.50': 594, 'S1.51': 595, 'S2.48': 599, 'S2.49': 600, 'S2.50': 601, 'S2.51': 602, 'S2.52': 603, 'S2.53': 604, 'S2.54': 605, 'S2.55': 606, 'S2.56': 607, 'S3.48': 614, 'S3.49': 615, 'S3.50': 616, 'S5.47': 633, 'S5.48': 634, 'S5.49': 635, 'S5.50': 636, 'S6.42': 676, 'S6.43': 677, 'S6.44': 678, 'S6.45': 679, 'S6.46': 680, 'S6.47': 681, 'S6.48': 682, 'S6.49': 683, 'S6.50': 684, 'S7.48': 708, 'S7.49': 709, 'S7.50': 710, 'S8.50': 714, 'S8.51': 715, 'S8.52': 716, 'S8.53': 717, 'S8.54': 718, 'S8.55': 719, 'S8.56': 720, 'S8.57': 721, 'S9.48': 729, 'S9.49': 730, 'S9.50': 731, 'S9.51': 732, 'S9.52': 733, 'S9.53': 734, 'S9.54': 735, 'S9.55': 736, 'S10.46': 750, 'S10.47': 751, 'S10.48': 752, 'S10.49': 753, 'S10.50': 754, 'S10.51': 755, 'S11.49': 765, 'S11.50': 766, 'S11.51': 767, 'S12.50': 771, 'S12.51': 772, 'S12.52': 773, 'S13.45': 780, 'S13.46': 781, 'S13.47': 782, 'S13.48': 783, 'S13.49': 784, 'S13.50': 785, 'S13.51': 786, 'S13.52': 787, 'S14.46': 790, 'S14.47': 791, 'S14.48': 792, 'S14.49': 793, 'S14.50': 794, 'S14.51': 795, 'S14.52': 796, 'GPS.-2': 787, 'GPS.-1': 788, 'GPS.+1': 789} {'H1.50': 471, 'H2.50': 491, 'H3.50': 505, 'H4.50': 541, 'H5.50': 556, 'H6.50': 574, 'S1.50': 594, 'S2.50': 601, 'S3.50': 616, 'S5.50': 636, 'S6.50': 684, 'S7.50': 710, 'S8.50': 714, 'S9.50': 734, 'S10.50': 754, 'S11.50': 766, 'S12.50': 771, 'S13.50': 785, 'S14.50': 794, 'GPS': 788}\n",
      "{'Accession': 0, 'Name': 1, 'Species': 2, 'type': 3, 'GPS-2': 4, 'GPS-1': 5, 'GPS+1': 6, 'H1.start': 7, 'H1.anchor': 8, 'H1.end': 9, 'H2.start': 10, 'H2.anchor': 11, 'H2.end': 12, 'H3.start': 13, 'H3.anchor': 14, 'H3.end': 15, 'H4.start': 16, 'H4.anchor': 17, 'H4.end': 18, 'H5.start': 19, 'H5.anchor': 20, 'H5.end': 21, 'H6.start': 22, 'H6.anchor': 23, 'H6.end': 24, 'S1.start': 25, 'S1.anchor': 26, 'S1.end': 27, 'S2.start': 28, 'S2.anchor': 29, 'S2.end': 30, 'S3.start': 31, 'S3.anchor': 32, 'S3.end': 33, 'S4.start': 34, 'S4.anchor': 35, 'S4.end': 36, 'S5.start': 37, 'S5.anchor': 38, 'S5.end': 39, 'S6.start': 40, 'S6.anchor': 41, 'S6.end': 42, 'S7.start': 43, 'S7.anchor': 44, 'S7.end': 45, 'S8.start': 46, 'S8.anchor': 47, 'S8.end': 48, 'S9.start': 49, 'S9.anchor': 50, 'S9.end': 51, 'S10.start': 52, 'S10.anchor': 53, 'S10.end': 54, 'S11.start': 55, 'S11.anchor': 56, 'S11.end': 57, 'S12.start': 58, 'S12.anchor': 59, 'S12.end': 60, 'S13.start': 61, 'S13.anchor': 62, 'S13.end': 63, 'S14.start': 64, 'S14.anchor': 65, 'S14.end': 66}\n",
      "Completed file ../data/human_indexing.csv .\n",
      "[DEBUG] construct_data_matrix: Found the following unique headers:\n",
      "\tindividual_headers = array(['H1.E1', 'H1.F4'], dtype='<U5')\n",
      "{'Accession': 0, 'Name': 1, 'Species': 2, 'type': 3, 'GPS-2': 4, 'GPS-1': 5, 'GPS+1': 6, 'H1.start': 7, 'H1.anchor': 8, 'H1.end': 9, 'H2.start': 10, 'H2.anchor': 11, 'H2.end': 12, 'H3.start': 13, 'H3.anchor': 14, 'H3.end': 15, 'H4.start': 16, 'H4.anchor': 17, 'H4.end': 18, 'H5.start': 19, 'H5.anchor': 20, 'H5.end': 21, 'H6.start': 22, 'H6.anchor': 23, 'H6.end': 24, 'S1.start': 25, 'S1.anchor': 26, 'S1.end': 27, 'S2.start': 28, 'S2.anchor': 29, 'S2.end': 30, 'S3.start': 31, 'S3.anchor': 32, 'S3.end': 33, 'S4.start': 34, 'S4.anchor': 35, 'S4.end': 36, 'S5.start': 37, 'S5.anchor': 38, 'S5.end': 39, 'S6.start': 40, 'S6.anchor': 41, 'S6.end': 42, 'S7.start': 43, 'S7.anchor': 44, 'S7.end': 45, 'S8.start': 46, 'S8.anchor': 47, 'S8.end': 48, 'S9.start': 49, 'S9.anchor': 50, 'S9.end': 51, 'S10.start': 52, 'S10.anchor': 53, 'S10.end': 54, 'S11.start': 55, 'S11.anchor': 56, 'S11.end': 57, 'S12.start': 58, 'S12.anchor': 59, 'S12.end': 60, 'S13.start': 61, 'S13.anchor': 62, 'S13.end': 63, 'S14.start': 64, 'S14.anchor': 65, 'S14.end': 66, 'H1.E1.start': 67, 'H1.E1.anchor': 68, 'H1.E1.end': 69, 'H1.F4.start': 70, 'H1.F4.anchor': 71, 'H1.F4.end': 72}\n",
      "Completed file ../data/human_indexing.unique.csv .\n"
     ]
    }
   ],
   "source": [
    "human_collection = pd.read_pickle(\"../data/human_collection.pkl\")\n",
    "\n",
    "human_accessions = [gain.name.split(\"-\")[0].split(\"_\")[0] for gain in human_collection.collection]\n",
    "human_sequences = [\"\".join(gain.sequence) for gain in human_collection.collection]\n",
    "\n",
    "human_fasta_offsets = gaingrn.scripts.alignment_utils.find_offsets(\"../data/all_query_sequences.fasta\", \n",
    "                                 human_accessions, \n",
    "                                 human_sequences)\n",
    "\n",
    "stal_human_indexing = stal_indexing = StAlIndexing(human_collection.collection, \n",
    "                             prefix=\"../../test_stal_indexing/test\", \n",
    "                             pdb_dir='../../all_pdbs/',  \n",
    "                             template_dir='../data/template_pdbs/', \n",
    "                             template_json = '../data/template_data.json',\n",
    "                             outlier_cutoff=5.0,\n",
    "                             fasta_offsets=human_fasta_offsets,\n",
    "                             gesamt_bin=GESAMT_BIN,\n",
    "                             n_threads=1,\n",
    "                             debug=False)\n",
    "\n",
    "with open(\"../data/human_indexing.pkl\", \"wb\") as humanfile:\n",
    "    pickle.dump(stal_human_indexing, humanfile, -1)\n",
    "\n",
    "header, matrix = stal_human_indexing.construct_data_matrix(overwrite_gps=True, unique_sse=False)\n",
    "stal_human_indexing.data2csv(header, matrix, \"../data/human_indexing.csv\")\n",
    "\n",
    "# Also include unique Helices in a separate file.\n",
    "header, matrix = stal_human_indexing.construct_data_matrix(overwrite_gps=True, unique_sse=True)\n",
    "stal_human_indexing.data2csv(header, matrix, \"../data/human_indexing.unique.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Indexing for the whole set is best constructed via multithreaded exection by _stal\\_indexing.py_ and saved in a pickle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
