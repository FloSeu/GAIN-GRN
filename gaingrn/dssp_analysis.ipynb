{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is dssp_analysis.ipynb, a notebook for determining whether DSSP or STRIDE is better to be used for assessing the segment / secondary structure composition of GAIN domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a database from a dataset for nomenclating\n",
    "# INPUT: a collection of GAIN domain PDBs, their sequences as one large \".fa\" file\n",
    "from gaingrn.scripts.gain_classes import GainDomain, GainCollection, Anchors, GPS\n",
    "import sse_func\n",
    "import numpy as np\n",
    "import glob\n",
    "#import multiprocessing as mp\n",
    "#from subprocess import Popen, PIPE\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from shutil import copyfile\n",
    "import math\n",
    "import gaingrn.scripts.alignment_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_receptor(sequences, selection):\n",
    "    new_list = []\n",
    "    for seq_tup in sequences:\n",
    "        if selection in seq_tup[0]:\n",
    "            new_list.append(seq_tup)\n",
    "    return new_list\n",
    "\n",
    "def filter_by_list(sequences, selection): # selection list\n",
    "    new_list = []\n",
    "    for seq_tup in sequences:\n",
    "        for it in selection:\n",
    "            if it in seq_tup[0]:\n",
    "                new_list.append(seq_tup)\n",
    "    return new_list\n",
    "\n",
    "def read_dssp(file):\n",
    "    '''\n",
    "    Similar to sse_func.read_sse_asg, parse the file and return the sse_sequence list\n",
    "    Parameters:\n",
    "        file : str, required\n",
    "            DSSP file to be read (for now, GUILLEs pre-compiled version)\n",
    "    Returns:\n",
    "        sse_sequence : list\n",
    "            LIST containing a sequence of all letters assigned to the residues\n",
    "    '''\n",
    "    sse_list = []\n",
    "    with open(file) as f:\n",
    "        for l in f.readlines():\n",
    "                items = l.split()\n",
    "                if len(items) > 1:\n",
    "                    one_letter_sse = items[1]\n",
    "                else:\n",
    "                    one_letter_sse = ''\n",
    "                sse_list.append(one_letter_sse)\n",
    "    return sse_list\n",
    "\n",
    "def build_dssp_dict(file):\n",
    "    # Build a more rudimentary SSE dictionary based on the SSE sequence from read_dssp:\n",
    "    def move2dict(sse_dict, dssp_keys, element, sse_tuple):\n",
    "        if element not in dssp_keys.keys():\n",
    "            #print(f\"Skipping element with DSSP assignment \\\"{element}\\\" \")\n",
    "            return sse_dict\n",
    "        sse_id = dssp_keys[element]\n",
    "        if sse_id not in sse_dict.keys():\n",
    "            sse_dict[sse_id] = [sse_tuple]\n",
    "        else:\n",
    "            sse_dict[sse_id].append(sse_tuple)\n",
    "        return sse_dict\n",
    "    \n",
    "    with open(file) as f:\n",
    "        first_index = int(f.read().split()[0][1:]) # the integer value of the first index\n",
    "        \n",
    "    sse_list = read_dssp(file)             \n",
    "    dssp_keys = { \"H\" : \"AlphaHelix\",\n",
    "                     \"B\" : \"Bridge\",\n",
    "                     \"E\" : \"Strand\",\n",
    "                     \"G\" : \"310Helix\",\n",
    "                     \"I\" : \"5Helix\",\n",
    "                     \"T\" : \"Turn\",\n",
    "                     \"S\" : \"Bend\"     }\n",
    "    sse_dict = {}\n",
    "    # Parse through the entries one by one and construc tuples (first_res, last_res)\n",
    "    # pass them into the dictionary with the corresponding key\n",
    "    stored_element = ''\n",
    "    within_element = False\n",
    "    for idx, assignment in enumerate(sse_list):\n",
    "        if assignment != stored_element:\n",
    "            if within_element:\n",
    "                last = idx + first_index\n",
    "                within_element = False\n",
    "                # Move the tuple into the dictionary\n",
    "                sse_dict = move2dict(sse_dict, dssp_keys, stored_element, (first,last))\n",
    "            if not within_element:\n",
    "                if assignment == \"\":\n",
    "                    stored_element = \"\"\n",
    "                    continue\n",
    "                first = idx + first_index\n",
    "                stored_element = assignment\n",
    "                within_element = True\n",
    "    return sse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dssp = '/home/hildilab/projects/agpcr_nom/dssp4/test.dssp'\n",
    "#print(build_dssp_dict(test_dssp))\n",
    "strides = glob.glob('/home/hildilab/projects/agpcr_nom/human_31_0/sse/*.stride')\n",
    "# COMPARE LISTS OF STRIDE AGAINST DSSP\n",
    "for f in strides:\n",
    "    print(\"\\n\", f)\n",
    "    dssp_list = read_dssp(f.replace(\".stride\",\"_gain.dssp\"))\n",
    "    with open(f.replace(\".stride\",\"_gain.dssp\")) as g:\n",
    "        data = g.read()\n",
    "        #print(data.split(\"\\n\"))\n",
    "        first_index = int(data.split()[0][1:]) # the integer value of the first index\n",
    "        last_index = int(data.split(\"\\n\")[-2].split(\"\\t\")[0][1:])#.split()[1:]\n",
    "        #print(last_index)\n",
    "        \n",
    "    stride_list, _ = sse_func.read_sse_asg(f)\n",
    "    dssp_dict = build_dssp_dict(f.replace(\".stride\",\"_gain.dssp\"))\n",
    "    stride_dict = sse_func.cut_sse_dict(first_index, last_index, sse_func.read_sse_loc(f))\n",
    "    for ki in dssp_dict.keys():\n",
    "        if ki in stride_dict.keys():\n",
    "            print(ki)\n",
    "            \"\"\"            for i, item in enumerate(dssp_dict[ki]):\n",
    "                try: \n",
    "                    print(item,\"\\t\\t\", stride_dict[ki][i])\n",
    "                except:\n",
    "                    print(item)\"\"\"\n",
    "            \n",
    "            print(\"DSSP\\t\", dssp_dict[ki])\n",
    "            print(\"STRIDE\\t\",stride_dict[ki])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seqs = sse_func.read_multi_seq(\"/home/hildilab/projects/agpcr_nom/app_gain_gain.fa\")\n",
    "print(len(valid_seqs))\n",
    "quality_file = \"/home/hildilab/projects/agpcr_nom/app_gain_gain.mafft.jal\"\n",
    "alignment_file = \"/home/hildilab/projects/agpcr_nom/app_gain_gain.mafft.fa\"\n",
    "stride_folder = \"/home/hildilab/projects/agpcr_nom/all_gps_stride\"\n",
    "#stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/all_gps_stride/*\")\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2/*\")\n",
    "quality = sse_func.read_quality(quality_file)\n",
    "gps_minus_one = 6781 # -1 of the ACTUAL COLUMN (6782) in JALVIEW since there is is ONE-INDEXED\n",
    "aln_cutoff = 6826 # \n",
    "alignment_dict = sse_func.read_alignment(alignment_file, aln_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_collection = GainCollection(  alignment_file = alignment_file,\n",
    "                                    aln_cutoff = aln_cutoff,\n",
    "                                    quality = quality,\n",
    "                                    gps_index = gps_minus_one,\n",
    "                                    stride_files = stride_files,\n",
    "                                    sequence_files=None,\n",
    "                                    sequences=valid_seqs,\n",
    "                                    alignment_dict = alignment_dict,\n",
    "                                    is_truncated = True,\n",
    "                                    stride_outlier_mode=True)\n",
    "#for gain in valid_collection.collection:\n",
    "#    gain.create_indexing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Parsing out specific Files from the overall dataset based on selection\n",
    "def grab_selection(parse_string, stride_path, pdb_list, sequences, profile_path, target_dir, seqs=None):\n",
    "    # grabs PDB file, stride file, profiles, sequence from FASTA and copies to target dir.\n",
    "    if seqs is None:\n",
    "        sub_seqs = [seq for seq in sequences if parse_string.lower() in seq[0].lower()]\n",
    "    else: sub_seqs = seqs\n",
    "    print(f\"Found {len(sub_seqs)} sequences.\")\n",
    "    strides = glob.glob(stride_path+\"*.stride\")#\n",
    "    profiles = glob.glob(profile_path+\"*.png\")\n",
    "    \n",
    "    sub_strides = []\n",
    "    sub_profiles = []\n",
    "    sub_pdbs = []\n",
    "    \n",
    "    for seq in sub_seqs:\n",
    "        ac = seq[0].split(\"-\")[0]\n",
    "        [sub_profiles.append(prof) for prof in profiles if ac in prof]\n",
    "        [sub_strides.append(stride) for stride in strides if ac in stride]\n",
    "        [sub_pdbs.append(pdb) for pdb in pdb_list if ac in pdb]\n",
    "    \n",
    "    for prof in sub_profiles:\n",
    "        name = prof.split(\"/\")[-1]\n",
    "        copyfile(prof, target_dir+\"profiles/\"+name)\n",
    "    \n",
    "    for stride in sub_strides:\n",
    "        name = stride.split(\"/\")[-1]\n",
    "        copyfile(stride, target_dir+\"strides/\"+name)\n",
    "    \n",
    "    for pdb in sub_pdbs:\n",
    "        name = pdb.split(\"/\")[-1]\n",
    "        copyfile(pdb, target_dir+\"pdbs/\"+name)\n",
    "        \n",
    "    for seq in sub_seqs:\n",
    "        sse_func.write2fasta(seq[1]+\"\\n\", seq[0], target_dir+\"seqs/\"+seq[0]+\".fa\")\n",
    "        \n",
    "    print(f\"Copied {len(sub_pdbs)} PDB files, {len(sub_strides)} STRIDE files,\",\n",
    "          f\" {len(sub_profiles)} Profiles and {len(sub_seqs)} Sequences\",\n",
    "          f\"for Selection {parse_string}\")\n",
    "    \n",
    "root_path = \"/home/hildilab/projects/agpcr_nom/\"\n",
    "profile_path = root_path+\"all_gps_profiles/\"\n",
    "pdb_list = glob.glob(f\"{root_path}all_gps*/batch*/*rank_1_*.pdb\")\n",
    "print(len(pdb_list))\n",
    "#valid_seqs\n",
    "target_dir = root_path+\"human/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_seqs = [\"Q9HBW9\",\"O60241\",\"Q6QNK2\",\"Q9UHX3\",\"Q5T601\",\"Q96PE1\",\"O60242\",\"Q86SQ4\",\n",
    "                \"O94910\",\"Q8IWK6\",\"Q8IZP9\",\"Q8WXG9\",\"Q86Y34\",\"O95490\",\"Q14246\",\"Q9BY15\",\n",
    "                \"Q8IZF2\",\"Q86SQ3\",\"Q8IZF6\",\"Q96K78\",\"Q8IZF3\",\"Q8IZF7\",\"Q8IZF5\",\"Q7Z7M1\",\n",
    "                \"Q8IZF4\",\"Q9HCU4\",\"Q9NYQ6\",\"Q9NYQ7\",\"Q9HAR2\",\"O14514\",\"P48960\",\n",
    "                \"Q9Y653\"]\n",
    "sigma_2_strides = glob.glob(\"sigma_2*/*.stride\")\n",
    "list_32 = filter_by_list(valid_seqs, human_seqs)\n",
    "\n",
    "human_collection = GainCollection( alignment_file = alignment_file,\n",
    "                                        aln_cutoff = aln_cutoff,\n",
    "                                        quality = quality,\n",
    "                                        gps_index = gps_minus_one,\n",
    "                                        stride_files =  sigma_2_strides, #stride_files,\n",
    "                                        sequence_files=None,\n",
    "                                        sequences=list_32,\n",
    "                                        alignment_dict = alignment_dict,\n",
    "                                        is_truncated = True,\n",
    "                                        stride_outlier_mode = True\n",
    "                                         )\n",
    "#print(len(human_collection.collection))\n",
    "\"\"\"for gain in human_collection.collection:\n",
    "    #print(gain.name, gain.start, gain.end, gain.sequence, gain.index, gain.subdomain_boundary)\n",
    "    pdb_out = root_path+\"human/trunc_pdbs/\"+gain.name+\"_gain.pdb\"\n",
    "    ac = gain.name.split(\"-\")[0]\n",
    "    found_pdb = [pdb for pdb in pdb_list if ac in pdb]\n",
    "    target_pdb = found_pdb[0]\n",
    "    gain.write_gain_pdb(target_pdb, pdb_out)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_collection.plot_sse_hist(title=f\"Receptor group: HUMAN_31 (Total: 31)\",\n",
    "                                   n_max=16,\n",
    "                                   savename=\"human_31.s2_newAnch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(valid_collection)\n",
    "occupancy = np.zeros([aln_cutoff],dtype=int)\n",
    "sse_matrix = np.zeros([len(valid_collection.collection), aln_cutoff])\n",
    "for i, gain in enumerate(valid_collection.collection):\n",
    "    #print(gain.sda_helices, gain.sdb_sheets)\n",
    "#    for res_id in range(gain.start,gain.end+1):\n",
    "    occupancy[gain.alignment_indices] += 1\n",
    "    \n",
    "    for helix in gain.sda_helices:\n",
    "        for res_id in range(helix[0],helix[1]+1):\n",
    "            sse_matrix[i,gain.alignment_indices[res_id]] = -1\n",
    "    for sheet in gain.sdb_sheets:\n",
    "        for res_id in range(sheet[0],sheet[1]+1):\n",
    "            sse_matrix[i,gain.alignment_indices[res_id]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors , anchor_occupation = valid_collection.find_anchors(cutoff=3000)\n",
    "print(anchors)\n",
    "for i, anchor in enumerate(anchors):\n",
    "    print(i, anchor)\n",
    "    if anchor < valid_collection.alignment_subdomain_boundary: \n",
    "        color = u'#1f77b4'\n",
    "    else: \n",
    "        color = u'#ff7f0e'\n",
    "    plt.scatter(anchor, valid_collection.anchor_hist[anchor]+1000, c=color, marker=\"1\",s=60)\n",
    "print(anchors, anchor_occupation)\n",
    "print(valid_collection.alignment_subdomain_boundary)\n",
    "anchor_dict = sse_func.make_anchor_dict(anchors, valid_collection.alignment_subdomain_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexing:\n",
    "    # A modified Indexing class similar to the indexing_classes.py Indexing, however specific for DSSP data integration.\n",
    "    def __init__(self, aGainCollection, fasta_offsets=None, split_mode='single'):\n",
    "        \n",
    "        length = len(aGainCollection.collection)\n",
    "        names = np.empty([length], dtype=object)\n",
    "        indexing_dirs = np.empty([length], dtype=object)\n",
    "        center_dirs = np.empty([length], dtype=object)\n",
    "        offsets = np.zeros([length], dtype=int)\n",
    "        total_keys = []\n",
    "        center_keys = []\n",
    "        if fasta_offsets is None:\n",
    "            self.fasta_offsets = np.zeros([length])\n",
    "        if fasta_offsets is not None: \n",
    "            corrected_offsets = []\n",
    "            for i in range(length):\n",
    "                # The existing FASTA offsets do not account for the residue starting not at 0,\n",
    "                # Therefore the value of the starting res (gain.start) needs to be subtracted.\n",
    "                corrected_offsets.append(fasta_offsets[i]-aGainCollection.collection[i].start)\n",
    "            self.fasta_offsets = np.array(corrected_offsets, dtype=int)\n",
    "            \n",
    "        for gain_index, gain in enumerate(aGainCollection.collection):\n",
    "            indexing_dir, indexing_centers = gain.create_indexing(anchors, \n",
    "                                                                  anchor_occupation, \n",
    "                                                                  anchor_dict,\n",
    "                                                                  split_mode=split_mode)\n",
    "            print(indexing_dir, indexing_centers)\n",
    "            for key in indexing_dir.keys():\n",
    "                if key not in total_keys:\n",
    "                    total_keys.append(key)\n",
    "                    \n",
    "            for key in indexing_centers.keys():\n",
    "                if key not in center_keys:\n",
    "                    center_keys.append(key)                 \n",
    "                \n",
    "            indexing_dirs[gain_index] = indexing_dir\n",
    "            center_dirs[gain_index] = indexing_centers\n",
    "            # Patch ADGRC/CELSR naming\n",
    "            names[gain_index] = gain.name.replace(\"CadherinEGFLAGseven-passG-typereceptor\", \"AGRC\")\n",
    "            offsets[gain_index] = gain.start\n",
    "\n",
    "        self.indexing_dirs = indexing_dirs\n",
    "        self.center_dirs = center_dirs\n",
    "        self.names = names\n",
    "        self.length = length\n",
    "        self.offsets = offsets\n",
    "        self.accessions = [gain.name.split(\"-\")[0].split(\"_\")[0] for gain in aGainCollection.collection]\n",
    "        self.sequences = [\"\".join(gain.sequence) for gain in aGainCollection.collection]\n",
    "        self.total_keys = sorted(total_keys)\n",
    "        self.center_keys = sorted(center_keys)\n",
    "        \n",
    "        print(\"Total of keys found in the dictionaries:\\n\", self.total_keys, self.center_keys)\n",
    "        print(\"First entry\", self.indexing_dirs[0], self.center_dirs[0])\n",
    "        \n",
    "        header_list = [\"Receptor\", \"Accession\"] + self.total_keys + self.center_keys\n",
    "        #header = \"Receptor,Accession,\" + \",\".join(self.total_keys) + \",\".join(self.center_keys)\n",
    "        header_list = [\"Receptor\", \"Accession\"] + self.total_keys + self.center_keys\n",
    "        header_dict = {}\n",
    "        for idx, item in enumerate(header_list):\n",
    "            header_dict[item] = idx\n",
    "\n",
    "        data_matrix = np.full([self.length, len(header_dict.keys())], fill_value='', dtype=object)\n",
    "        # Go through each of the sub-dictionaries and populate the dataframe:\n",
    "        for row in range(self.length):\n",
    "                # Q5T601_Q5KU15_..._Q9H615-AGRF1_HUMAN-AGRF1-Homo_sapiens.fa\n",
    "                # 0                        1           2     3\n",
    "            name_parts = self.names[row].split(\"-\")\n",
    "            data_matrix[row, header_dict[\"Receptor\"]] = name_parts[2]\n",
    "            data_matrix[row, header_dict[\"Accession\"]] = name_parts[0].split(\"_\")[0]\n",
    "            offset = self.offsets[row]\n",
    "            fa_offset = self.fasta_offsets[row]\n",
    "\n",
    "            for key in self.indexing_dirs[row].keys():\n",
    "                if key == \"GPS\":\n",
    "                    sse=[int(x+fa_offset) for x in self.indexing_dirs[row][key]]\n",
    "                    data_matrix[row, header_dict[key]] = f\"{sse[0]}-{sse[-1]}\"\n",
    "                else:\n",
    "                    sse = [int(x+offset+fa_offset) for x in self.indexing_dirs[row][key]]\n",
    "                    data_matrix[row, header_dict[key]] = f\"{sse[0]}-{sse[1]}\"\n",
    "\n",
    "            for key in self.center_dirs[row].keys():\n",
    "                data_matrix[row, header_dict[key]] = str(self.center_dirs[row][key]+offset+fa_offset)\n",
    "            \n",
    "            self.data_header = \",\".join(header_list)\n",
    "            self.data_matrix = data_matrix\n",
    "\n",
    "    def data2csv(self, outfile):\n",
    "        with open(outfile, \"w\") as f:\n",
    "            f.write(self.data_header+\"\\n\")\n",
    "            for row in range(self.length):\n",
    "                f.write(\",\".join(self.data_matrix[row,:])+\"\\n\")\n",
    "        print(\"Completed file\", outfile, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_base = Indexing(human_collection, split_mode='double')\n",
    "\n",
    "big_seq_file = \"agpcr_celsr.fasta\"\n",
    "fasta_offsets = gaingrn.scripts.alignment_utils.find_offsets(big_seq_file, \n",
    "                                 human_base.accessions, \n",
    "                                 human_base.sequences)\n",
    "\n",
    "fa_human_base = Indexing(human_collection, fasta_offsets = fasta_offsets, split_mode='double')\n",
    "\n",
    "human_base.data2csv(f\"default_indexed_s2_s2a_re_double_split.csv\")\n",
    "fa_human_base.data2csv(f\"uniprot_indexed_s2_s2a_re_double_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_pdb(in_pdb, out_pdb, offset):\n",
    "    with open(in_pdb,\"r\") as p:\n",
    "        data = p.readlines()\n",
    "    offset_pdb = open(out_pdb, \"w\")\n",
    "    \n",
    "    for line in data:\n",
    "        if line.startswith(\"ATOM\"):\n",
    "            res_id = int(line[22:26])\n",
    "            offset_pdb.write(f\"{line[:22]}{str(res_id+offset).rjust(4)}{line[26:]}\")\n",
    "        else:\n",
    "            offset_pdb.write(line)\n",
    "    \n",
    "    print(f\"Created PDB {out_pdb} with last residue {res_id+offset}. Total offset {offset} .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gain in enumerate(human_collection.collection):\n",
    "    #if \"Q6QNK2\" in gain.name:\n",
    "        x1, x2 = gain.create_indexing(anchors, anchor_occupation, anchor_dict, \n",
    "                    outdir = \"/home/hildilab/projects/agpcr_nom/human_31/indexing_files_s2_dsp\",\n",
    "                    #offset = fasta_offsets[i]-gain.start+1,\n",
    "                    split_mode='double')\n",
    "        \n",
    "        print(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for gain in human_collection.collection:\n",
    "    gain.GPS.info()\n",
    "    #for res in gain.GPS.indices:\n",
    "    print(np.where(gain.alignment_indices == gps_minus_one))\n",
    "    #for i in range(len(gain.alignment_indices)):\n",
    "    #    print(gain.alignment_indices[i], gain.sequence[i])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
