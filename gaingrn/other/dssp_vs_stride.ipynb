{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is dssp_analysis.ipynb, a notebook for determining whether DSSP or STRIDE is better to be used for assessing the segment / secondary structure composition of GAIN domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a database from a dataset for nomenclating\n",
    "# INPUT: a collection of GAIN domain PDBs, their sequences as one large \".fa\" file\n",
    "from gaingrn.utils.gain_classes import GainDomain, GainCollection, Anchors, GPS\n",
    "import numpy as np\n",
    "import glob\n",
    "#import multiprocessing as mp\n",
    "#from subprocess import Popen, PIPE\n",
    "from matplotlib import pyplot as plt\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dssp(file):\n",
    "    '''\n",
    "    Similar to sse_func.read_sse_asg, parse the file and return the sse_sequence list\n",
    "    Parameters:\n",
    "        file : str, required\n",
    "            DSSP file to be read (for now, GUILLEs pre-compiled version)\n",
    "    Returns:\n",
    "        sse_sequence : list\n",
    "            LIST containing a sequence of all letters assigned to the residues\n",
    "    '''\n",
    "    sse_list = []\n",
    "    with open(file) as f:\n",
    "        for l in f.readlines():\n",
    "                items = l.split()\n",
    "                if len(items) > 1:\n",
    "                    one_letter_sse = items[1]\n",
    "                else:\n",
    "                    one_letter_sse = ''\n",
    "                sse_list.append(one_letter_sse)\n",
    "    return sse_list\n",
    "\n",
    "def build_dssp_dict(file):\n",
    "    # Build a more rudimentary SSE dictionary based on the SSE sequence from read_dssp:\n",
    "    def move2dict(sse_dict, dssp_keys, element, sse_tuple):\n",
    "        if element not in dssp_keys.keys():\n",
    "            #print(f\"Skipping element with DSSP assignment \\\"{element}\\\" \")\n",
    "            return sse_dict\n",
    "        sse_id = dssp_keys[element]\n",
    "        if sse_id not in sse_dict.keys():\n",
    "            sse_dict[sse_id] = [sse_tuple]\n",
    "        else:\n",
    "            sse_dict[sse_id].append(sse_tuple)\n",
    "        return sse_dict\n",
    "    \n",
    "    with open(file) as f:\n",
    "        first_index = int(f.read().split()[0][1:]) # the integer value of the first index\n",
    "        \n",
    "    sse_list = read_dssp(file)             \n",
    "    dssp_keys = { \"H\" : \"AlphaHelix\",\n",
    "                     \"B\" : \"Bridge\",\n",
    "                     \"E\" : \"Strand\",\n",
    "                     \"G\" : \"310Helix\",\n",
    "                     \"I\" : \"5Helix\",\n",
    "                     \"T\" : \"Turn\",\n",
    "                     \"S\" : \"Bend\"     }\n",
    "    sse_dict = {}\n",
    "    # Parse through the entries one by one and construc tuples (first_res, last_res)\n",
    "    # pass them into the dictionary with the corresponding key\n",
    "    stored_element = ''\n",
    "    within_element = False\n",
    "    for idx, assignment in enumerate(sse_list):\n",
    "        if assignment != stored_element:\n",
    "            if within_element:\n",
    "                last = idx + first_index\n",
    "                within_element = False\n",
    "                # Move the tuple into the dictionary\n",
    "                sse_dict = move2dict(sse_dict, dssp_keys, stored_element, (first,last))\n",
    "            if not within_element:\n",
    "                if assignment == \"\":\n",
    "                    stored_element = \"\"\n",
    "                    continue\n",
    "                first = idx + first_index\n",
    "                stored_element = assignment\n",
    "                within_element = True\n",
    "    return sse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dssp = '/home/hildilab/projects/agpcr_nom/dssp4/test.dssp'\n",
    "#print(build_dssp_dict(test_dssp))\n",
    "import gaingrn.utils.structure_utils\n",
    "\n",
    "\n",
    "strides = glob.glob('/home/hildilab/projects/agpcr_nom/human_31_0/sse/*.stride')\n",
    "# COMPARE LISTS OF STRIDE AGAINST DSSP\n",
    "for f in strides:\n",
    "    print(\"\\n\", f)\n",
    "    dssp_list = read_dssp(f.replace(\".stride\",\"_gain.dssp\"))\n",
    "    with open(f.replace(\".stride\",\"_gain.dssp\")) as g:\n",
    "        data = g.read()\n",
    "        #print(data.split(\"\\n\"))\n",
    "        first_index = int(data.split()[0][1:]) # the integer value of the first index\n",
    "        last_index = int(data.split(\"\\n\")[-2].split(\"\\t\")[0][1:])#.split()[1:]\n",
    "        #print(last_index)\n",
    "        \n",
    "    stride_list, _ = gaingrn.utils.io.read_sse_asg(f)\n",
    "    dssp_dict = build_dssp_dict(f.replace(\".stride\",\"_gain.dssp\"))\n",
    "    stride_dict = gaingrn.utils.structure_utils.cut_sse_dict(first_index, last_index, sse_func.read_sse_loc(f))\n",
    "    for ki in dssp_dict.keys():\n",
    "        if ki in stride_dict.keys():\n",
    "            print(ki)\n",
    "            \"\"\"            for i, item in enumerate(dssp_dict[ki]):\n",
    "                try: \n",
    "                    print(item,\"\\t\\t\", stride_dict[ki][i])\n",
    "                except:\n",
    "                    print(item)\"\"\"\n",
    "            \n",
    "            print(\"DSSP\\t\", dssp_dict[ki])\n",
    "            print(\"STRIDE\\t\",stride_dict[ki])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seqs = gaingrn.utils.io.read_multi_seq(\"/home/hildilab/projects/agpcr_nom/app_gain_gain.fa\")\n",
    "print(len(valid_seqs))\n",
    "quality_file = \"/home/hildilab/projects/agpcr_nom/app_gain_gain.mafft.jal\"\n",
    "alignment_file = \"/home/hildilab/projects/agpcr_nom/app_gain_gain.mafft.fa\"\n",
    "stride_folder = \"/home/hildilab/projects/agpcr_nom/all_gps_stride\"\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2/*\")\n",
    "quality = gaingrn.utils.io.read_quality(quality_file)\n",
    "gps_minus_one = 6781 # -1 of the ACTUAL COLUMN (6782) in JALVIEW since there is is ONE-INDEXED\n",
    "aln_cutoff = 6826 # \n",
    "alignment_dict = gaingrn.utils.io.read_alignment(alignment_file, aln_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_collection = GainCollection(  alignment_file = alignment_file,\n",
    "                                    aln_cutoff = aln_cutoff,\n",
    "                                    quality = quality,\n",
    "                                    gps_index = gps_minus_one,\n",
    "                                    stride_files = stride_files,\n",
    "                                    sequence_files=None,\n",
    "                                    sequences=valid_seqs,\n",
    "                                    alignment_dict = alignment_dict,\n",
    "                                    is_truncated = True,\n",
    "                                    stride_outlier_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "root_path = \"/home/hildilab/projects/agpcr_nom/\"\n",
    "profile_path = root_path+\"all_gps_profiles/\"\n",
    "pdb_list = glob.glob(f\"{root_path}all_gps*/batch*/*rank_1_*.pdb\")\n",
    "print(len(pdb_list))\n",
    "#valid_seqs\n",
    "target_dir = root_path+\"human/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_seqs = [\"Q9HBW9\",\"O60241\",\"Q6QNK2\",\"Q9UHX3\",\"Q5T601\",\"Q96PE1\",\"O60242\",\"Q86SQ4\",\n",
    "                \"O94910\",\"Q8IWK6\",\"Q8IZP9\",\"Q8WXG9\",\"Q86Y34\",\"O95490\",\"Q14246\",\"Q9BY15\",\n",
    "                \"Q8IZF2\",\"Q86SQ3\",\"Q8IZF6\",\"Q96K78\",\"Q8IZF3\",\"Q8IZF7\",\"Q8IZF5\",\"Q7Z7M1\",\n",
    "                \"Q8IZF4\",\"Q9HCU4\",\"Q9NYQ6\",\"Q9NYQ7\",\"Q9HAR2\",\"O14514\",\"P48960\",\n",
    "                \"Q9Y653\"]\n",
    "sigma_2_strides = glob.glob(\"sigma_2*/*.stride\")\n",
    "list_32 = gaingrn.utils.io.filter_by_list(valid_seqs, human_seqs)\n",
    "\n",
    "human_collection = GainCollection( alignment_file = alignment_file,\n",
    "                                        aln_cutoff = aln_cutoff,\n",
    "                                        quality = quality,\n",
    "                                        gps_index = gps_minus_one,\n",
    "                                        stride_files =  sigma_2_strides, #stride_files,\n",
    "                                        sequence_files=None,\n",
    "                                        sequences=list_32,\n",
    "                                        alignment_dict = alignment_dict,\n",
    "                                        is_truncated = True,\n",
    "                                        stride_outlier_mode = True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_collection.plot_sse_hist(title=f\"Receptor group: HUMAN_31 (Total: 31)\",\n",
    "                                   n_max=16,\n",
    "                                   savename=\"human_31.s2_newAnch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(valid_collection)\n",
    "occupancy = np.zeros([aln_cutoff],dtype=int)\n",
    "sse_matrix = np.zeros([len(valid_collection.collection), aln_cutoff])\n",
    "for i, gain in enumerate(valid_collection.collection):\n",
    "    #print(gain.sda_helices, gain.sdb_sheets)\n",
    "#    for res_id in range(gain.start,gain.end+1):\n",
    "    occupancy[gain.alignment_indices] += 1\n",
    "    \n",
    "    for helix in gain.sda_helices:\n",
    "        for res_id in range(helix[0],helix[1]+1):\n",
    "            sse_matrix[i,gain.alignment_indices[res_id]] = -1\n",
    "    for sheet in gain.sdb_sheets:\n",
    "        for res_id in range(sheet[0],sheet[1]+1):\n",
    "            sse_matrix[i,gain.alignment_indices[res_id]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors , anchor_occupation = valid_collection.find_anchors(cutoff=3000)\n",
    "print(anchors)\n",
    "for i, anchor in enumerate(anchors):\n",
    "    print(i, anchor)\n",
    "    if anchor < valid_collection.alignment_subdomain_boundary: \n",
    "        color = u'#1f77b4'\n",
    "    else: \n",
    "        color = u'#ff7f0e'\n",
    "    plt.scatter(anchor, valid_collection.anchor_hist[anchor]+1000, c=color, marker=\"1\",s=60)\n",
    "print(anchors, anchor_occupation)\n",
    "print(valid_collection.alignment_subdomain_boundary)\n",
    "anchor_dict = gaingrn.utils.alignment_utils.make_anchor_dict(anchors, valid_collection.alignment_subdomain_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompareIndexing:\n",
    "    # A modified Indexing class similar to the indexing_classes.py Indexing, however specific for DSSP data integration.\n",
    "    def __init__(self, aGainCollection, fasta_offsets=None, split_mode='single'):\n",
    "        \n",
    "        length = len(aGainCollection.collection)\n",
    "        names = np.empty([length], dtype=object)\n",
    "        indexing_dirs = np.empty([length], dtype=object)\n",
    "        center_dirs = np.empty([length], dtype=object)\n",
    "        offsets = np.zeros([length], dtype=int)\n",
    "        total_keys = []\n",
    "        center_keys = []\n",
    "        if fasta_offsets is None:\n",
    "            self.fasta_offsets = np.zeros([length])\n",
    "        if fasta_offsets is not None: \n",
    "            corrected_offsets = []\n",
    "            for i in range(length):\n",
    "                # The existing FASTA offsets do not account for the residue starting not at 0,\n",
    "                # Therefore the value of the starting res (gain.start) needs to be subtracted.\n",
    "                corrected_offsets.append(fasta_offsets[i]-aGainCollection.collection[i].start)\n",
    "            self.fasta_offsets = np.array(corrected_offsets, dtype=int)\n",
    "            \n",
    "        for gain_index, gain in enumerate(aGainCollection.collection):\n",
    "            indexing_dir, indexing_centers = gain.create_indexing(anchors, \n",
    "                                                                  anchor_occupation, \n",
    "                                                                  anchor_dict,\n",
    "                                                                  split_mode=split_mode)\n",
    "            print(indexing_dir, indexing_centers)\n",
    "            for key in indexing_dir.keys():\n",
    "                if key not in total_keys:\n",
    "                    total_keys.append(key)\n",
    "                    \n",
    "            for key in indexing_centers.keys():\n",
    "                if key not in center_keys:\n",
    "                    center_keys.append(key)                 \n",
    "                \n",
    "            indexing_dirs[gain_index] = indexing_dir\n",
    "            center_dirs[gain_index] = indexing_centers\n",
    "            # Patch ADGRC/CELSR naming\n",
    "            names[gain_index] = gain.name.replace(\"CadherinEGFLAGseven-passG-typereceptor\", \"AGRC\")\n",
    "            offsets[gain_index] = gain.start\n",
    "\n",
    "        self.indexing_dirs = indexing_dirs\n",
    "        self.center_dirs = center_dirs\n",
    "        self.names = names\n",
    "        self.length = length\n",
    "        self.offsets = offsets\n",
    "        self.accessions = [gain.name.split(\"-\")[0].split(\"_\")[0] for gain in aGainCollection.collection]\n",
    "        self.sequences = [\"\".join(gain.sequence) for gain in aGainCollection.collection]\n",
    "        self.total_keys = sorted(total_keys)\n",
    "        self.center_keys = sorted(center_keys)\n",
    "        \n",
    "        print(\"Total of keys found in the dictionaries:\\n\", self.total_keys, self.center_keys)\n",
    "        print(\"First entry\", self.indexing_dirs[0], self.center_dirs[0])\n",
    "        \n",
    "        header_list = [\"Receptor\", \"Accession\"] + self.total_keys + self.center_keys\n",
    "        #header = \"Receptor,Accession,\" + \",\".join(self.total_keys) + \",\".join(self.center_keys)\n",
    "        header_list = [\"Receptor\", \"Accession\"] + self.total_keys + self.center_keys\n",
    "        header_dict = {}\n",
    "        for idx, item in enumerate(header_list):\n",
    "            header_dict[item] = idx\n",
    "\n",
    "        data_matrix = np.full([self.length, len(header_dict.keys())], fill_value='', dtype=object)\n",
    "        # Go through each of the sub-dictionaries and populate the dataframe:\n",
    "        for row in range(self.length):\n",
    "                # Q5T601_Q5KU15_..._Q9H615-AGRF1_HUMAN-AGRF1-Homo_sapiens.fa\n",
    "                # 0                        1           2     3\n",
    "            name_parts = self.names[row].split(\"-\")\n",
    "            data_matrix[row, header_dict[\"Receptor\"]] = name_parts[2]\n",
    "            data_matrix[row, header_dict[\"Accession\"]] = name_parts[0].split(\"_\")[0]\n",
    "            offset = self.offsets[row]\n",
    "            fa_offset = self.fasta_offsets[row]\n",
    "\n",
    "            for key in self.indexing_dirs[row].keys():\n",
    "                if key == \"GPS\":\n",
    "                    sse=[int(x+fa_offset) for x in self.indexing_dirs[row][key]]\n",
    "                    data_matrix[row, header_dict[key]] = f\"{sse[0]}-{sse[-1]}\"\n",
    "                else:\n",
    "                    sse = [int(x+offset+fa_offset) for x in self.indexing_dirs[row][key]]\n",
    "                    data_matrix[row, header_dict[key]] = f\"{sse[0]}-{sse[1]}\"\n",
    "\n",
    "            for key in self.center_dirs[row].keys():\n",
    "                data_matrix[row, header_dict[key]] = str(self.center_dirs[row][key]+offset+fa_offset)\n",
    "            \n",
    "            self.data_header = \",\".join(header_list)\n",
    "            self.data_matrix = data_matrix\n",
    "\n",
    "    def data2csv(self, outfile):\n",
    "        with open(outfile, \"w\") as f:\n",
    "            f.write(self.data_header+\"\\n\")\n",
    "            for row in range(self.length):\n",
    "                f.write(\",\".join(self.data_matrix[row,:])+\"\\n\")\n",
    "        print(\"Completed file\", outfile, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_base = CompareIndexing(human_collection, split_mode='double')\n",
    "\n",
    "big_seq_file = \"../data/agpcr_celsr.fasta\"\n",
    "fasta_offsets = gaingrn.utils.alignment_utils.find_offsets(big_seq_file, \n",
    "                                 human_base.accessions, \n",
    "                                 human_base.sequences)\n",
    "\n",
    "fa_human_base = CompareIndexing(human_collection, fasta_offsets = fasta_offsets, split_mode='double')\n",
    "\n",
    "human_base.data2csv(f\"default_indexed_s2_s2a_re_double_split.csv\")\n",
    "fa_human_base.data2csv(f\"uniprot_indexed_s2_s2a_re_double_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gain in enumerate(human_collection.collection):\n",
    "    #if \"Q6QNK2\" in gain.name:\n",
    "        x1, x2 = gain.create_indexing(anchors, anchor_occupation, anchor_dict, \n",
    "                    outdir = \"/home/hildilab/projects/agpcr_nom/human_31/indexing_files_s2_dsp\",\n",
    "                    #offset = fasta_offsets[i]-gain.start+1,\n",
    "                    split_mode='double')\n",
    "        \n",
    "        print(x1, x2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
