{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Assign GAIN-GRN\n",
    "#### This is a notebook to construct the complete GAIN GRN indexing on a collection of GAIN domains.\n",
    "\n",
    "The completed indexing objects are present in the ../data folder as stal_indexing.pkl, human_indexing.pkl and pkd_indexing.pkl, respectively.\n",
    "\n",
    "Requirements:\n",
    "> - GESAMT binary\n",
    "> - STRIDE set of files (one for each entry in the dataset, here we use float-modified STRIDE files for the outliers)\n",
    "> - A Folder of template PDBs\n",
    "> - template_data.json with all information about the template elements and centers\n",
    "\n",
    "The main challenge in constructing a good indexing on each GAIN-Domain is the detection of the start and end of each segment. Often, segments are continuously indexed as Helix or Strand, despite it being two actual segments (i.e. a kink between Helix 4,5 and 6, but all residues are in a single helical segment).\n",
    "\n",
    "*To run this notebook, you will need to download all PDB models and the GAIN-GRN data from the zenodo repository:* \n",
    "\n",
    "https://dx.doi.org/10.5281/zenodo.12515545/gaingrn_data.tgz : download via `gaingrn.scripts.io.download_data()` into `path/to/GAIN-GRN/data`\n",
    "\n",
    "https://dx.doi.org/10.5281/zenodo.12515545/agpcr_gains.tgz : download via `gaingrn.scripts.io.download_pdbs(target_directory=my/dir/to/pdbs)` and specify the `PDB_DIR` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle \n",
    "\n",
    "# LOCAL IMPORTS\n",
    "import gaingrn.scripts.io\n",
    "import gaingrn.scripts.assign\n",
    "import gaingrn.scripts.alignment_utils\n",
    "import gaingrn.scripts.bb_angle_tools\n",
    "import gaingrn.scripts.indexing_utils\n",
    "from gaingrn.scripts.indexing_classes import StAlIndexing\n",
    "\n",
    "\n",
    "try: \n",
    "    GESAMT_BIN = os.environ.get('GESAMT_BIN')\n",
    "except:\n",
    "    GESAMT_BIN = \"/home/hildilab/lib/xtal/ccp4-8.0/ccp4-8.0/bin/gesamt\"\n",
    "if GESAMT_BIN is None:\n",
    "    GESAMT_BIN = \"/home/hildilab/lib/xtal/ccp4-8.0/ccp4-8.0/bin/gesamt\"\n",
    "\n",
    "PDB_DIR = \"../../all_pdbs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle the issue of a very broad assignment of some elements, and to help with splitting them into their respective elements, there is a two-fold modification of STRIDE files in place:\n",
    "\n",
    "1. Every residue outside of 2 Sigmas of the mean (keep in mind, this is circular statistics) gets assigned a lower-case letter as the SSE descriptor, enabling resolving element ambiguities\n",
    "2. The multiple of sigmas for outliers is written into columns 66-70 of the stride file. If this exceeds a defines threshold (usually 5.0), the element is truncated here always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is already done in your set, just for further reference.\n",
    "\n",
    "# import gaingrn.scripts.bb_angle_tools\n",
    "# gaingrn.scripts.bb_angle_tools.stride_file_processing(stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2/*\"), outfolder = \"../data/gain_strides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the GainCollection objects to be indexed. Here, we have the whole 14435 structure set (valid_collection) and the 31 structure set (human_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_collection = pd.read_pickle(\"../data/valid_collection.pkl\")\n",
    "human_collection = pd.read_pickle(\"../data/human_collection.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For testing, in this cell an individual indexing can be constructed. \n",
    "\n",
    "We have implemented a heirarchy of \"split\" modes, which will disambiguate continuous SSE where multiple segment centers are contained --> **split_modes**\n",
    "\n",
    "Setting _debug=True_ will result in a large amount of information being printed, enabling the tracing of errors and irregularities during the assignment process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a Uniprot identifier here.\n",
    "uniprot = \"Q8IZF6\"\n",
    "\n",
    "for i, gain in enumerate(human_collection.collection):\n",
    "    if uniprot not in gain.name: \n",
    "        continue\n",
    "    file_prefix = f\"../test_stal_indexing/f3_test{gain.name}\" # a temp folder where calculations and outputs will be stored.\n",
    "    print(\"_\"*30, f\"\\n{i} {gain.name}\")\n",
    "    element_intervals, element_centers, residue_labels, unindexed_elements, params = gaingrn.scripts.assign.assign_indexing(gain, \n",
    "                                                                                                file_prefix=file_prefix, \n",
    "                                                                                                gain_pdb=gaingrn.scripts.io.find_pdb(gain.name, PDB_DIR), \n",
    "                                                                                                template_dir='../data/template_pdbs/',\n",
    "                                                                                                template_json='../data/template_data.json',\n",
    "                                                                                                outlier_cutoff=5.0,\n",
    "                                                                                                gesamt_bin=GESAMT_BIN,\n",
    "                                                                                                debug=False, # If you want ALL that is happening\n",
    "                                                                                                create_pdb=False,\n",
    "                                                                                                hard_cut={\"S2\":7,\"S7\":3,\"H5\":3},\n",
    "                                                                                                patch_gps=True\n",
    "                                                                                                )\n",
    "    # This dictionary denotes the priority line of splitting. Only the lowest-heirarchy split is indicated, i.e. the higher the number, the \"worse\" the split.\n",
    "    split_modes = {\n",
    "        0:\"No Split.\",\n",
    "        1:\"Split by coiled residue.\",\n",
    "        2:\"Split by disordered residue.\",\n",
    "        3:\"Split by Proline/Glycine\",\n",
    "        4:\"Split by hard cut.\",\n",
    "        5:\"Overwrite by anchor priority.\"\n",
    "    }\n",
    "    print(gain.name, gain.subdomain_boundary)\n",
    "    if params[\"split_mode\"] > 0:\n",
    "        print(params[\"split_mode\"], split_modes[params[\"split_mode\"]])\n",
    "    #print(element_intervals, element_centers, residue_labels, unindexed_elements, sep=\"\\n\")\n",
    "    print(unindexed_elements, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, the full __StAlIndexing__ may be constructed test-wise, or by default the pickle of the Indexing is loaded. \n",
    "Keep in mind that within this jupyter notebook - due to its handling of multiprocessig.Pool - the number of threads is limited to 1 and this takes a while for the full set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessions = [gain.name.split(\"-\")[0].split(\"_\")[0] for gain in valid_collection.collection]\n",
    "sequences = [\"\".join(gain.sequence) for gain in valid_collection.collection]\n",
    "\n",
    "fasta_offsets = gaingrn.scripts.alignment_utils.find_offsets(\"../data/seq_aln/all_query_sequences.fasta\",\n",
    "                                 accessions, \n",
    "                                 sequences)\n",
    "\n",
    "# Pseudocenter cases: cases, where the segment center is NOT part of the segment in question. Therefore, an alternative indexing method is applied,\n",
    "# using a \"pseudocenter\", a residue which matches the segment, but is not the .50 residue.\n",
    "ps_file = \"../data/pseudocenters.csv\"\n",
    "open(ps_file,\"w\").write(f\"GAIN,res,elem\\n\")\n",
    "\n",
    "# Careful when running, this takes a lot of time to calculate on single-thread. use run_indexing.py for fast multithreaded calculation.\n",
    "stal_indexing = StAlIndexing(valid_collection.collection, \n",
    "                             prefix=\"../test_stal_indexing/test20\", \n",
    "                             pdb_dir=f'{PDB_DIR}/',\n",
    "                             template_json='../data/template_data.json',\n",
    "                             gesamt_bin=GESAMT_BIN, \n",
    "                             template_dir='../data/template_pdbs/', \n",
    "                             fasta_offsets=fasta_offsets,\n",
    "                             n_threads=1,\n",
    "                             #pseudocenters=ps_file,\n",
    "                             debug=False)\n",
    "#with open(\"../data/stal_indexing.pkl\",\"wb\") as save:\n",
    "#    pickle.dump(stal_indexing, save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a file contaning all segment starts, ends and center residues for each GAIN domain. This is the basis for the GPCRdb implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-calculated STAL indexing, which saves some time.\n",
    "stal_indexing = pd.read_pickle(\"../data/stal_indexing.pkl\")\n",
    "\n",
    "header, matrix = stal_indexing.construct_data_matrix(unique_sse=False)\n",
    "stal_indexing.data2csv(header, matrix, \"../data/gaingrn_indexing.csv\")\n",
    "header, matrix = stal_indexing.construct_data_matrix(unique_sse=True)\n",
    "stal_indexing.data2csv(header, matrix, \"../data/gaingrn_indexing.unique.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we construct the indexing for the human set with the modified STRIDE files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_collection = pd.read_pickle(\"../data/human_collection.pkl\")\n",
    "\n",
    "human_accessions = [gain.name.split(\"-\")[0].split(\"_\")[0] for gain in human_collection.collection]\n",
    "human_sequences = [\"\".join(gain.sequence) for gain in human_collection.collection]\n",
    "\n",
    "human_fasta_offsets = gaingrn.scripts.alignment_utils.find_offsets(\"../data/seq_aln/all_query_sequences.fasta\", \n",
    "                                 human_accessions, \n",
    "                                 human_sequences)\n",
    "\n",
    "stal_human_indexing  = StAlIndexing(human_collection.collection, \n",
    "                             prefix=\"../../test_stal_indexing/test\", \n",
    "                             pdb_dir=f'{PDB_DIR}/',  \n",
    "                             template_dir='../data/template_pdbs/', \n",
    "                             template_json = '../data/template_data.json',\n",
    "                             outlier_cutoff=5.0,\n",
    "                             fasta_offsets=human_fasta_offsets,\n",
    "                             gesamt_bin=GESAMT_BIN,\n",
    "                             n_threads=1,\n",
    "                             debug=False)\n",
    "\n",
    "with open(\"../data/human_indexing.pkl\", \"wb\") as humanfile:\n",
    "    pickle.dump(stal_human_indexing, humanfile, -1)\n",
    "\n",
    "header, matrix = stal_human_indexing.construct_data_matrix(overwrite_gps=True, unique_sse=False)\n",
    "stal_human_indexing.data2csv(header, matrix, \"../data/human_indexing.csv\")\n",
    "\n",
    "# Also include unique Helices in a separate file.\n",
    "header, matrix = stal_human_indexing.construct_data_matrix(overwrite_gps=True, unique_sse=True)\n",
    "stal_human_indexing.data2csv(header, matrix, \"../data/human_indexing.unique.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../data/human_indexing.pkl\", \"wb\") as humanfile:\n",
    "    pickle.dump(stal_human_indexing, humanfile, -1)\n",
    "\n",
    "header, matrix = stal_human_indexing.construct_data_matrix(overwrite_gps=True, unique_sse=False)\n",
    "stal_human_indexing.data2csv(header, matrix, \"../data/human_indexing.csv\")\n",
    "\n",
    "# Also include unique Helices in a separate file.\n",
    "header, matrix = stal_human_indexing.construct_data_matrix(overwrite_gps=True, unique_sse=True)\n",
    "stal_human_indexing.data2csv(header, matrix, \"../data/human_indexing.unique.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Indexing for the whole set is best constructed via multithreaded exection by _stal\\_indexing.py_ and saved in a pickle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
