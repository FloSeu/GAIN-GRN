{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "import glob\n",
    "#from shutil import copyfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import logomaker\n",
    "from scipy import stats\n",
    "# LOCAL IMPORTS\n",
    "#from indexing_classes import GPCRDBIndexing\n",
    "import sse_func\n",
    "import matplotlib.pyplot as plt\n",
    "import template_finder as tf\n",
    "import glob\n",
    "import pickle\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, FixedLocator)\n",
    "# LOCAL IMPORTS\n",
    "from gain_classes import GainDomain, GainCollection, Anchors, GPS\n",
    "from indexing_classes import StAlIndexing\n",
    "\n",
    "gesamt_bin = \"/home/hildilab/lib/xtal/ccp4-8.0/ccp4-8.0/bin/gesamt\"\n",
    "\n",
    "def find_pdb(name, pdb_folder):\n",
    "    identifier = name.split(\"-\")[0]\n",
    "    target_pdb = glob.glob(f\"{pdb_folder}/*{identifier}*.pdb\")[0]\n",
    "    return target_pdb\n",
    "\n",
    "def find_offsets(fasta_file, accessions, sequences):\n",
    "    # searches through the accessions in the big sequence file,\n",
    "    # finds the start for the provided sequence\n",
    "    with open(fasta_file,\"r\") as fa:\n",
    "        fa_data = fa.read()\n",
    "        fasta_entries = fa_data.split(\">\")\n",
    "    seqs = []\n",
    "    headers = []\n",
    "    offsets = []\n",
    "    for seq in fasta_entries:\n",
    "        # Fallback for too short sequences\n",
    "        if len(seq) < 10: \n",
    "            continue\n",
    "        data = seq.strip().split(\"\\n\")\n",
    "        headers.append(data[0].split(\"|\")[1]) # This is only the UniProtKB Accession Number and will be matched EXACTLY\n",
    "        seqs.append(\"\".join(data[1:]))\n",
    "    \n",
    "    heads = np.array(headers)\n",
    "    for idx, accession in enumerate(accessions):\n",
    "        seq_idx = np.where(heads == accession)[0][0]\n",
    "        offset = sse_func.find_the_start(seqs[seq_idx], sequences[idx])\n",
    "        #print(offset)\n",
    "        offsets.append(offset)\n",
    "    \n",
    "    return offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_collection = pd.read_pickle(\"valid_collection.pkl\")\n",
    "human_collection = pd.read_pickle(\"../human_collection.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gain in enumerate(human_collection.collection):\n",
    "    file_prefix = f\"../test_stal_indexing/test_single_{i}\"\n",
    "    print(\"_\"*30, f\"\\n{i} {gain.name}\")\n",
    "    print(gain.sse_dict)\n",
    "    #for i, hel in enumerate(gain.sda_helices):\n",
    "    #    print(f\"H#{i}: {hel[0]+gain.start}-{hel[1]+gain.start}\")\n",
    "    #for i, hel in enumerate(gain.sdb_sheets):\n",
    "    #    print(f\"S#{i}: {hel[0]+gain.start}-{hel[1]+gain.start}\")\n",
    "    element_intervals, element_centers, residue_labels, unindexed_elements, params = tf.assign_indexing(gain, \n",
    "                                                                                                file_prefix=file_prefix, \n",
    "                                                                                                gain_pdb=find_pdb(gain.name, '../all_pdbs'), \n",
    "                                                                                                template_dir='../r2_template_pdbs/',\n",
    "                                                                                                template_json='template_data.json',\n",
    "                                                                                                gesamt_bin=gesamt_bin,\n",
    "                                                                                                debug=True, \n",
    "                                                                                                create_pdb=True,\n",
    "                                                                                                hard_cut={\"S2\":7,\"S6\":3,\"H5\":3},\n",
    "                                                                                                patch_gps=True\n",
    "                                                                                                )\n",
    "    #print(\"ELEMENT CENTERS = \", element_centers)\n",
    "    #split_modes = {\n",
    "    #    0:\"No Split.\",\n",
    "    #    1:\"Split by coiled residue.\",\n",
    "    #    2:\"Split by disordered residue.\",\n",
    "    #    3:\"Split by Proline/Glycine\",\n",
    "    #    4:\"Split by hard cut.\",\n",
    "    #    5:\"Overwrite by anchor priority.\"\n",
    "    #}\n",
    "    #print(gain.name, gain.subdomain_boundary)\n",
    "    #if params[\"split_mode\"] > 0:\n",
    "    #    print(params[\"split_mode\"], split_modes[params[\"split_mode\"]])\n",
    "    #print(element_intervals, element_centers, residue_labels, unindexed_elements, sep=\"\\n\")\n",
    "schwqabbel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = gain.name.split('-')[0]\n",
    "\n",
    "out = tf.run_command(f\"find ../sigmas/sigma_2/ -name \\\"*{identifier}*stride\\\"\")\n",
    "print(\"subl\", out)\n",
    "print(\"pymol\", find_pdb(identifier, '../all_pdbs'))\n",
    "print(gain.sequence[717-gain.start:725-gain.start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(element_intervals, element_centers, residue_labels, unindexed_elements, params, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#offset information\n",
    "\n",
    "\n",
    "\"\"\"all_accessions = [gain.name.split(\"-\")[0].split(\"_\")[0] for gain in valid_collection.collection]\n",
    "all_sequences = [\"\".join(gain.sequence) for gain in valid_collection.collection]\n",
    "\n",
    "fasta_offsets = find_offsets(\"/home/hildilab/projects/GPS_massif/uniprot_query/agpcr_celsr.fasta\", \n",
    "                                 all_accessions, \n",
    "                                 all_sequences)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"stal_indexing = StAlIndexing(valid_collection.collection[:10], \n",
    "                             prefix=\"../test_stal_indexing/test\", \n",
    "                             pdb_dir='../all_pdbs/',  \n",
    "                             template_dir='../r2_template_pdbs/', \n",
    "                             fasta_offsets=fasta_offsets,\n",
    "                             n_threads=1,\n",
    "                             debug=False)\"\"\"\n",
    "\n",
    "\"\"\"stal_indexing = StAlIndexing(valid_collection.collection[200:300], \n",
    "                             prefix=\"../test_stal_indexing/t_class\", \n",
    "                             pdb_dir='../all_pdbs/',  \n",
    "                             template_dir='../r2_template_pdbs/',\n",
    "                             n_threads=6,\n",
    "                             debug=False)\"\"\"\n",
    "\"\"\"import pickle\n",
    "print(stal_indexing.indexing_dirs[0], stal_indexing.center_dirs[0], stal_indexing.intervals[0], sep=\"\\n\")\n",
    "header, matrix = stal_indexing.construct_data_matrix(unique_sse=False)\n",
    "\n",
    "\n",
    "with open(\"stal_indexing.pkl\",\"wb\") as save:\n",
    "    pickle.dump(stal_indexing, save)\"\"\"\n",
    "\n",
    "\n",
    "stal_indexing = pd.read_pickle(\"stal_indexing.pkl\")\n",
    "#header, matrix = stal_indexing.construct_data_matrix(unique_sse=False)\n",
    "#stal_indexing.data2csv(header, matrix, \"stal_indexing.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Analysis and Graphical statistics for StAl_indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Block for parsing information\n",
    "def get_loops(indexing_dir):\n",
    "    # Returns a named dict with loop lengths, i.e. {\"H1-H2\":13, \"H8-S1\":12}\n",
    "    inverted_dir = {sse[0] : (sse[1],ki) for ki, sse in indexing_dir.items()} # The begin of each sse is here {0:(13, \"H2\")}\n",
    "    loop_dir = {}\n",
    "    ordered_starts = sorted(inverted_dir.keys())\n",
    "    for i, sse_start in enumerate(ordered_starts):\n",
    "        if i == 0: \n",
    "            continue # Skip the first and go from the second SSE onwards, looking in N-terminal direction.\n",
    "        c_label = inverted_dir[sse_start][1]\n",
    "        n_end, n_label = inverted_dir[ordered_starts[i-1]]\n",
    "        loop_dir[f\"{n_label}-{c_label}\"] = sse_start - n_end - 1\n",
    "    return loop_dir\n",
    "\n",
    "def get_sse_len(indexing_dir, total_keys):\n",
    "    # Returns a dict with the length of each SSE in respective GAIN domain.\n",
    "    len_dir = {x:0 for x in total_keys}\n",
    "    for ki in indexing_dir.keys():\n",
    "        start = indexing_dir[ki][0]\n",
    "        end = indexing_dir[ki][1]\n",
    "        len_dir[ki] = end - start + 1\n",
    "    return len_dir\n",
    "\n",
    "def get_pos_res(pos_dir, gain):\n",
    "    # Returns a dict with the One-Letter-Code of each SSE position in the respective GAIN domain.\n",
    "    pos_res = {k : gain.sequence[v-gain.start] for k,v in pos_dir.items() if v is not None and v-gain.start < len(gain.sequence)}\n",
    "    return pos_res\n",
    "\n",
    "def match_dirs(single_dir, collection_dir, exclude=[]):\n",
    "    for k, v in single_dir.items():\n",
    "        if v in exclude:\n",
    "            continue\n",
    "        if k not in collection_dir.keys():\n",
    "            collection_dir[k] = [v]\n",
    "            continue\n",
    "        collection_dir[k].append(v)\n",
    "    return collection_dir\n",
    "\n",
    "def plot_hist(datarow, color, name, length):\n",
    "    max = np.max(datarow)\n",
    "    try: \n",
    "        dens = stats.gaussian_kde(datarow)\n",
    "    except:\n",
    "        print(np.unique(datarow))\n",
    "        return\n",
    "    fig = plt.figure(figsize=[4,2])\n",
    "    fig.set_facecolor('w')\n",
    "    n, x, _ = plt.hist(datarow, bins=np.linspace(0,max,max+1), histtype=u'step', density=True, color='white',alpha=0)\n",
    "    plt.plot(x, dens(x),linewidth=2,color=color,alpha=1)\n",
    "    plt.fill_between(x,dens(x), color=color,alpha=0.1)\n",
    "    ax = plt.gca()\n",
    "    ymax = ax.get_ylim()[1]\n",
    "    val_string = f'{round(np.average(datarow),2)}±{round(np.std(datarow),2)}'\n",
    "    plt.text(max, ymax*0.95, name, horizontalalignment='right', fontsize=14, verticalalignment='top')\n",
    "    plt.text(max, ymax*0.8, val_string, horizontalalignment='right', fontsize=14, verticalalignment='top')\n",
    "    plt.text(max, ymax*0.65, f\"{round(len(datarow)/length*100, 1)}%\", horizontalalignment='right', fontsize=14, verticalalignment='top')\n",
    "    plt.xlabel('Element Length [Residues]')\n",
    "    plt.ylabel('Relative density [AU]')\n",
    "    plt.savefig(f'{name}_hist.svg')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def parse_conservation(datarow, length):\n",
    "    total = len(datarow)\n",
    "    letters, counts = np.unique(np.array(datarow), return_counts=True)\n",
    "\n",
    "    resid_counts = {}\n",
    "    for i, res in enumerate(letters):\n",
    "            resid_counts[int(counts[i])] = res\n",
    "    \n",
    "    sorted_counts = sorted(resid_counts.keys())[::-1]\n",
    "\n",
    "    occupancy = round(total/length*100, 1)\n",
    "    conserv_string = []\n",
    "    residue_occupancies = [ int( x*100 / total ) for x in sorted_counts]\n",
    "    for idx, occ in enumerate(residue_occupancies):\n",
    "        if occ >= 5: conserv_string.append(f\"{resid_counts[sorted_counts[idx]]}:{occ}%\")\n",
    "\n",
    "    return occupancy, \", \".join(conserv_string)\n",
    "\n",
    "def construct_identifiers(intervals:dict, center_dir:dict, plddt_values:dict, max_id_dir:dict, name:str, seq=None, gain_start=0, debug=False):\n",
    "    id_dir = {}\n",
    "    plddts = {}\n",
    "    sse_seq = {}\n",
    "    if debug:\n",
    "        print(\"DEBUG\",f\"{len(plddt_values) = }\", f\"{len(seq) = }\", f\"{gain_start = }\", sep=\"\\n\\t\")\n",
    "    for sse in intervals.keys():\n",
    "        if sse == 'GPS' :\n",
    "            continue\n",
    "        start = intervals[sse][0]\n",
    "        end = intervals[sse][1]\n",
    "        if end-start > 45:\n",
    "            print(f\"NOTE: SKIPPDING TOO LONG SSE WITH LENGTH {end-start}\\n{name}: {sse}\")\n",
    "            continue\n",
    "        center_resid = center_dir[f\"{sse}.50\"]\n",
    "        first_resid = 50 - center_resid + start\n",
    "        for k in range(end-start+1):\n",
    "            if sse not in max_id_dir.keys():\n",
    "                max_id_dir[sse] = []\n",
    "            if first_resid+k not in max_id_dir[sse]:\n",
    "                max_id_dir[sse].append(first_resid+k)\n",
    "        id_dir[sse] = [first_resid+k for k in range(end-start+1)]\n",
    "        plddts[sse] = [plddt_values[k] for k in range(start, end+1)]\n",
    "        if seq is not None:\n",
    "            sse_seq[sse] = [seq[k-gain_start] for k in range(start, end+1) if k-gain_start<len(seq)]\n",
    "    if seq is None:\n",
    "        sse_seq = None\n",
    "    return max_id_dir, id_dir, plddts, sse_seq\n",
    "\n",
    "def get_plddt_dir(file='all_plddt.tsv'):\n",
    "    plddt_dir = {}\n",
    "    with open(file) as f:\n",
    "        data = [l.strip() for l in f.readlines()[1:]]\n",
    "        for l in data:\n",
    "            i,v  = tuple(l.split(\"\\t\"))\n",
    "            plddt_dir[i] = [float(val) for val in v.split(\",\")]\n",
    "    return plddt_dir\n",
    "\n",
    "def make_id_list(id_dir):\n",
    "    id_list = []\n",
    "    for sse in id_dir.keys():\n",
    "        for res in id_dir[sse]:\n",
    "            id_list.append(f\"{sse}.{res}\")\n",
    "    return id_list #np.array(id_list)\n",
    "\n",
    "def compact_label_positions(id_collection, plddt_collection, sse_keys, debug=False):\n",
    "    # Stacks label positions on one another\n",
    "    label_plddts = {}\n",
    "    for sse in sse_keys:\n",
    "        label_plddts[sse] = {}\n",
    "\n",
    "    for i in range(len(id_collection)):\n",
    "        gain_positions = id_collection[i]\n",
    "        plddt_positions = plddt_collection[i]\n",
    "        if debug: \n",
    "            print(i,gain_positions, plddt_positions, sep=\"\\n\")\n",
    "        for sse, v in gain_positions.items():\n",
    "            if v == []:\n",
    "                continue\n",
    "            for j, pos in enumerate(v):\n",
    "                pos = int(pos)\n",
    "                if j >= len(plddt_positions[sse]):\n",
    "                    continue\n",
    "                if pos not in label_plddts[sse].keys():\n",
    "                    label_plddts[sse][pos] = [plddt_positions[sse][j]]\n",
    "                else:\n",
    "                    label_plddts[sse][pos].append(plddt_positions[sse][j])\n",
    "\n",
    "    return label_plddts\n",
    "\n",
    "def construct_id_occupancy(indexing_dirs, center_dirs, length, plddt_dir, names, seqs, starts:list, debug=False):\n",
    "    newkeys = ['H1','H1.D1','H1.E1','H1.F4','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13']\n",
    "    id_collection = []\n",
    "    plddt_collection = []\n",
    "    seq_collection = []\n",
    "    all_id_dir = {x:[] for x in newkeys}\n",
    "    for k in range(length):\n",
    "        identifier = names[k].split(\"-\")[0]\n",
    "        plddt_values = plddt_dir[identifier]\n",
    "        all_id_dir, id_dir, plddts, sse_seq = construct_identifiers(indexing_dirs[k], center_dirs[k], plddt_values, all_id_dir, names[k], seqs[k], starts[k], debug=debug)\n",
    "        #print(k, sse_seq)\n",
    "        id_collection.append(id_dir)\n",
    "        #print(id_dir)\n",
    "        plddt_collection.append(plddts)\n",
    "        seq_collection.append(sse_seq)\n",
    "    print(\"Completed creating value collection.\")\n",
    "    print(id_collection[0])\n",
    "    print(plddt_collection[0])\n",
    "\n",
    "    # Here, parse through the id_dirs to count the occurrence of positions per SSE\n",
    "    # Dictionary to map any label identifier to a respective position.\n",
    "    id_map = {}\n",
    "    i = 0\n",
    "    for sse in newkeys:\n",
    "        for res in all_id_dir[sse]:\n",
    "            id_map[f'{sse}.{res}'] = i \n",
    "            i += 1\n",
    "    \n",
    "    max_id_list = []\n",
    "    for i, id_dict in enumerate(id_collection):\n",
    "        max_id_list.append(make_id_list(id_dict))\n",
    "    flat_id_list = np.array([item for sublist in max_id_list for item in sublist])\n",
    "    print(\"Finished constructing flat_id_list.\")\n",
    "    labels, occ = np.unique(flat_id_list, return_counts=True)\n",
    "    # Parse through labels, occ to generate the sse-specific data\n",
    "    occ_dict = {labels[u]:occ[u] for u in range(len(labels))}\n",
    "    # Transform occ_dict to the same format as label_plddts (one dict per sse):\n",
    "    label_occ = {}\n",
    "    for sse in newkeys:\n",
    "        label_occ[sse] = {int(k[-2:]):v for k,v in occ_dict.items() if sse in k}\n",
    "    #print(labels, occ)\n",
    "    label_plddts = compact_label_positions(id_collection, plddt_collection, newkeys, debug=debug)\n",
    "    label_seq = compact_label_positions(id_collection, seq_collection, newkeys, debug=debug)\n",
    "    #print(labels)\n",
    "    return label_plddts, label_occ, label_seq\n",
    "    #[print(k, len(v)) for k,v in label_plddts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCCUPANCY MATRIX\n",
    "\n",
    "#print(dir(stal_indexing))\n",
    "#newkeys = ['H1',,'H2','H3','H4','H5','H6','H7','H8','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13']\n",
    "newkeys = ['H1','H1.D1','H1.E1','H1.F4','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13']\n",
    "loop_lengths = {}\n",
    "sse_lengths = {}\n",
    "center_residues = {}\n",
    "sse_matrix = np.zeros(shape=(len(stal_indexing.total_keys),len(stal_indexing.total_keys)))\n",
    "for idx in range(stal_indexing.length):\n",
    "    #Sanity Check - Do the identifiers match? Yes, they do.\n",
    "    #if stal_indexing.names[idx].split(\"-\")[0] != valid_collection.collection[idx].name.split(\"-\")[0]:\n",
    "    #    print(stal_indexing.names[idx].split(\"-\")[0], valid_collection.collection[idx].name.split(\"-\")[0])\n",
    "    #    raise IndexError\n",
    "    loop_lengths = match_dirs(get_loops(stal_indexing.intervals[idx]), loop_lengths)\n",
    "    sse_lengths = match_dirs(get_sse_len(stal_indexing.intervals[idx], stal_indexing.total_keys), sse_lengths, exclude=[0])\n",
    "    center_res = match_dirs(get_pos_res(stal_indexing.center_dirs[idx], valid_collection.collection[idx]), center_residues)\n",
    "\n",
    "    present_sse = stal_indexing.intervals[idx].keys()\n",
    "    for i, kk in enumerate(newkeys):\n",
    "        for j in range(i,len(newkeys)):\n",
    "            if kk in present_sse and newkeys[j] in present_sse:\n",
    "                sse_matrix[j,i] += 1\n",
    "\n",
    "plt.imshow(sse_matrix, cmap='gist_yarg')\n",
    "plt.xticks(ticks= range(len(newkeys)), labels=newkeys, rotation=90)\n",
    "plt.yticks(ticks= range(len(newkeys)), labels=newkeys)\n",
    "plt.xlim(-0.5,18.5)\n",
    "plt.ylim(18.5,-0.5)\n",
    "cbar = plt.colorbar(shrink=0.5)\n",
    "plt.savefig(\"stal_occ_map_unique.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plddt_dir = get_plddt_dir('all_plddt.tsv')\n",
    "all_starts = [ gain.start for gain in valid_collection.collection ]\n",
    "#print(list(plddt_dir.keys())[:10])\n",
    "plddt_values, occ_values, label_seq = construct_id_occupancy(stal_indexing.intervals, \n",
    "                                                             stal_indexing.center_dirs, \n",
    "                                                             stal_indexing.length, \n",
    "                                                             plddt_dir, \n",
    "                                                             stal_indexing.names, \n",
    "                                                             stal_indexing.sequences,\n",
    "                                                             all_starts,\n",
    "                                                             debug=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the numerical statistics for each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sse in newkeys:\n",
    "    # Transform the values first\n",
    "    pp = plddt_values[sse]\n",
    "    #print(occ_values[sse])\n",
    "    av_pp = {k:np.average(np.array(v))/100 for k,v in pp.items()}\n",
    "    #print(av_pp)\n",
    "    norm_occ = {k:v/14435 for k,v in occ_values[sse].items()}\n",
    "    xax = sorted(av_pp.keys())\n",
    "    y_pp = [av_pp[x] for x in xax]\n",
    "    y_occ = [norm_occ[x] for x in xax]\n",
    "    norm_pp = np.array(y_pp)*np.array(y_occ)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[5,2])\n",
    "    fig.set_facecolor('w')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1)) #AutoMinorLocator())\n",
    "    ax.xaxis.set_major_locator(FixedLocator([a for a in range(2,100,3)]))#MultipleLocator(3)))\n",
    "    ax.tick_params(which='both', width=2)\n",
    "    ax.tick_params(which='major', length=8)\n",
    "    ax.tick_params(which='minor', length=6)\n",
    "    plt.bar(xax,y_pp, color='silver', alpha=0.7)\n",
    "    plt.plot(xax, y_occ, color='dodgerblue')\n",
    "    plt.bar(xax, norm_pp, color='xkcd:lightish red', alpha=0.1)\n",
    "    plt.title(f'Element Composition ({sse})')\n",
    "    plt.yticks(ticks = [0, 0.2, 0.4, 0.6, 0.8, 1], labels = ['0%', '20%', '40%', '60%', '80%', '100%'])\n",
    "    #plt.ylabel('')\n",
    "    ax.set_xticklabels([f'{sse}.{str(int(v))}' for v in ax.get_xticks()], rotation=90)\n",
    "    plt.savefig(f'../fig/r2stal/stal_{sse}_stats.svg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a DataFrame for AA-logoplots and plot them element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE A FULL DATAFRAME FOR THE LABELED POSITIONS AND THEIR RESPECTIVE AA FREQUENCIES FOR LOGOPLOTS\n",
    "sse_aa_freqs = {}\n",
    "aastr = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "cols = {aa:i for i,aa in enumerate(aastr)}\n",
    "for sse in newkeys:\n",
    "    sse_dict = label_seq[sse]\n",
    "    aafreqs = np.zeros(shape=(len(sse_dict.keys()), 21))\n",
    "    for p_index, pos in enumerate(sorted(sse_dict.keys())):\n",
    "        aas, freq = np.unique(np.array(sse_dict[pos]), return_counts=True)\n",
    "        for i, aa in enumerate(aas):\n",
    "            aafreqs[p_index, cols[aa]] = freq[i]/14435\n",
    "    sse_aa_freqs[sse] = aafreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGOPLOTS FOR THE ELEMENTS\n",
    "\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "for sse in newkeys:\n",
    "\n",
    "    lframe = pd.DataFrame(data=sse_aa_freqs[sse], columns=[c for c in aastr], index = sorted(plddt_values[sse].keys()))\n",
    "\n",
    "    # Note down the first and last row where the occupation threshold is met.\n",
    "    firstval = None\n",
    "    for i, r in lframe.iterrows():\n",
    "        if np.sum(r) > 0.05: \n",
    "            if firstval is None:\n",
    "                firstval = i\n",
    "            lastval = i\n",
    "    print(firstval, lastval)\n",
    "    subframe = lframe.truncate(before=firstval, after=lastval)\n",
    "    #x_offset = sorted(plddt_values[sse].keys())[0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[5,2])\n",
    "    cons_logo = logomaker.Logo(subframe,\n",
    "                                ax=ax,\n",
    "                                color_scheme='chemistry',\n",
    "                                show_spines=False,\n",
    "                                font_name='DejaVu Mono')\n",
    "\n",
    "    fig.set_facecolor('w')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1)) #AutoMinorLocator())\n",
    "    ax.xaxis.set_major_locator(FixedLocator([a for a in range(2,100,3)]))#MultipleLocator(3))\n",
    "    ax.tick_params(which='both', width=2)\n",
    "    ax.tick_params(which='major', length=8)\n",
    "    ax.tick_params(which='minor', length=6)\n",
    "    ax.set_xticklabels([f'{sse}.{str(int(v))}' for v in ax.get_xticks()], rotation=90)\n",
    "    cons_logo.draw()\n",
    "    fig.tight_layout()\n",
    "    fig.set_facecolor('w')\n",
    "    plt.savefig(f\"../fig/r2stal/stal_conslogo_{sse}.svg\", bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify all stride files in place, including the sigma multiplier of the outlier\n",
    "def modify_stride(stride_file, outfolder, phi_lim, psi_lim, n_sigma=2.0):\n",
    "    outliers = []\n",
    "    # also add the max float mult of sigma into the \"~~~~\" (line[75:79])\n",
    "    # \"{:.2f}\".format(maxsigma)\n",
    "    with open(stride_file) as stride:\n",
    "        d = stride.readlines()\n",
    "    newdata = []\n",
    "    for l in d:\n",
    "        if not l.startswith(\"ASG\") or l[24] != \"E\":\n",
    "            newdata.append(l)\n",
    "            continue\n",
    "\n",
    "        i = l.split()\n",
    "        angles = [float(i[7]), float(i[8])]\n",
    "        adj_angles = [a+360 if a<0 else a for a in angles]\n",
    "        if abs(sse_func.angle_diff(adj_angles[0], phi_lim[0])) > n_sigma*phi_lim[1] or abs(sse_func.angle_diff( adj_angles[1], psi_lim[0])) > n_sigma*psi_lim[1]:\n",
    "            # print(\"outlier found.\", l, sep=\"\\n\")\n",
    "            maxsigma = max([ abs(sse_func.angle_diff(adj_angles[0], phi_lim[0]) / phi_lim[1]) , \n",
    "                             abs(sse_func.angle_diff(adj_angles[1],psi_lim[0]) / psi_lim[1])    \n",
    "                           ])\n",
    "            k = l[:24]+\"e\"+l[25:75]+\"{:.2f}\".format(maxsigma)+\"\\n\"\n",
    "            #print(\"DEBUG:\", k)\n",
    "            newdata.append(k)\n",
    "            outliers.append(round(maxsigma, 2))\n",
    "            continue\n",
    "        \n",
    "        newdata.append(l)\n",
    "    \n",
    "    open(f\"{outfolder}/{stride_file.split('/')[-1]}\", 'w').write(\"\".join(newdata))\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2/*\")\n",
    "phi_lim = [-113.01754866504291, 29.968104201971208]#[245.248, 30.129]\n",
    "psi_lim = [132.75257372738366, 31.172184167730734]#[136.615, 33.950]\n",
    "outfolder = \"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2_floats\"\n",
    "outliers = []\n",
    "for stride_file in stride_files:\n",
    "    outliers += modify_stride(stride_file, outfolder, phi_lim, psi_lim)\n",
    "print(max(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sse_func, glob\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2/*\")\n",
    "hphi, hpsi, sphi, spsi = sse_func.get_bb_distribution(stride_files)\n",
    "print(hphi, hpsi, sphi, spsi)\n",
    "#for angles in [[350, 10], [90, 180, 270, 360], [10, 20, 30]]:\n",
    "#    print('The mean angle of', angles, 'is:', round(mean_angle(angles), 12), 'degrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_collection = pd.read_pickle(\"../human_collection.pkl\")\n",
    "\n",
    "human_accessions = [gain.name.split(\"-\")[0].split(\"_\")[0] for gain in human_collection.collection]\n",
    "human_sequences = [\"\".join(gain.sequence) for gain in human_collection.collection]\n",
    "\n",
    "human_fasta_offsets = find_offsets(\"/home/hildilab/projects/GPS_massif/uniprot_query/agpcr_celsr.fasta\", \n",
    "                                 human_accessions, \n",
    "                                 human_sequences)\n",
    "\n",
    "\"\"\"for i, gain in enumerate(human_collection.collection):\n",
    "    print(gain.name, human_fasta_offsets[i])\n",
    "    element_intervals, element_centers, residue_labels, unindexed_elements, params = tf.assign_indexing(gain, \n",
    "                                                                                                file_prefix=file_prefix, \n",
    "                                                                                                gain_pdb=find_pdb(gain.name, '../all_pdbs'), \n",
    "                                                                                                template_dir='../r2_template_pdbs/',\n",
    "                                                                                                gesamt_bin=gesamt_bin,\n",
    "                                                                                                debug=True, \n",
    "                                                                                                create_pdb=True,\n",
    "                                                                                                hard_cut={\"S2\":7,\"S6\":3,\"H5\":3},\n",
    "                                                                                                patch_gps=True\n",
    "                                                                                                )\"\"\"\n",
    "    \n",
    "stal_human_indexing = stal_indexing = StAlIndexing(human_collection.collection, \n",
    "                             prefix=\"../test_stal_indexing/test\", \n",
    "                             pdb_dir='../all_pdbs/',  \n",
    "                             template_dir='../r2_template_pdbs/', \n",
    "                             template_json = 'template_data.json',\n",
    "                             fasta_offsets=human_fasta_offsets,\n",
    "                             gesamt_bin=gesamt_bin,\n",
    "                             n_threads=1,\n",
    "                             debug=False)\n",
    "\n",
    "header, matrix = stal_human_indexing.construct_data_matrix(unique_sse=False)\n",
    "stal_human_indexing.data2csv(header, matrix, \"human_indexing.overlap.g5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header, matrix = stal_human_indexing.construct_data_matrix(unique_sse=True)\n",
    "stal_human_indexing.data2csv(header, matrix, \"human_indexing_unique.g5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(t) for t in (zip(stal_human_indexing.b_templates, stal_human_indexing.names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcenters = {\n",
    "                    \"A\" :{\"H1\"   :417 , \"H2\":439 , \"H3\":454 , \"H4\":489 , \"H5\":496 , \"H6\":514 },\n",
    "                    \"C\" :{\"H1\"   :464 , \"H2\":484 , \"H3\":498 , \"H4\":541 , \"H5\":549 , \"H6\":567 },\n",
    "                    \"D\" :{\"H1.D1\":390 , \"H2\":420 , \"H3\":435 , \"H4\":480 , \"H5\":488 , \"H6\":506 },\n",
    "                    \"E1\":{\"H1.E1\":142 ,            \"H3\":159 , \"H4\":198 ,            \"H6\":221 },\n",
    "                    \"E5\":{                         \"H3\":268 , \"H4\":303 ,            \"H6\":329 },\n",
    "                    \"F5\":{\"H1\"   :537 , \"H2\":555 , \"H3\":572 , \"H4\":606 , \"H5\":614 , \"H6\":632 },\n",
    "                    \"F4\":{\"H1.F4\":133 , \"H2\":145 , \"H3\":158 ,            \"H5\":201 , \"H6\":218 },\n",
    "                    \"G1\":{                                    \"H4\":165 ,            \"H6\":189 },\n",
    "                    \"G5\":{                                    \"H4\":33  ,            \"H6\": 52 },\n",
    "                    \"G7\":{\"H1\"   :149 , \"H2\":164 , \"H3\":178 , \"H4\":212 , \"H5\":219 , \"H6\":239 },\n",
    "                    \"L\" :{\"H1\"   :495 , \"H2\":509 , \"H3\":521 , \"H4\":580 , \"H5\":588 , \"H6\":608 },\n",
    "                    \"L4\":{\"H1\"   :196 , \"H2\":212 , \"H3\":224 , \"H4\":266 , \"H5\":274 , \"H6\":294 },\n",
    "                    \"V\" :{\"H1\"   :529 , \"H2\":546 , \"H3\":558 , \"H4\":589 ,            \"H6\":607 },\n",
    "                    \"E5b\":  {\"S1\":324, \"S2\":333, \"S3\":350, \"S4\":359, \"S5\":381, \"S6\":409, \"S7\":413, \"S8\":430, \"S9\":453, \"S10\":459, \"S11\":464, \"S12\":478 ,\"S13\":487},\n",
    "                    \"G5b\":  {\"S1\": 65, \"S2\":74,  \"S3\":88 , \"S4\":107 ,\"S5\":130,           \"S7\":148 ,\"S8\":166 ,\"S9\":186 ,\"S10\":198 ,\"S11\":203 ,\"S12\":217 ,\"S13\":226}\n",
    "                  }\n",
    "\n",
    "\n",
    "template_names  = {\"A\":\"A0A2Y9F628\",\n",
    "\"C\":\"A0A7K6E127\",\n",
    "\"D\":\"A0A1A7WJQ6\",\n",
    "\"E1\":\"A0A2I2YJG7\",\n",
    "\"E5b\":\"A0A3P8S994\",\n",
    "\"E5\":\"G1TKX5\",\n",
    "\"F4\":\"W5PQ70\",\n",
    "\"F5\":\"A0A7L3N0A5\",\n",
    "\"G1\":\"A0A7L3GD10\",\n",
    "\"G5\":\"A0A6J3IBI5\",\n",
    "\"G5b\":\"A0A6J3IBI5\",\n",
    "\"G7\":\"A0A2K5Y1I7\",\n",
    "\"L4\":\"A0A7L3KTA8\",\n",
    "\"L\":\"A0A452HCU9\",\n",
    "\"V\":\"A0A6Q2XYK2\"}\n",
    "template_elements = {}\n",
    "\n",
    "for t_id, name in template_names.items():\n",
    "\n",
    "    for gain in valid_collection.collection:\n",
    "        if name not in gain.name:\n",
    "            continue\n",
    "        print(gain.start, gain.subdomain_boundary, gain.end)\n",
    "\n",
    "        res_centers = {v:k for k,v in allcenters[t_id].items()}\n",
    "        #print(res_centers)\n",
    "        if \"b\" in t_id:\n",
    "            target_el = gain.sdb_sheets\n",
    "        else:\n",
    "            target_el = gain.sda_helices\n",
    "        curr_extent_dir = {}\n",
    "        for el in target_el:\n",
    "            for center, el_name in res_centers.items():\n",
    "                if center <= el[1]+gain.start and center >= el[0]+gain.start:\n",
    "                    curr_extent_dir[el_name] = [el[0]+gain.start, el[1]+gain.start]\n",
    "\n",
    "        template_elements[t_id] = curr_extent_dir\n",
    "\n",
    "print(template_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from types import SimpleNamespace\n",
    "template_data = SimpleNamespace(**json.load(open(\"template_data.json\")))\n",
    "template_data.element_extents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
