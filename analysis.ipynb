{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FixedLocator)\n",
    "import logomaker\n",
    "# LOCAL IMPORTS\n",
    "from indexing_classes import GPCRDBIndexing\n",
    "from gain_classes import GainDomain, GainCollection\n",
    "import sse_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_receptor(sequences, selection):\n",
    "    new_list = []\n",
    "    for seq_tup in sequences:\n",
    "        if selection in seq_tup[0]:\n",
    "            new_list.append(seq_tup)\n",
    "    return new_list\n",
    "\n",
    "def filter_by_list(sequences, selection): # selection list\n",
    "    new_list = []\n",
    "    for seq_tup in sequences:\n",
    "        for it in selection:\n",
    "            if it in seq_tup[0]:\n",
    "                new_list.append(seq_tup)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enum = 153\n",
    "valid_seqs = sse_func.read_multi_seq(\"/home/hildilab/projects/agpcr_nom/app_gain_gain.fa\") #app_gain_gain.fa # app_gain_001_gain.fa\n",
    "print(len(valid_seqs))\n",
    "quality_file = \"/home/hildilab/projects/agpcr_nom/app_gain_gain.mafft.jal\"\n",
    "#quality_file = \"/home/hildilab/projects/agpcr_nom/all001.jal\"\n",
    "#quality_file = f\"/home/hildilab/projects/agpcr_nom/app_gain_gain_mafft.{enum}.jal\"\n",
    "alignment_file = \"/home/hildilab/projects/agpcr_nom/app_gain_gain.mafft.fa\"\n",
    "#alignment_file = \"/home/hildilab/projects/agpcr_nom/all001.fa\"\n",
    "#alignment_file = f\"/home/hildilab/projects/agpcr_nom/app_gain_gain_mafft.{enum}.fa\"\n",
    "#stride_folder = \"/home/hildilab/projects/agpcr_nom/all_gps_stride\"\n",
    "#stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/all_gps_stride/*\")\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2/*\")\n",
    "# This only contains the sigma files for truncated (?) PDBs.\n",
    "quality = sse_func.read_quality(quality_file)\n",
    "# -1 of the ACTUAL COLUMN (6782) in JALVIEW since there is is ONE-INDEXED\n",
    "gps_minus_one = 6781 # default: 6781; 160: 6607; 155: 6719\n",
    "aln_cutoff = 6826 # default: 6826; 160: 6645; 155: 6765\n",
    "#gps_minus_one = 5247 # for the 0.01 constructed set\n",
    "#aln_cutoff = 5288 # for the 0.01 constructed set\n",
    "alignment_dict = sse_func.read_alignment(alignment_file, aln_cutoff)\n",
    "\n",
    "#anchors = [ 662, 1194, 1912, 2490, 2848, 3011, 3073, 3260, #H1-H8\n",
    "#            3455, 3607, 3998, 4279, 4850, 5339, #5341 S1-S6, S7 REMOVED!\n",
    "#            5413, 5813, 6337, 6659, 6696, 6765, 6808] #S8-13\n",
    "#anchor_occupation = [ 4594.,  6539., 11392., 13658.,  8862., 5092.,  3228., 14189., #H1-H8\n",
    "#                      9413., 12760.,  9420., 11201., 12283., 3676.,#  4562. S1-S6, S7 REMOVED!\n",
    "#                     13992., 12575., 13999., 14051., 14353., 9760., 14215.] #S8-13\n",
    "#anchor_dict = sse_func.make_anchor_dict(anchors, 3425)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-offset the sequences to match the exact PDB indexing\n",
    "full_seqs = sse_func.read_alignment(\"../full_sequences_23.fa\")\n",
    "valid_adj_seqs = []\n",
    "for tup in valid_seqs:\n",
    "    x = sse_func.find_the_start(longseq=full_seqs[tup[0]], shortseq=tup[1])\n",
    "    if x == 0:\n",
    "        print(\"already 1st res.\\n\", tup[0], tup[1][:10], tup[1][-10:])\n",
    "        valid_adj_seqs.append( (tup[0],full_seqs[tup[0]][:len(tup[1])-1]) )\n",
    "        print(full_seqs[tup[0]][:len(tup[1])-1])\n",
    "    else:\n",
    "        valid_adj_seqs.append( (tup[0],full_seqs[tup[0]][x-1:x+len(tup[1])-1]) )\n",
    "              \n",
    "\"\"\"with open(\"offset_valid_seqs.fa\",\"w\") as fa:\n",
    "    for tup in valid_adj_seqs:\n",
    "        fa.write(f\">{tup[0]}\\n{tup[1]}\\n\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_file = \"../offset_valid_seqs.mafft.fa\"\n",
    "quality_file = \"/home/hildilab/projects/agpcr_nom/offset_valid_seqs.jal\"\n",
    "gps_minus_one = 6553 # default: 6781; 160: 6607; 155: 6719\n",
    "aln_cutoff = 6567 # default: 6826; 160: 6645; 155: 6765\n",
    "alignment_dict = sse_func.read_alignment(alignment_file, aln_cutoff)\n",
    "quality = sse_func.read_quality(quality_file)\n",
    "\n",
    "valid_collection = GainCollection(  alignment_file = alignment_file,\n",
    "                                    aln_cutoff = aln_cutoff,\n",
    "                                    quality = quality,\n",
    "                                    gps_index = gps_minus_one,\n",
    "                                    stride_files = stride_files,\n",
    "                                    sequence_files=None,\n",
    "                                    sequences=valid_adj_seqs,#valid_seqs,\n",
    "                                    alignment_dict = alignment_dict,\n",
    "                                    is_truncated = True,\n",
    "                                    coil_weight=0.00, # TESTING\n",
    "                                    #domain_threshold=20, # TESTING\n",
    "                                    stride_outlier_mode=True,\n",
    "                                    debug=False)\n",
    "#valid_collection = pd.read_pickle(\"../valid_collection.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(valid_collection, open('../valid_collection.p.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_pdb(start, end, oldpdb, newpdb):\n",
    "    with open(oldpdb) as p:\n",
    "        data = p.readlines()\n",
    "    newdata = []\n",
    "    for line in data:\n",
    "        if line.startswith('ATOM'):\n",
    "            resid = int(line[22:26])\n",
    "            if start > resid or end < resid:\n",
    "                continue\n",
    "        newdata.append(line)\n",
    "    with open(newpdb, 'w') as new:\n",
    "        new.write(''.join(newdata))\n",
    "\n",
    "pdbs = glob.glob(\"/home/hildilab/projects/agpcr_nom/*output/**/*_rank_1_*.pdb\")\n",
    "print(f\"Found {len(pdbs)} best ranked models in target directories.\")\n",
    "valid_ct = 0\n",
    "\n",
    "for gain in valid_collection.collection:\n",
    "    valid_ct +=1\n",
    "    name = gain.name\n",
    "    tar_pdb = [p for p in pdbs if name.split(\"_\")[0] in p][0]\n",
    "    new_pdb_path = f'../all_pdbs/{gain.name.split(\"-\")[0]}.pdb'\n",
    "    truncate_pdb(gain.start,gain.end, tar_pdb, new_pdb_path)\n",
    "print(\"Copied and truncated\", valid_ct, \"GAIN domains.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"receptor_list = [\"AGRA2\",\"AGRA3\", \"AGRB1\", \"AGRB2\", \"AGRB3\", \"AGRC1\", \"AGRC2\", \"AGRC3\", \"AGRD1\", \n",
    "                \"AGRD2\", \"AGRE1\", \"AGRE2\", \"AGRE3\", \"AGRE4\", \"AGRE5\", \"AGRF1\", \"AGRF2\", \"AGRF3\", \n",
    "                \"AGRF4\", \"AGRF5\", \"AGRG1\", \"AGRG2\", \"AGRG3\", \"AGRG4\", \"AGRG5\", \"AGRG6\", \"AGRG7\", \n",
    "                \"AGRL1\", \"AGRL2\", \"AGRL3\", \"AGRL4\", \"AGRV1\", \"AGRA\", \"AGRB\", \"AGRC\", \"AGRD\", \n",
    "                \"AGRE\", \"AGRF\", \"AGRG\", \"AGRL\", \"AGRV\", \"_\" ]\n",
    "                #[\"EGFLAGseven-passG-typereceptor1\", \"CELR1\", \"CELSR1\"], \n",
    "                #[\"EGFLAGseven-passG-typereceptor2\", \"CELR2\", \"CELSR2\"]\n",
    "                #[\"EGFLAGseven-passG-typereceptor3\", \"CELR3\", \"CELSR3\"]]\n",
    "                 \n",
    "receptor_list = [[\"EGFLAGseven-passG-typereceptor1\", \"CELR1\", \"CELSR1\"], \n",
    "                 [\"EGFLAGseven-passG-typereceptor2\", \"CELR2\", \"CELSR2\"],\n",
    "                 [\"EGFLAGseven-passG-typereceptor3\", \"CELR3\", \"CELSR3\"],\n",
    "                 [\"EGFLAGseven-passG-typereceptor\", \"CELR\", \"CELSR\"]]\n",
    "out_names = (\"ADGRC1\", \"ADGRC2\", \"ADGRC3\", \"ADGRC\")\n",
    "for i,group in enumerate(receptor_list):\n",
    "    parse_string = group\n",
    "    \n",
    "    print(parse_string)\n",
    "    filtered_sequences = filter_by_receptor(valid_seqs, parse_string)\n",
    "    #filtered_sequences = filter_by_list(valid_seqs, group)\n",
    "    if len(filtered_sequences) == 0:\n",
    "        continue\n",
    "    print(f\"Parsed with {parse_string = }: Result : {len(filtered_sequences)} Sequences\")\n",
    "    parsed_collection = GainCollection( alignment_file = alignment_file,\n",
    "                                        aln_cutoff = aln_cutoff,\n",
    "                                        quality = quality,\n",
    "                                        gps_index = gps_minus_one,\n",
    "                                        stride_files = stride_files,\n",
    "                                        sequence_files=None,\n",
    "                                        sequences=filtered_sequences,\n",
    "                                        alignment_dict = alignment_dict,\n",
    "                                        is_truncated = True,\n",
    "                                        coil_weight=0.08 # testing\n",
    "                                         )\n",
    "    parsed_collection.plot_sse_hist(title=f\"Receptor group: {parse_string} (Total: {len(filtered_sequences)})\",\n",
    "                                   n_max=14,\n",
    "                                   #savename=\"../fig/hists/%s.adj\"%(out_names[i]))\n",
    "                                   savename=\"../fig/hists/%s\"%(str(group)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    parse_string = \"_\"\n",
    "    print(parse_string)\n",
    "    filtered_sequences = filter_by_receptor(valid_seqs, parse_string)\n",
    "    #if len(filtered_sequences) == 0:\n",
    "    print(f\"Parsed with {parse_string = }: Result : {len(filtered_sequences)} Sequences\")\n",
    "    parsed_collection = GainCollection( alignment_file = alignment_file,\n",
    "                                        aln_cutoff = aln_cutoff,\n",
    "                                        quality = quality,\n",
    "                                        gps_index = gps_minus_one,\n",
    "                                        stride_files = stride_files,\n",
    "                                        sequence_files=None,\n",
    "                                        sequences=filtered_sequences,\n",
    "                                        alignment_dict = alignment_dict,\n",
    "                                        is_truncated = True\n",
    "                                         )\n",
    "    parsed_collection.plot_sse_hist(title=f\"Receptor group: {parse_string} (Total: {len(filtered_sequences)})\",\n",
    "                                   n_max=16,\n",
    "                                   savename=\"hists/%s\"%(str(group)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Parsing out specific Files from the overall dataset based on selection\n",
    "def grab_selection(parse_string, stride_path, pdb_list, sequences, profile_path, target_dir, seqs=None):\n",
    "    # grabs PDB file, stride file, profiles, sequence from FASTA and copies to target dir.\n",
    "    if seqs is None:\n",
    "        sub_seqs = [seq for seq in sequences if parse_string.lower() in seq[0].lower()]\n",
    "    else: sub_seqs = seqs\n",
    "    print(f\"Found {len(sub_seqs)} sequences.\")\n",
    "    strides = glob.glob(stride_path+\"*.stride\")#\n",
    "    profiles = glob.glob(profile_path+\"*.png\")\n",
    "    \n",
    "    sub_strides = []\n",
    "    sub_profiles = []\n",
    "    sub_pdbs = []\n",
    "    \n",
    "    for seq in sub_seqs:\n",
    "        ac = seq[0].split(\"-\")[0]\n",
    "        [sub_profiles.append(prof) for prof in profiles if ac in prof]\n",
    "        [sub_strides.append(stride) for stride in strides if ac in stride]\n",
    "        [sub_pdbs.append(pdb) for pdb in pdb_list if ac in pdb]\n",
    "    \n",
    "    for prof in sub_profiles:\n",
    "        name = prof.split(\"/\")[-1]\n",
    "        copyfile(prof, target_dir+\"profiles/\"+name)\n",
    "    \n",
    "    for stride in sub_strides:\n",
    "        name = stride.split(\"/\")[-1]\n",
    "        copyfile(stride, target_dir+\"strides/\"+name)\n",
    "    \n",
    "    for pdb in sub_pdbs:\n",
    "        name = pdb.split(\"/\")[-1]\n",
    "        copyfile(pdb, target_dir+\"pdbs/\"+name)\n",
    "        \n",
    "    for seq in sub_seqs:\n",
    "        sse_func.write2fasta(seq[1]+\"\\n\", seq[0], target_dir+\"seqs/\"+seq[0]+\".fa\")\n",
    "        \n",
    "    print(f\"Copied {len(sub_pdbs)} PDB files, {len(sub_strides)} STRIDE files,\",\n",
    "          f\" {len(sub_profiles)} Profiles and {len(sub_seqs)} Sequences\",\n",
    "          f\"for Selection {parse_string}\")\n",
    "    \n",
    "root_path = \"/home/hildilab/projects/agpcr_nom/\"\n",
    "profile_path = root_path+\"all_gps_profiles/\"\n",
    "pdb_list = glob.glob(f\"{root_path}all_gps*/batch*/*rank_1_*.pdb\")\n",
    "print(len(pdb_list))\n",
    "#valid_seqs\n",
    "target_dir = root_path+\"human/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"grab_selection(parse_string='HUMAN',\n",
    "              stride_path = root_path+\"all_gps_stride/\",\n",
    "              pdb_list = pdb_list,\n",
    "              sequences = valid_seqs,\n",
    "              profile_path = profile_path,\n",
    "              target_dir = target_dir)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_seqs = [\"Q9HBW9\",\"O60241\",\"Q6QNK2\",\"Q9UHX3\",\"Q5T601\",\"Q96PE1\",\"O60242\",\"Q86SQ4\",\n",
    "                \"O94910\",\"Q8IWK6\",\"Q8IZP9\",\"Q8WXG9\",\"Q86Y34\",\"O95490\",\"Q14246\",\"Q9BY15\",\n",
    "                \"Q8IZF2\",\"Q86SQ3\",\"Q8IZF6\",\"Q96K78\",\"Q8IZF3\",\"Q8IZF7\",\"Q8IZF5\",\"Q7Z7M1\",\n",
    "                \"Q8IZF4\",\"Q9HCU4\",\"Q9NYQ6\",\"Q9NYQ7\",\"Q9HAR2\",\"O14514\",\"P48960\",\n",
    "                \"Q9Y653\"]\n",
    "list_31 = filter_by_list(valid_seqs, human_seqs)\n",
    "sigma_2_strides = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigma_2*/*.stride\")\n",
    "\n",
    "\n",
    "human_collection = GainCollection( alignment_file = alignment_file,\n",
    "                                        aln_cutoff = aln_cutoff,\n",
    "                                        quality = quality,\n",
    "                                        gps_index = gps_minus_one,\n",
    "                                        stride_files =  sigma_2_strides,\n",
    "                                        sequence_files=None,\n",
    "                                        sequences=list_31,\n",
    "                                        alignment_dict = alignment_dict,\n",
    "                                        is_truncated = True,\n",
    "                                        coil_weight = 0.00, # TESTING\n",
    "                                        stride_outlier_mode = True,\n",
    "                                        debug=False\n",
    "                                         )\n",
    "\n",
    "pickle.dump(human_collection, open('../human_collection.p.pkl', 'wb'))\n",
    "#print(len(human_collection.collection))\n",
    "\"\"\"for gain in human_collection.collection:\n",
    "    #print(gain.name, gain.start, gain.end, gain.sequence, gain.index, gain.subdomain_boundary)\n",
    "    pdb_out = root_path+\"human/trunc_pdbs/\"+gain.name+\"_gain.pdb\"\n",
    "    ac = gain.name.split(\"-\")[0]\n",
    "    found_pdb = [pdb for pdb in pdb_list if ac in pdb]\n",
    "    target_pdb = found_pdb[0]\n",
    "    gain.write_gain_pdb(target_pdb, pdb_out)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plt.plot(human_collection.anchor_hist)\n",
    "human_collection.plot_sse_hist(title=f\"Receptor group: HUMAN_31 (Total: 31)\",\n",
    "                                   n_max=16,\n",
    "                                   savename=f\"hists/human_31.s2_newAnch.{enum}\")\"\"\"\n",
    "def find_pdb(name, pdb_folder):\n",
    "    identifier = name.split(\"-\")[0]\n",
    "    target_pdb = glob.glob(f\"{pdb_folder}/*{identifier}*.pdb\")[0]\n",
    "    return target_pdb\n",
    "l = []\n",
    "for gain in human_collection.collection:\n",
    "    l.append(find_pdb(gain.name, \"../all_pdbs/\"))\n",
    "print(\"pymol\", \" \".join(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy = np.zeros([aln_cutoff],dtype=int)\n",
    "sse_matrix = np.zeros([len(valid_collection.collection), aln_cutoff])\n",
    "print(f\"{occupancy.shape = }\\n{sse_matrix.shape = }\")\n",
    "for i, gain in enumerate(valid_collection.collection):\n",
    "    \n",
    "    #print(gain.sda_helices, gain.sdb_sheets)\n",
    "#    for res_id in range(gain.start,gain.end+1):\n",
    "    occupancy[gain.alignment_indices] += 1\n",
    "    #print(\"_\"*30, \"\\n\",gain.name, \"\\n\",gain.sda_helices,\"\\n\", gain.sdb_sheets)\n",
    "    #print(len(gain.alignment_indices))\n",
    "    for helix in gain.sda_helices:\n",
    "        for res_id in range(helix[0]-1,helix[1]): # Residue 1 means that the index is zero.\n",
    "            #print(gain.alignment_indices[res_id], res_id)\n",
    "            gain.alignment_indices[res_id]\n",
    "            sse_matrix[i, gain.alignment_indices[res_id]] = -1\n",
    "    for sheet in gain.sdb_sheets:\n",
    "        for res_id in range(sheet[0]-1,sheet[1]):\n",
    "            sse_matrix[i, gain.alignment_indices[res_id]] = 1\n",
    "\n",
    "anchors , anchor_occupation = valid_collection.find_anchors(cutoff=3000)\n",
    "anchor_dict = sse_func.make_anchor_dict(anchors, valid_collection.alignment_subdomain_boundary)\n",
    "#dir(human_collection.collection[0])\n",
    "#human_collection.collection[0].create_indexing(anchors, anchor_occupation, anchor_dict)\n",
    "print(anchor_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gain in enumerate(valid_collection.collection):\n",
    "    print(\"_\"*30)\n",
    "    print(i, gain.name)\n",
    "    #print(gain.alignment_indices)\n",
    "    #print(gain.sse_sequence.keys())\n",
    "    _,_,_,_ = sse_func.create_indexing(gain, anchors, anchor_occupation, anchor_dict, debug=False, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(anchors, anchor_occupation, valid_collection.alignment_subdomain_boundary)\n",
    "# 000 + 000 ADJUSTED AGAINST S6\n",
    "#anchors = [ 662, 1194, 1912, 2490, 2848, 3011, 3073, 3260, 3455, 3607, 3998, 4279, 4850, 5341,\n",
    "# 5413, 5813, 6337, 6659, 6696, 6765, 6808] # removed S6 @  5339\n",
    "#anchor_occupation = [ 4594.,  6539., 11392., 13658.,  8862., 5092.,  3228., 14189.,  9413., 12760.,\n",
    "#  9420., 11201., 12283.,  4562.,  13992., 12575., 13999., 14051., 14353., 9760., 14215.] # removed S6 @ ,3676.,\n",
    "# 000 + 000 ADJUSTED AGAINST S7\n",
    "#anchors = [ 662, 1194, 1912, 2490, 2848, 3011, 3073, 3260, 3455, 3607, 3998, 4279, 4850, 5339,\n",
    "# 5413, 5813, 6337]#, 6659, 6696, 6765, 6808] # removed S7 @  5341\n",
    "#anchor_occupation = [ 4594.,  6539., 11392., 13658.,  8862., 5092.,  3228., 14189.,  9413., 12760.,\n",
    "#  9420., 11201., 12283.,  3676.,   13992., 12575., 13999.]# 14051., 14353., 9760., 14215.] # removed S7 @ 4562.,\n",
    "# 000 + 001 ADJUSTED\n",
    "# anchors = [ 662, 1194, 1912, 2490, 2848, 2982, 3011, 3073, 3260, 3455, 3607, 3998, 4279, 4850,\n",
    "#  5341, 5413, 5813, 6337, 6659, 6696, 6765, 6808] # removed S6 @ 5339\n",
    "# anchor_odccupation = [ 3866.,  6379., 11353., 13614.,  8346.,  3109.,  4990.,  3313., 14135.,  9137.,\n",
    "# 12558.,  9407., 10876., 12246.,   3921., 13287., 11921., 13976., 14016., 13335., 8890., 14171.] # removed S6 @  3592.,\n",
    "# 001 ADJUSTED\n",
    "#anchors = [ 302,  696, 1114, 1409, 1520, 1630, 1916, 2150, 2293, 2839, 2947, 3515, 3995,\n",
    "# 4070, 4344, 4809, 5137, 5172, 5233, 5275] # removed 5th Helix anchor @1473\n",
    "#anchor_occupation = [10180., 12340., 12657.,  8731.,  4467.,  4250., 13886., 10035., 12556.,\n",
    "# 10593., 3416., 13734.,  5258., 13976., 13628., 13977., 14032., 14328.,  9736., 14194.] # removed 5th Helix anchor quality @ 5274\n",
    "#anchor_dict = sse_func.make_anchor_dict(anchors, valid_collection.alignment_subdomain_boundary)\n",
    "#anchor_dict = sse_func.make_anchor_dict(anchors, 3425)\n",
    "#print(anchor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "plt.figure(figsize=[math.ceil(aln_cutoff/600)+1,4])\n",
    "plt.subplot(3,1,1)\n",
    "plt.vlines(valid_collection.alignment_subdomain_boundary,-10000,10000, linewidth=3, color='k')\n",
    "plt.plot(np.sum(sse_matrix, axis=0))\n",
    "plt.subplot(3,1,2)\n",
    "plt.vlines(valid_collection.alignment_subdomain_boundary, -50000,100000, linewidth=3, color='k')\n",
    "plt.plot(np.convolve(np.sum(sse_matrix, axis=0), np.ones([20])))\n",
    "plt.subplot(3,1,3)\n",
    "plt.vlines(valid_collection.alignment_subdomain_boundary, 0, 13000, linewidth=3, color='k')\n",
    "plt.bar(np.arange(aln_cutoff),valid_collection.anchor_hist, width=2)\n",
    "\n",
    "for i, anchor in enumerate(anchors):\n",
    "    print(i, anchor)\n",
    "    if anchor < valid_collection.alignment_subdomain_boundary: \n",
    "        color = u'#1f77b4'\n",
    "    else: \n",
    "        color = u'#ff7f0e'\n",
    "    plt.scatter(anchor, valid_collection.anchor_hist[anchor]+1000, c=color, marker=\"1\",s=60)\n",
    "plt.savefig(f\"adj.big_signal.s2.png\", dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#offset information\n",
    "def find_offsets(fasta_file, accessions, sequences):\n",
    "    # searches through the accessions in the big sequence file,\n",
    "    # finds the start for the provided sequence\n",
    "    with open(fasta_file,\"r\") as fa:\n",
    "        fa_data = fa.read()\n",
    "        fasta_entries = fa_data.split(\">\")\n",
    "    seqs = []\n",
    "    headers = []\n",
    "    offsets = []\n",
    "    for seq in fasta_entries:\n",
    "        # Fallback for too short sequences\n",
    "        if len(seq) < 10: \n",
    "            continue\n",
    "        data = seq.strip().split(\"\\n\")\n",
    "        headers.append(data[0].split(\"|\")[1]) # This is only the UniProtKB Accession Number and will be matched EXACTLY\n",
    "        seqs.append(\"\".join(data[1:]))\n",
    "    \n",
    "    heads = np.array(headers)\n",
    "    for idx, accession in enumerate(accessions):\n",
    "        #print(np.where(heads == accession))\n",
    "        seq_idx = np.where(heads == accession)[0][0]\n",
    "        offset = sse_func.find_the_start(seqs[seq_idx], sequences[idx])\n",
    "        #print(offset)\n",
    "        offsets.append(offset)\n",
    "    \n",
    "    return offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gain in human_collection.collection:\n",
    "    if \"Q86Y34\" in gain.name:\n",
    "        print(anchors, anchor_dict)\n",
    "        x,y,u,a = gain.create_indexing(silent=False,anchors=anchors, anchor_occupation=anchor_occupation, anchor_dict=anchor_dict, debug=True)\n",
    "        print(x,\"\\n\",y,\"\\n\",u,\"\\n\",a)\n",
    "        print(gain.sse_dict)\n",
    "        print(gain.sda_helices)\n",
    "        print(gain.sdb_sheets)\n",
    "        \n",
    "        print(gain.start)\n",
    "        print(gain.end)\n",
    "        print(len(gain.alignment_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'double'\n",
    "\n",
    "human_accessions = [gain.name[:6] for gain in human_collection.collection]\n",
    "human_gain_seqs = [gain.sequence for gain in human_collection.collection]\n",
    "print(human_accessions)\n",
    "#for gain in human_collection.collection:\n",
    "#    print(gain.name,\"\\n\",gain.a_breaks,\"\\n\",gain.b_breaks,\"\\n\",\"_\"*70)\n",
    "\n",
    "big_seq_file = \"/home/hildilab/projects/GPS_massif/uniprot_query/agpcr_celsr.fasta\"\n",
    "human_base = GPCRDBIndexing(human_collection, split_mode=mode, anchors=anchors, anchor_occupation=anchor_occupation, anchor_dict=anchor_dict)\n",
    "\"\"\"raw_offsets = find_offsets(big_seq_file, human_accessions, human_gain_seqs)\n",
    "refined_offsets = []\n",
    "for j, gain in enumerate(human_collection.collection):\n",
    "    refined_offsets.append(raw_offsets[j]-gain.start)\n",
    "print(raw_offsets)\n",
    "print(refined_offsets)\"\"\"\n",
    "\n",
    "fasta_offsets = find_offsets(big_seq_file, \n",
    "                                 human_base.accessions, \n",
    "                                 human_base.sequences)\n",
    "print(f\"{fasta_offsets =}\")\n",
    "fa_human_base = GPCRDBIndexing(human_collection, fasta_offsets = fasta_offsets, split_mode=mode, anchors=anchors, anchor_occupation=anchor_occupation, anchor_dict=anchor_dict)\n",
    "#print(fa_human_base.data2csv)\n",
    "human_base.data2csv(f\"default_indexed_8-13_gpcrdb.csv\")\n",
    "fa_human_base.data2csv(f\"uniprot_indexed_8-13_gpcrdb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_base = GPCRDBIndexing(valid_collection, anchors, anchor_occupation, anchor_dict, split_mode='double')\n",
    "fasta_offsets = find_offsets(big_seq_file, \n",
    "                                 all_base.accessions, \n",
    "                                 all_base.sequences)\n",
    "all_base.data2csv(f\"default_indexed_all.813.csv\")\n",
    "\n",
    "fa_all_base = GPCRDBIndexing(valid_collection, fasta_offsets = fasta_offsets,  anchors=anchors, anchor_occupation=anchor_occupation, anchor_dict=anchor_dict, split_mode=mode)\n",
    "\n",
    "fa_all_base.data2csv(f\"uniprot_indexed_all.813.csv\")\n",
    "\n",
    "\n",
    "#for i, offset in enumerate(fa_human_base.fasta_offsets):\n",
    "#    sub_pdb = [pdb for pdb in in_pdbs if fa_human_base.accessions[i] in pdb][0]\n",
    "#    offset_pdb(sub_pdb, sub_pdb.replace(\"trunc_pdbs\",\"uniprot_indexed_pdbs\"), offset)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_pdb(in_pdb, out_pdb, offset):\n",
    "    with open(in_pdb,\"r\") as p:\n",
    "        data = p.readlines()\n",
    "    offset_pdb = open(out_pdb, \"w\")\n",
    "    \n",
    "    for line in data:\n",
    "        if line.startswith(\"ATOM\"):\n",
    "            res_id = int(line[22:26])\n",
    "            offset_pdb.write(f\"{line[:22]}{str(res_id+offset).rjust(4)}{line[26:]}\")\n",
    "        else:\n",
    "            offset_pdb.write(line)\n",
    "    \n",
    "    print(f\"Created PDB {out_pdb} with last residue {res_id+offset}. Total offset {offset} .\")\n",
    "\n",
    "print(len(all_base.unindexed), np.unique(np.array(all_base.unindexed), return_counts=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"stal_indexing\" Indexing and the \"validCollection\" Collection, we can query the data to give us statistics about the anchors and individual SSE composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in_pdbs = glob.glob(\"/home/hildilab/projects/agpcr_nom/human_31/aligned/subd_B/trunc_pdbs/*.pdb\")\n",
    "# For each SSE, get the number of structures having this anchor and stats of which residue is the .50 anchor (via np.where -> anchor)\n",
    "# Get statistics of the length of each SSE\n",
    "# Get statistics of loop lengths.\n",
    "# This may also be subqueued to families...\n",
    "\"\"\"print(dir(stal_indexing))\n",
    "print(all_base.indexing_dirs[0])\n",
    "print(valid_collection.collection[0].alignment_indices)\n",
    "print(len(valid_collection.collection[0].sequence))\n",
    "print(anchor_dict)\"\"\"\n",
    "import pandas as pd\n",
    "stal_indexing=pd.read_pickle('stal_indexing.pkl')\n",
    "valid_collection=pd.read_pickle('../valid_collection.p.pkl')\n",
    "def get_loops(indexing_dir):\n",
    "    # Returns a named dict with loop lengths, i.e. {\"H1-H2\":13, \"H8-S1\":12}\n",
    "    inverted_dir = {sse[0] : (sse[1],ki) for ki, sse in indexing_dir.items()} # The begin of each sse is here {0:(13, \"H2\")}\n",
    "    loop_dir = {}\n",
    "    ordered_starts = sorted(inverted_dir.keys())\n",
    "    for i, sse_start in enumerate(ordered_starts):\n",
    "        if i == 0: \n",
    "            continue # Skip the first and go from the second SSE onwards, looking in N-terminal direction.\n",
    "        c_label = inverted_dir[sse_start][1]\n",
    "        n_end, n_label = inverted_dir[ordered_starts[i-1]]\n",
    "        loop_dir[f\"{n_label}-{c_label}\"] = sse_start - n_end - 1\n",
    "    return loop_dir\n",
    "\n",
    "def get_sse_len(intervals, total_keys):\n",
    "    # Returns a dict with the length of each SSE in respective GAIN domain.\n",
    "    len_dir = {x:0 for x in total_keys}\n",
    "    for ki in intervals.keys():\n",
    "        start = intervals[ki][0]\n",
    "        end = intervals[ki][1]\n",
    "        len_dir[ki] = end - start + 1\n",
    "    return len_dir\n",
    "\n",
    "def get_pos_res(pos_dir, gain):\n",
    "    # Returns a dict with the One-Letter-Code of each SSE position in the respective GAIN domain.\n",
    "    pos_res = {k : gain.sequence[pos_dir[k]] for k in pos_dir.keys()}\n",
    "    return pos_res\n",
    "\n",
    "def match_dirs(single_dir, collection_dir, exclude=[]):\n",
    "    for k, v in single_dir.items():\n",
    "        if v in exclude:\n",
    "            continue\n",
    "        if k not in collection_dir.keys():\n",
    "            collection_dir[k] = [v]\n",
    "            continue\n",
    "        collection_dir[k].append(v)\n",
    "    return collection_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = [gain.sequence for gain in valid_collection.collection]\n",
    "print(dir(stal_indexing))\n",
    "# Test with the first Gain domain of the collection:\n",
    "#print(stal_indexing.indexing_dirs[0], stal_indexing.total_keys, stal_indexing.center_dirs[0], stal_indexing.intervals[0], sep ='\\n')\n",
    "#print(get_loops(stal_indexing.indexing_dirs[0]))\n",
    "#print(get_sse_len(stal_indexing.intervals[0], stal_indexing.total_keys))\n",
    "#print(get_pos_res(stal_indexing.center_dirs[0], valid_collection.collection[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#print(dir(all_base))\n",
    "newkeys = ['H1','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13','S14']\n",
    "loop_lengths = {}\n",
    "sse_lengths = {}\n",
    "center_residues = {}\n",
    "sse_matrix = np.zeros(shape=(len(stal_indexing.total_keys),len(stal_indexing.total_keys)))\n",
    "for idx in range(stal_indexing.length):\n",
    "    #Sanity Check - Do the identifiers match? Yes, they do.\n",
    "    #if all_base.names[idx].split(\"-\")[0] != valid_collection.collection[idx].name.split(\"-\")[0]:\n",
    "    #    print(all_base.names[idx].split(\"-\")[0], valid_collection.collection[idx].name.split(\"-\")[0])\n",
    "    #    raise IndexError\n",
    "    #loop_lengths = match_dirs(get_loops(all_base.indexing_dirs[idx]), loop_lengths)\n",
    "    sse_lengths = match_dirs(get_sse_len(stal_indexing.intervals[idx], stal_indexing.total_keys), sse_lengths, exclude=[0])\n",
    "    #center_res = match_dirs(get_pos_res(all_base.center_dirs[idx], valid_collection.collection[idx]), center_residues)\n",
    "\n",
    "    present_sse = stal_indexing.indexing_dirs[idx].keys()\n",
    "    for i, kk in enumerate(newkeys):\n",
    "        for j in range(i,len(newkeys)):\n",
    "            if kk in present_sse and newkeys[j] in present_sse:\n",
    "                sse_matrix[j,i] += 1\n",
    "\n",
    "#plt.imshow(sse_matrix, cmap='gist_yarg')\n",
    "#plt.xticks(ticks= range(len(newkeys)), labels=newkeys, rotation=90)\n",
    "#plt.yticks(ticks= range(len(newkeys)), labels=newkeys)\n",
    "#plt.xlim(-0.5,20.5)\n",
    "#plt.ylim(20.5,-0.5)\n",
    "#cbar = plt.colorbar(shrink=0.5)\n",
    "#plt.savefig(\"occ_map.svg\")\n",
    "#print(f\"\\tHas H4 and H5: {len(has4and5)}\\n\\tHas H5 and H6: {len(has5and6)}\\n\\tHas H4 and H6: {len(has4and6)}\\n\\tHas H4,5,6: {len(has_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(datarow, color, name, length):\n",
    "    max = np.max(datarow)\n",
    "    try: \n",
    "        dens = stats.gaussian_kde(datarow)\n",
    "    except:\n",
    "        print(np.unique(datarow))\n",
    "        return\n",
    "    fig = plt.figure(figsize=[4,2])\n",
    "    fig.set_facecolor('w')\n",
    "    n, x, _ = plt.hist(datarow, bins=np.linspace(0,max,max+1), histtype=u'step', density=True, color='white',alpha=0)\n",
    "    plt.plot(x, dens(x),linewidth=2,color=color,alpha=1)\n",
    "    plt.fill_between(x,dens(x), color=color,alpha=0.1)\n",
    "    ax = plt.gca()\n",
    "    ymax = ax.get_ylim()[1]\n",
    "    val_string = f'{round(np.average(datarow),2)}±{round(np.std(datarow),2)}'\n",
    "    plt.text(max, ymax*0.95, name, horizontalalignment='right', fontsize=14, verticalalignment='top')\n",
    "    plt.text(max, ymax*0.8, val_string, horizontalalignment='right', fontsize=14, verticalalignment='top')\n",
    "    plt.text(max, ymax*0.65, f\"{round(len(datarow)/length*100, 1)}%\", horizontalalignment='right', fontsize=14, verticalalignment='top')\n",
    "    plt.xlabel('Element Length [Residues]')\n",
    "    plt.ylabel('Relative density [AU]')\n",
    "    plt.savefig(f'{name}_hist.svg')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def parse_conservation(datarow, length):\n",
    "    total = len(datarow)\n",
    "    letters, counts = np.unique(np.array(datarow), return_counts=True)\n",
    "\n",
    "    resid_counts = {}\n",
    "    for i, res in enumerate(letters):\n",
    "            resid_counts[int(counts[i])] = res\n",
    "    \n",
    "    sorted_counts = sorted(resid_counts.keys())[::-1]\n",
    "\n",
    "    occupancy = round(total/length*100, 1)\n",
    "    conserv_string = []\n",
    "    residue_occupancies = [ int( x*100 / total ) for x in sorted_counts]\n",
    "    for idx, occ in enumerate(residue_occupancies):\n",
    "        if occ >= 5: conserv_string.append(f\"{resid_counts[sorted_counts[idx]]}:{occ}%\")\n",
    "\n",
    "    return occupancy, \", \".join(conserv_string)\n",
    "# HISTOGRAMS FOR ALL SSE Lengths.\n",
    "\n",
    "newkeys = ['H1','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13','S14']\n",
    "\n",
    "\"\"\"with open('adj.sse_lengths.tsv','w') as lenfile:\n",
    "    lenfile.write(\"Element\\tLength\\n\")\n",
    "    for ki in newkeys:\n",
    "        datarow = sse_lengths[ki]\n",
    "        if \"S\" in ki: c = 'xkcd:orange'\n",
    "        else: c = 'xkcd:denim'\n",
    "        print(len(datarow))\n",
    "        plot_hist(datarow, c, ki, stal_indexing.length)\n",
    "        val_string = f'{round(np.average(datarow),1)} ± {round(np.std(datarow),1)}'\n",
    "        lenfile.write(f\"{ki}\\t{val_string}\\n\")\"\"\"\n",
    "\n",
    "\"\"\"# Statistics for SSE residue conservation\n",
    "with open(\"adj.sse_stats.tsv\", \"w\") as statfile:\n",
    "    statfile.write(\"Element\\tOccupation\\tConsensus\\n\")\n",
    "    for sse in newkeys:\n",
    "        datarow = center_res[f\"{sse}.50\"]\n",
    "        occ, residues = parse_conservation(datarow, all_base.length)\n",
    "        statfile.write(f\"{sse}\\t{occ}%\\t{residues}\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TSV file with the individual pLDDT values.\n",
    "import json\n",
    "\n",
    "jsons = glob.glob('/home/hildilab/projects/agpcr_nom/*_output/batch*/*rank_1*scores.json')\n",
    "\n",
    "import re\n",
    "identifiers = []\n",
    "with open('all_plddt.tsv', 'w') as c:\n",
    "    c.write('Identifier\\tplddt_values\\n')\n",
    "    for j in jsons:\n",
    "        identifier = re.findall(r'\\/[\\w]+-', j)[0][1:-1]\n",
    "        if identifier in identifiers:\n",
    "            continue\n",
    "        identifiers.append(identifier)\n",
    "        with open(j) as jx:\n",
    "            data = json.load(jx)\n",
    "        c.write(f\"{identifier}\\t{','.join(['{:.2f}'.format(k) for k in data['plddt']])}\\n\")\n",
    "#print(len(identifiers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file where all pLDDT values are contained within, with each identifier\n",
    "# construct a dictionary where for each identifier (i.e. H6.49), the total occ. is plottef\n",
    "\n",
    "def construct_identifiers(indexing_dir, center_dir, plddt_values, max_id_dir, name, gain_start, seq=None):\n",
    "    # Bugfix for premature end\n",
    "    seq=list(seq)\n",
    "    if seq is not None:\n",
    "        seq.append(\"X\")\n",
    "    id_dir = {}\n",
    "    plddts = {}\n",
    "    sse_seq = {}\n",
    "    for sse in indexing_dir.keys():\n",
    "        if sse == 'GPS' :\n",
    "            continue\n",
    "        start = indexing_dir[sse][0]\n",
    "        end = indexing_dir[sse][1]\n",
    "        if end-start > 45:\n",
    "            print(f\"NOTE: SKIPPDING TOO LONG SSE WITH LENGTH {end-start}\\n{name}: {sse}\")\n",
    "            continue\n",
    "        center_res = center_dir[f\"{sse}.50\"]\n",
    "        first_res = 50 - center_res + start\n",
    "        for k in range(end-start+1):\n",
    "            if sse not in max_id_dir.keys():\n",
    "                max_id_dir[sse] = []\n",
    "            if first_res+k not in max_id_dir[sse]:\n",
    "                max_id_dir[sse].append(first_res+k)\n",
    "        id_dir[sse] = [first_res+k for k in range(end-start+1)]\n",
    "        plddts[sse] = [plddt_values[k] for k in range(start, end+1)]\n",
    "        if seq is not None:\n",
    "            #print(len(seq), gain_start, start, end, start-gain_start-1, end-gain_start)\n",
    "            sse_seq[sse] = [seq[k] for k in range(start-gain_start, end-gain_start+1)]\n",
    "    if seq is None:\n",
    "        sse_seq = None\n",
    "    return max_id_dir, id_dir, plddts, sse_seq\n",
    "\n",
    "def get_plddt_dir(file='all_plddt.tsv'):\n",
    "    plddt_dir = {}\n",
    "    with open(file) as f:\n",
    "        data = [l.strip() for l in f.readlines()[1:]]\n",
    "        for l in data:\n",
    "            i,v  = tuple(l.split(\"\\t\"))\n",
    "            plddt_dir[i] = [float(val) for val in v.split(\",\")]\n",
    "    return plddt_dir\n",
    "\n",
    "def make_id_list(id_dir):\n",
    "    id_list = []\n",
    "    for sse in id_dir.keys():\n",
    "        for res in id_dir[sse]:\n",
    "            id_list.append(f\"{sse}.{res}\")\n",
    "    return id_list #np.array(id_list)\n",
    "\n",
    "def compact_label_positions(id_collection, plddt_collection, sse_keys, return_unique=True):\n",
    "    label_plddts = {}\n",
    "    print(\"compact_label_positions:\", sse_keys)\n",
    "    for sse in sse_keys:\n",
    "        label_plddts[sse] = {}\n",
    "\n",
    "    for i in range(len(id_collection)):\n",
    "        gain_positions = id_collection[i]\n",
    "        plddt_positions = plddt_collection[i]\n",
    "        #print(plddt_positions.keys())\n",
    "        for sse in gain_positions.keys():\n",
    "            if not return_unique and \".\" in sse:\n",
    "                print(\"Found unique segment\", sse)\n",
    "                sse_id = sse.split(\".\")[0]\n",
    "            else:\n",
    "                sse_id = sse\n",
    "            for j, pos in enumerate(gain_positions[sse]):\n",
    "                pos = int(pos)\n",
    "                if pos < 10 or pos > 90:\n",
    "                    continue\n",
    "                #print(pos, label_plddts[sse_id].keys())\n",
    "                if pos not in label_plddts[sse_id].keys():\n",
    "                    label_plddts[sse_id][pos] = [plddt_positions[sse][j]]\n",
    "                else:\n",
    "                    label_plddts[sse_id][pos].append(plddt_positions[sse][j])\n",
    "\n",
    "    return label_plddts\n",
    "\n",
    "def construct_id_occupancy(intervals, center_dirs, length, plddt_dir, names, seqs):\n",
    "    newkeys = ['H1','H1.D1','H1.E1', 'H1.F4','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13','S14']\n",
    "    id_collection = []\n",
    "    plddt_collection = []\n",
    "    seq_collection = []\n",
    "    all_id_dir = {x:[] for x in newkeys}\n",
    "    for k in range(length):\n",
    "        identifier = names[k].split(\"-\")[0]\n",
    "        plddt_values = plddt_dir[identifier]\n",
    "        gain_start = valid_collection.collection[k].start\n",
    "        #print(identifier, valid_collection.collection[k].name, len(plddt_values))\n",
    "        #print(\"INPUT:\", k, intervals[k], center_dirs[k], plddt_values, all_id_dir, names[k], seqs[k], sep='\\n')\n",
    "        all_id_dir, id_dir, plddts, sse_seq = construct_identifiers(intervals[k], center_dirs[k], plddt_values, all_id_dir, names[k], gain_start, seqs[k])\n",
    "        #print(\"OUTPUT:\", all_id_dir, id_dir, plddts, sse_seq, sep='\\n')\n",
    "        id_collection.append(id_dir)\n",
    "        #print(id_dir)\n",
    "        plddt_collection.append(plddts)\n",
    "        seq_collection.append(sse_seq)\n",
    "    print(\"Completed creating value collection.\")\n",
    "    print(id_collection[0])\n",
    "    print(plddt_collection[0])\n",
    "    print(valid_collection.collection[0].name)\n",
    "    # Here, parse through the id_dirs to count the occurrence of positions per SSE\n",
    "    # Dictionary to map any label identifier to a respective position.\n",
    "    id_map = {}\n",
    "    i = 0\n",
    "    for sse in newkeys:\n",
    "        for res in all_id_dir[sse]:\n",
    "            id_map[f'{sse}.{res}'] = i \n",
    "            i += 1\n",
    "    \n",
    "    max_id_list = []\n",
    "    for i, id_dict in enumerate(id_collection):\n",
    "        max_id_list.append(make_id_list(id_dict))\n",
    "    flat_id_list = np.array([item for sublist in max_id_list for item in sublist])\n",
    "    print(\"Finished constructing flat_id_list.\")\n",
    "    labels, occ = np.unique(flat_id_list, return_counts=True)\n",
    "    occ_dict = dict(zip(labels,occ))\n",
    "    # Parse through labels, occ to generate the sse-specific data\n",
    "    #occ_dict = {labels[u]:occ[u] for u in range(len(labels))}\n",
    "    # Transform occ_dict to the same format as label_plddts (one dict per sse):\n",
    "    label_occ = {}\n",
    "    for sse in newkeys:\n",
    "        print(sse)\n",
    "        label_occ[sse] = {int(k[-2:]):v for k,v in occ_dict.items() if k[:-3] == sse}\n",
    "    #print(labels, occ)\n",
    "    label_plddts = compact_label_positions(id_collection, plddt_collection, newkeys, return_unique=True)\n",
    "    label_seq = compact_label_positions(id_collection, seq_collection, newkeys, return_unique=True)\n",
    "    #print(labels)\n",
    "    return label_plddts, label_occ, label_seq\n",
    "    #[print(k, len(v)) for k,v in label_plddts.items()]\n",
    "\n",
    "plddt_dir = get_plddt_dir()\n",
    "print(list(plddt_dir.keys())[:10])\n",
    "plddt_values, occ_values, label_seq = construct_id_occupancy(stal_indexing.intervals, \n",
    "                                                             stal_indexing.center_dirs, \n",
    "                                                             stal_indexing.length, \n",
    "                                                             plddt_dir, \n",
    "                                                             stal_indexing.names, \n",
    "                                                             seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(occ_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT THE POSITION OCCUPANCY AND THE AVERAGE PLDDT PER POSITION. with plddt_values, occ_values\n",
    "#newkeys = ['H1','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13','S14']\n",
    "newkeys = ['H1','H1.D1','H1.E1', 'H1.F4','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13','S14']\n",
    "for sse in newkeys:\n",
    "    # Transform the values first\n",
    "    pp = plddt_values[sse]\n",
    "    #print(occ_values[sse])\n",
    "    av_pp = {k:np.average(np.array(v))/100 for k,v in pp.items()}\n",
    "    #print(av_pp)\n",
    "    norm_occ = {k:v/14435 for k,v in occ_values[sse].items()}\n",
    "    xax = sorted(av_pp.keys())\n",
    "    y_pp = [av_pp[x] for x in xax]\n",
    "    y_occ = [norm_occ[x] for x in xax]\n",
    "    norm_pp = np.array(y_pp)*np.array(y_occ)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[5,2])\n",
    "    fig.set_facecolor('w')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1)) #AutoMinorLocator())\n",
    "    ax.xaxis.set_major_locator(FixedLocator([a for a in range(2,100,3)]))#MultipleLocator(3)))\n",
    "    ax.tick_params(which='both', width=2)\n",
    "    ax.tick_params(which='major', length=8)\n",
    "    ax.tick_params(which='minor', length=6)\n",
    "    plt.bar(xax,y_pp, color='silver', alpha=0.7)\n",
    "    plt.plot(xax, y_occ, color='dodgerblue')\n",
    "    plt.bar(xax, norm_pp, color='xkcd:lightish red', alpha=0.1)\n",
    "    plt.title(f'Element Composition ({sse})')\n",
    "    plt.yticks(ticks = [0, 0.2, 0.4, 0.6, 0.8, 1], labels = ['0%', '20%', '40%', '60%', '80%', '100%'])\n",
    "    #plt.ylabel('')\n",
    "    ax.set_xticklabels([f'{sse}.{str(int(v))}' for v in ax.get_xticks()], rotation=90)\n",
    "    plt.savefig(f'../fig/r3stal/{sse}_stats.svg', bbox_inches='tight')\n",
    "    #plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the occupancy of certain positions:\n",
    "enriched_positions = ['H1.50','H1.54','H1.57','H1.61','H2.56','H2.57','H2.60','H2.61','H3.36','H3.43',\n",
    "'H3.44','H3.51','H3.53','H3.56','H4.38','H4.41','H4.51','H5.37','H5.38','H5.42','H5.44',\n",
    "'H5.48','H5.50','H5.59','H6.42','H6.54','H6.56','H7.40','H7.51','H8.46','H8.58','H8.60',\n",
    "'S1.48','S2.47','S2.51','S2.53','S2.58','S3.53','S3.55','S5.56','S6.48','S6.52','S6.55',\n",
    "'S7.50','S8.45','S8.52','S8.57','S10.53','S11.50','S12.54','S12.55','S13.47','S13.50','S13.52','S14.48','S14.50']\n",
    "for sse in newkeys:\n",
    "    sub_positions = [k for k in enriched_positions if f'{sse}.' in k]\n",
    "    # Transform the values first\n",
    "    #pp = plddt_values[sse]\n",
    "    #print(occ_values[sse])\n",
    "    #av_pp = {k:np.average(np.array(v))/100 for k,v in pp.items()}\n",
    "    #print(av_pp)\n",
    "    norm_occ = {f'{sse}.{k}':v/14435 for k,v in occ_values[sse].items()}\n",
    "    #print(norm_occ)\n",
    "    for k in sub_positions:\n",
    "        print(round(norm_occ[k],2),k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE A FULL DATAFRAME FOR THE LABELED POSITIONS AND THEIR RESPECTIVE AA FREQUENCIES FOR LOGOPLOTS\n",
    "sse_aa_freqs = {}\n",
    "aastr = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "cols = {aa:i for i,aa in enumerate(aastr)}\n",
    "for sse in newkeys:\n",
    "    sse_dict = label_seq[sse]\n",
    "    aafreqs = np.zeros(shape=(len(sse_dict.keys()), 21))\n",
    "    for p_index, pos in enumerate(sorted(sse_dict.keys())):\n",
    "        aas, freq = np.unique(np.array(sse_dict[pos]), return_counts=True)\n",
    "        for i, aa in enumerate(aas):\n",
    "            aafreqs[p_index, cols[aa]] = freq[i]/14435\n",
    "    sse_aa_freqs[sse] = aafreqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the sequence composition for each cancer-enriched-position\n",
    "\n",
    "for sse in newkeys:\n",
    "    sub_positions = [k for k in enriched_positions if f'{sse}.' in k]\n",
    "    lframe = pd.DataFrame(data=sse_aa_freqs[sse], columns=[c for c in aastr], index = sorted(plddt_values[sse].keys()))\n",
    "    #print(lframe)\n",
    "    for pos in sub_positions:\n",
    "        idx = int(pos[-2:]) # get the row number in the SSE\n",
    "        res_data = lframe.loc[[idx]]\n",
    "        total_freqs = res_data.sum(axis=1).to_list()[0]\n",
    "        #print(f\"{total_freqs = }, {type(total_freqs)}\")\n",
    "\n",
    "        norm_freq_dict = {round(freq.to_list()[0]/total_freqs, 5) : aa for aa, freq in res_data.items()}\n",
    "        sorted_norm_freqs = sorted(norm_freq_dict.keys())[::-1]\n",
    "        #print(pos)\n",
    "        xstring = ''\n",
    "        for k in sorted_norm_freqs[:3]: \n",
    "            xstring = xstring +f'{norm_freq_dict[k]}:{round(k*100)}%,'\n",
    "        print(pos, xstring)\n",
    "\n",
    "            # normalize frequency with the total sum\n",
    "\n",
    "\"\"\"for sse in newkeys:\n",
    "    for pos in enriched_positions:\n",
    "        datarow = center_res[pos]\n",
    "        occ, residues = parse_conservation(datarow, all_base.length)\n",
    "        print(f\"{sse}\\t{occ}%\\t{residues}\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGOPLOTS FOR THE ELEMENTS\n",
    "\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "for sse in newkeys:\n",
    "\n",
    "    lframe = pd.DataFrame(data=sse_aa_freqs[sse], columns=[c for c in aastr], index = sorted(plddt_values[sse].keys()))\n",
    "\n",
    "    # Note down the first and last row where the occupation threshold is met.\n",
    "    firstval = None\n",
    "    for i, r in lframe.iterrows():\n",
    "        if np.sum(r) > 0.05: \n",
    "            if firstval is None:\n",
    "                firstval = i\n",
    "            lastval = i\n",
    "    print(firstval, lastval)\n",
    "    subframe = lframe.truncate(before=firstval, after=lastval)\n",
    "    #x_offset = sorted(plddt_values[sse].keys())[0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[5,2])\n",
    "    cons_logo = logomaker.Logo(subframe,\n",
    "                                ax=ax,\n",
    "                                color_scheme='chemistry',\n",
    "                                show_spines=False,\n",
    "                                font_name='DejaVu Sans Mono')\n",
    "\n",
    "    fig.set_facecolor('w')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1)) #AutoMinorLocator())\n",
    "    ax.xaxis.set_major_locator(FixedLocator([a for a in range(2,100,3)]))#MultipleLocator(3))\n",
    "    ax.tick_params(which='both', width=2)\n",
    "    ax.tick_params(which='major', length=8)\n",
    "    ax.tick_params(which='minor', length=6)\n",
    "    ax.set_xticklabels([f'{sse}.{str(int(v))}' for v in ax.get_xticks()], rotation=90)\n",
    "    cons_logo.draw()\n",
    "    fig.tight_layout()\n",
    "    fig.set_facecolor('w')\n",
    "    plt.savefig(f\"../fig/r3stal/conslogo_{sse}.svg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gain in enumerate(human_collection.collection):\n",
    "    #if \"Q6QNK2\" in gain.name:\n",
    "        x1, x2,_,_ = gain.create_indexing(anchors, anchor_occupation, anchor_dict, \n",
    "                    outdir = \"/home/hildilab/projects/agpcr_nom/human_31/indexing_files_s2_dsp\",\n",
    "                    #offset = fasta_offsets[i]-gain.start+1,\n",
    "                    split_mode='double')\n",
    "        \n",
    "        print(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A CELL FOR SPECIFICALLY EVALUATING INDIVIDUAL GAIN DOMAINS.\n",
    "# MOSTLY FOR DEBUGGING OR CHECKING OUT ANOMALITIES\n",
    "#[('G1SHT5-G1SHT5_RABIT-AGRG6-Oryctolagus_cuniculus', 'REANEVASEILNLTADGQNLTSANITSIVEQVKRIVNKEENIDVTLGSTLMNIFSNILSNSDSDLLESSSEALKTIDELAFKIDLNSTPHVNIATRNLALGVSSVSPGTNVISNFSIGLPSNNESYFQMDFESGQVDPLASVILPPNLLENLSQEDSILVRRAQFTFFNKTGLFQDVGPQRKTLVSYVMACSIGNITIQDLKDPVQIKIKHTRTQEVHHPICAFWDLNKNKSFGGWNTSGCIAHRDSDASETICLCNHFTHFGVLMD')\n",
    "# ('G1SQ79-G1SQ79_RABIT-AGRG4-Oryctolagus_cuniculus', 'LQGLPDKILDLANITVSDENANDVAEHILNLLNESPPLDEEETKIIVSKVSDISLCEKISMNLTQLMLQIINAVLEKQNDSASGLHEVSNEILRLIERAGHKMEFWGRTANLMVARLALAMLRVDHKFEGVTFSIRSYTEGTDPEIYLGDVPAGKVLASIYLPKSLKKRLRVNNLQTILFNFFGQTSLFKVKNVSKALTTYVVSASISDLSIQNLADPVVITLQHVEGSQKYDQVHCAFWDFEKNNGLGGWNSSGCKVKETNVNYTICQCDHLTHFGVLMDL')\n",
    "# ('G1T5U9-G1T5U9_RABIT-AGRL2-Oryctolagus_cuniculus'\n",
    "tar = [\"A0A151X191\"]\n",
    "tar_seqs = filter_by_list(valid_seqs, tar)\n",
    "print(tar_seqs)\n",
    "Xalignment_file = \"/home/hildilab/projects/agpcr_nom/big_mafft14792.fa\" # This is a combined alignment of ALL sequences in ALL queries!\n",
    "Xquality_file = \"/home/hildilab/projects/agpcr_nom/big_mafft14792.jal\"  # ^ corresponding quality file\n",
    "Xquality = sse_func.read_quality(Xquality_file)\n",
    "Xgps_minus_one = 19258 #appended_big_mafft: 21160  #big_mafft14792: 19258\n",
    "Xaln_cutoff = 19822 #appended_big_mafft: 21813 #big_mafft14792: 19822\n",
    "Xstride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/all_gps_stride/*\")\n",
    "Xalignment_dict = sse_func.read_alignment(alignment_file, aln_cutoff)\n",
    "Xname, Xseq = tar_seqs[0]\n",
    "tar_stride = \"/home/hildilab/projects/agpcr_nom/all_gps_stride/A0A369RSM9-A0A369RSM9_9METZ-AGRL3-Trichoplax_sp._H2.stride\"\n",
    "tar_gain = GainDomain(alignment_file = Xalignment_file,\n",
    "                                        aln_cutoff = Xaln_cutoff,\n",
    "                                        quality = Xquality,\n",
    "                                        gps_index = Xgps_minus_one,\n",
    "                                        name=Xname,\n",
    "                                        #stride_files = Xstride_files,\n",
    "                                        sequence = Xseq,\n",
    "                                        alignment_dict = Xalignment_dict,\n",
    "                                        explicit_stride_file=tar_stride,\n",
    "                                        is_truncated = True,\n",
    "                                        coil_weight = 0.01, # TESTING\n",
    "                                        stride_outlier_mode = True,\n",
    "                                        without_anchors=True)\n",
    "#print(tar_gain.isValid)\n",
    "#if tar_gain.isValid: \n",
    "#    tar_gain.plot_helicality(savename=f\"{tar_gain.name.split('-')[0]}_helicality.full.svg\", debug=True)\n",
    "#    tar_gain.plot_profile()\n",
    "#print(tar_gain.subdomain_boundary, tar_gain.start, tar_gain.end)\n",
    "    #print(gain.name, gain.start, gain.end, gain.sequence, gain.index, gain.subdomain_boundary)\n",
    "    #pdb_out = root_path+\"human_32/trunc_pdbs/\"+gain.name+\"_gain.pdb\"\n",
    "    #ac = gain.name.split(\"-\")[0]\n",
    "    #found_pdb = [pdb for pdb in pdb_list if ac in pdb]\n",
    "    #target_pdb = found_pdb[0]\n",
    "    #gain.write_gain_pdb(target_pdb, pdb_out) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln_col = []\n",
    "\n",
    "for idx, idx_dir in enumerate(all_base.indexing_dirs):\n",
    "    try:\n",
    "        l = idx_dir['H6']\n",
    "    except:\n",
    "        continue\n",
    "    print(l)\n",
    "    \n",
    "    for i in range(l[0], l[1]+1):\n",
    "        if valid_collection.collection[idx].sequence[i] == 'W':\n",
    "            aln_col.append(valid_collection.collection[idx].alignment_indices[i])\n",
    "            print(valid_collection.collection[idx].sequence[i],valid_collection.collection[idx].alignment_indices[i])\n",
    "    \n",
    "print(len(aln_col), np.unique(np.array(aln_col), return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATISTICS FOR ELEMENT-CONNECTING LOOPS\n",
    "[print(v, len(k), round(np.average(k),2), round(np.std(k),2)) for v,k in loop_lengths.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loop_stats(indexing_dir, sequence):\n",
    "    # Returns a named dict with loop lengths, i.e. {\"H1-H2\":13, \"H8-S1\":12}\n",
    "    inverted_dir = {sse[0] : (sse[1],ki) for ki, sse in indexing_dir.items() if \"GPS\" not in ki} # The begin of each sse is here {0:(13, \"H2\")}\n",
    "    loop_loc = {}\n",
    "    loop_dir = {}\n",
    "    ordered_starts = sorted(inverted_dir.keys())\n",
    "    for i, sse_start in enumerate(ordered_starts):\n",
    "        if i == 0: \n",
    "            continue # Skip the first and go from the second SSE onwards, looking in N-terminal direction.\n",
    "        c_label = inverted_dir[sse_start][1]\n",
    "        n_end, n_label = inverted_dir[ordered_starts[i-1]]\n",
    "        loop_loc[f\"{n_label}-{c_label}\"] = (n_end, sse_start-1)\n",
    "        loop_dir[f\"{n_label}-{c_label}\"] = sequence[n_end+1:sse_start] # The one-letter-coded seqeuence. Will be a list of lists\n",
    "    return loop_loc, loop_dir\n",
    "\n",
    "loop_lengths = {}\n",
    "loop_seqs = {}\n",
    "loop_seq = {}\n",
    "\n",
    "loop_info = {}\n",
    "#[loop_info[loop] = {} for loop in loop_seqs.keys()] # into each of these keys, any entry is composed of \"name\":$name, \"sequence\":$seq\n",
    "\n",
    "for idx in range(all_base.length):\n",
    "    curr_name = all_base.names[idx]\n",
    "    start = valid_collection.collection[idx].start\n",
    "    i_loc, i_dir = get_loop_stats(all_base.indexing_dirs[idx], valid_collection.collection[idx].sequence)\n",
    "    for k, seq in i_dir.items():\n",
    "        if k not in loop_info.keys():\n",
    "            loop_info[k] = []\n",
    "        loop_info[k].append({'name':f'{all_base.names[idx]}_{i_loc[k][0]+start}-{i_loc[k][1]+start}', 'sequence':''.join(seq)})\n",
    "    #print(i_len)\n",
    "    #loop_lengths = match_dirs(i_len, loop_lengths)\n",
    "    #loop_seqs = match_dirs(i_dir, loop_seqs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the collected loop sequences to a FASTA file for later alignment.\n",
    "def loop2fasta(outfile, itemlist):\n",
    "    with open(outfile, 'w') as out:\n",
    "        for subdict in itemlist:\n",
    "            out.write(f\">{subdict['name']}\\n{subdict['sequence']}\\n\")\n",
    "    print(\"Done with\", outfile)\n",
    "\n",
    "for loop in loop_info.keys():\n",
    "    loop2fasta(f\"../loops/{loop}.fa\", loop_info[loop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all structures containing 7 Helices in Subdomain A.\n",
    "\n",
    "mystring = ''\n",
    "no=0\n",
    "for gain in valid_collection.collection:\n",
    "        try:sse_labl, _, _, _ = gain.create_indexing(silent=True,anchors=anchors, anchor_occupation=anchor_occupation, anchor_dict=anchor_dict, debug=False)\n",
    "        except:no+=1;continue\n",
    "        kk = len([k for k in sse_labl.keys() if \"H\" in k])\n",
    "        if kk >= 7: mystring += gain.name+\"\\n\"#+str(kk)+\"\\n\"\n",
    "print(no)\n",
    "print(mystring)\n",
    "with open(\"7helix.txt\", 'w') as seven:\n",
    "        seven.write(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mystring.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all structures containing 14+ Strands in Subdomain A.\n",
    "\n",
    "mystring = ''\n",
    "no=0\n",
    "num_sheets = np.zeros(shape=(14432))\n",
    "total_strand = np.zeros(shape=(14432))\n",
    "for i, gain in enumerate(valid_collection.collection):\n",
    "        #try:sse_labl, _, _, _ = gain.create_indexing(silent=True,anchors=anchors, anchor_occupation=anchor_occupation, anchor_dict=anchor_dict, debug=False)\n",
    "        #except:no+=1;continue\n",
    "        num_sheets[i] = len(gain.sdb_sheets)\n",
    "        if len(gain.sdb_sheets) > 14 and len(gain.sdb_sheets) < 17:\n",
    "                mystring += gain.name+\"\\n\"#+str(kk)+\"\\n\"\n",
    "\n",
    "        total_strand[i] = np.sum([strand[1]-strand[0] for strand in gain.sdb_sheets])\n",
    "        #k = len([k for k in sse_labl.keys() if \"S\" in k])\n",
    "        #if kk >= 13: mystring += gain.name+\"\\n\"#+str(kk)+\"\\n\"\n",
    "print(no)\n",
    "print(np.unique(num_sheets, return_counts=True))\n",
    "\n",
    "#print(np.unique(total_strand, return_counts=True, return_index=True))\n",
    "with open(\"14plussheet.txt\", 'w') as seven:\n",
    "        seven.write(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_collection.collection[11476].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extents = {}\n",
    "for gain in valid_collection.collection:\n",
    "    extents[gain.name.split(\"-\")[0]] = [str(gain.start+1), str(gain.subdomain_boundary+1), str(gain.end+1)] # make it compatible with ONE-indexed PDBs.\n",
    "\n",
    "import json\n",
    "with open('domain_extents.json', 'w') as j:\n",
    "    dump = json.dumps(extents)\n",
    "    j.write(dump)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "efcc3436bf700bf51081b251413b556e30c22be82f452601745119c8a669a2f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
