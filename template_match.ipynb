{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "import glob, math, json, glob, re\n",
    "#from shutil import copyfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from matplotlib import pyplot as plt\n",
    "#from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, FixedLocator)\n",
    "#import logomaker\n",
    "# LOCAL IMPORTS\n",
    "#from indexing_classes import GPCRDBIndexing\n",
    "from gain_classes import GainDomain, GainCollection, Anchors, GPS\n",
    "import sse_func\n",
    "import matplotlib.pyplot as plt\n",
    "import template_finder as tf\n",
    "\n",
    "allsse =[f'H{a}' for a in range(1,7)]+[f'S{a}' for a in range(1,15)]\n",
    "gesamt_bin = 'home/hildilab/lib/xtal/ccp4-8.0/bin/gesamt'\n",
    "\n",
    "def calc_identity(aln_matrix):\n",
    "    # This takes an alignment matrix with shape=(n_columns, n_sequences) and generates counts based on the identity matrix.\n",
    "    # Returns the highest non \"-\" residue count as the most conserved residue and its occupancy based on count(\"-\") - n_struc\n",
    "    n_struc = aln_matrix.shape[0]\n",
    "    quality = []\n",
    "    occ = []\n",
    "    for col in range(aln_matrix.shape[1]):\n",
    "        chars, count = np.unique(aln_matrix[:,col], return_counts=True)\n",
    "        dtype = [('aa', 'S1'), ('counts', int)]\n",
    "        values = np.array(list(zip(chars,count)), dtype=dtype)\n",
    "        s_values = np.sort(values, order='counts')\n",
    "\n",
    "        if s_values[-1][0] == b'-':\n",
    "            q = s_values[-2][1]\n",
    "        else:\n",
    "            q = s_values[-1][1]\n",
    "        x = np.where(chars == '-')[0][0]\n",
    "        occ.append(n_struc - count[x])\n",
    "        quality.append(q)\n",
    "    return quality, occ\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize pre-calculated metrix for the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_seqs = sse_func.read_multi_seq(\"/home/hildilab/projects/agpcr_nom/app_gain_gain.fa\")\n",
    "quality_file = \"/home/hildilab/projects/agpcr_nom/app_gain_gain.mafft.jal\"\n",
    "alignment_file = \"/home/hildilab/projects/agpcr_nom/app_gain_gain.mafft.fa\"\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2_floats/*\")\n",
    "alignment_dict = sse_func.read_alignment(alignment_file)\n",
    "# This only contains the sigma files for truncated (?) PDBs.\n",
    "#quality = sse_func.read_quality(quality_file)\n",
    "gps_minus_one = 6781 \n",
    "aln_cutoff = 6826 \n",
    "#alignment_dict = sse_func.read_alignment(alignment_file, aln_cutoff)\n",
    "aln_matrix = np.array([list(seq) for seq in alignment_dict.values()])\n",
    "#print(aln_matrix.shape)\n",
    "quality, occ = calc_identity(aln_matrix)\n",
    "\n",
    "precalc_anchors = [ 662, 1194, 1912, 2490, 2848, 3011, 3073, 3260, #H1-H8\n",
    "            3455, 3607, 3998, 4279, 4850, 5339, #5341 S1-S6, S7 REMOVED!\n",
    "            5413, 5813, 6337, 6659, 6696, 6765, 6808] #S8-13\n",
    "precalc_anchor_occupation = [ 4594.,  6539., 11392., 13658.,  8862., 5092.,  3228., 14189., #H1-H8\n",
    "                      9413., 12760.,  9420., 11201., 12283., 3676.,#  4562. S1-S6, S7 REMOVED!\n",
    "                     13992., 12575., 13999., 14051., 14353., 9760., 14215.] #S8-13\n",
    "precalc_anchor_dict = sse_func.make_anchor_dict(precalc_anchors, 3425)\n",
    "t_anchors = [1717, 2983]\n",
    "print(precalc_anchor_dict)\n",
    "t_anchor_dict = {1717: \"H.E\", 2983: \"H.AB\", 662:\"H.D0\", 1094:\"H.A1_0\", 2660:\"H.C0\", 1068:\"H.D1\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to the collection, we need the corresponding PDB file.\n",
    "valid_collection = pd.read_pickle(\"../valid_collection.o.pkl\")\n",
    "allpdbs = glob.glob('../all_pdbs/*.pdb')\n",
    "\"\"\"valid_collection = GainCollection(  alignment_file = alignment_file,\n",
    "                                    aln_cutoff = aln_cutoff,\n",
    "                                    quality = quality,\n",
    "                                    gps_index = gps_minus_one,\n",
    "                                    stride_files = stride_files,\n",
    "                                    sequence_files=None,\n",
    "                                    sequences=valid_seqs,\n",
    "                                    alignment_dict = alignment_dict,\n",
    "                                    is_truncated = True,\n",
    "                                    coil_weight=0.00, # TESTING\n",
    "                                    #domain_threshold=20, # TESTING\n",
    "                                    stride_outlier_mode=True)\"\"\"\n",
    "#for gain in valid_collection.collection:\n",
    "#    gain.create_indexing()\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show anchors for the new conservation metric\n",
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(12,5), facecolor='w')\n",
    "anchors , anchor_occupation = valid_collection.find_anchors(cutoff=3000)\n",
    "anchor_dict = sse_func.make_anchor_dict(anchors, valid_collection.alignment_subdomain_boundary)\n",
    "\n",
    "for i, anchor in enumerate(anchors):\n",
    "    print(i, anchor)\n",
    "    if anchor < valid_collection.alignment_subdomain_boundary: \n",
    "        color = u'#1f77b4'\n",
    "    else: \n",
    "        color = u'#ff7f0e'\n",
    "    #print(anchor)\n",
    "    ax[0].scatter(anchor, valid_collection.anchor_hist[anchor]+1000, c=color, marker=\"1\",s=60)\n",
    "    ax[1].bar(anchor,valid_collection.anchor_hist[anchor]/occ[anchor], width=2)\n",
    "#ax[2].bar(np.arange(aln_cutoff),occ, width=2)\n",
    "ax[0].bar(np.arange(aln_cutoff),valid_collection.anchor_hist, width=2)\n",
    "ax[1].bar(np.arange(aln_cutoff),valid_collection.anchor_hist/occ, width=2)\n",
    "ax[2].bar(np.arange(aln_cutoff),occ, width=2)\n",
    "plt.savefig(\"conservation_anchors.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdb_extent(pdb):\n",
    "    with open(f'../{pdb}') as p:\n",
    "        data = [line for line in p.readlines() if line.startswith(\"ATOM\")]\n",
    "    first_res = int(data[0][22:26])\n",
    "    last_res = int(data[-1][22:26])\n",
    "    return first_res, last_res    \n",
    "\n",
    "extent_list = []\n",
    "realpdbs = []\n",
    "#with open('domain_extents.json') as j:\n",
    "#    extents = json.loads(j.read())\n",
    "\n",
    "for gain in valid_collection.collection:\n",
    "    prot_id =  gain.name.split(\"-\")[0]\n",
    "    gain_pdb = [x[3:] for x in allpdbs if prot_id in x][0]\n",
    "    first_res, last_res = get_pdb_extent(gain_pdb)\n",
    "    realpdbs.append([x[3:] for x in allpdbs if prot_id in x][0])\n",
    "    exlist = [gain.start, gain.subdomain_boundary, gain.end]\n",
    "    if gain.start == 0:\n",
    "        exlist[0] = 1\n",
    "    if exlist[0] < first_res:\n",
    "        exlist[0] = first_res\n",
    "        print(f'Override domain extent! {gain.name}: starts at -->{first_res} instead of {gain.start}')\n",
    "    if gain.end > last_res:\n",
    "        exlist[2] = last_res\n",
    "        print(f'Override domain extent! {gain.name}: ends at -->{last_res} instead of {gain.end}')\n",
    "    extent_list.append(exlist) # exlist = [gain.start, gain.subdomain_boundary, gain.end] --> 0,1 for subdomain A - 1,2 for subdomain B\n",
    "print(\"Found all filenames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glob.glob(\"../r3_sdb_templates/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a GESAMT bashfile for pairwise aln to each subdomain. Output the resulting PDB into respective folder\n",
    "domains = ['sda', 'sdb']\n",
    "#templates = ['../A_sda_A0A6G1Q0B9.pdb', '../E_sdb_A0A3P8S994.pdb']#sdb_template.pdb'] # For the alignment, take all residues from the template\n",
    "templates = ['../r3_sdb_templates/ADGRG4_sda_A0A2I4CCH8.pdb', \n",
    "             '../r3_sdb_templates/ADGRG6_sda_F6QI92.pdb', \n",
    "             '../r3_sdb_templates/ADGRG7_sda_A0A2K5Y1I7.pdb', \n",
    "             '../r3_sdb_templates/ADGRA3_sda_A0A093HFD2.pdb', \n",
    "             '../r3_sdb_templates/ADGRG2_sda_A0A2K5MG19.pdb', \n",
    "             '../r3_sdb_templates/ADGRA2_sda_A0A7N6BTD2.pdb']\n",
    "\n",
    "for temp in templates:\n",
    "    for i in [1]:\n",
    "        gesamt_outfolder = f'../r3_{temp.split(\"_\")[-1][:-4]}_{domains[i]}_matches'\n",
    "        tf.run_gesamt_execution(valid_collection.collection, \n",
    "                                gesamt_bin=gesamt_bin,\n",
    "                                outfolder=gesamt_outfolder, \n",
    "                                pdb_folder='../all_pdbs', \n",
    "                                domain=domains[i], \n",
    "                                n_threads=5, \n",
    "                                max_struc=len(valid_collection.collection), \n",
    "                                no_run=False,\n",
    "                                template=temp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the pairwise GESAMT, we can use the resulting OUT and PDB files for analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def gain_set_to_template(list_of_gains, index_list, anchors, anchor_occupation, anchor_dict, mode='template', penalty=None, n_anch=23, print_matches=False):\n",
    "    distances = np.full(shape=(len(list_of_gains), n_anch), fill_value=penalty)\n",
    "    all_sda_anch = []\n",
    "    all_sdb_anch = []\n",
    "\n",
    "    for gain_idx, gain in enumerate(list_of_gains):\n",
    "        gain_distances, template_sda_anchors, template_sdb_anchors = match_to_template(index_list[gain_idx], \n",
    "                                                                                       gain, \n",
    "                                                                                       anchors=anchors, \n",
    "                                                                                       anchor_occupation=anchor_occupation, \n",
    "                                                                                       anchor_dict=anchor_dict,\n",
    "                                                                                       n_anch=n_anch,\n",
    "                                                                                       penalty=penalty, mode=mode)\n",
    "        distances[gain_idx,:] = gain_distances\n",
    "        all_sda_anch.append(template_sda_anchors)\n",
    "        all_sdb_anch.append(template_sdb_anchors)\n",
    "        if print_matches : print(template_sda_anchors, template_sdb_anchors)\n",
    "    return distances, all_sda_anch, all_sdb_anch\n",
    "\n",
    "def match_to_template(gain_idx, gain, anchors:list, anchor_occupation:list, anchor_dict:dict, n_anch:int, \n",
    "                      sda_anchors:dict, sdb_anchors:dict, sda_a_coords:dict, sdb_a_coords:dict, allsse:list, penalty=None,  mode='anchor-anchor'):\n",
    "    '''\n",
    "    Match a GAIN domain to a template PDB. This function has two modes. \n",
    "    In the 'template' mode, we match the closest residue to the template as the new anchor and output the residues and their distances.\n",
    "    In the 'anchor-anchor' mode, we also calculate anchor-anchor distances and output them.\n",
    "\n",
    "    gain_idx indicates the index of the domain used for locating GESAMT alignment files.\n",
    "    template PDBs are global, but not needed since we are handling only the aligned files / PDBs.\n",
    "\n",
    "    -GLOBALS\n",
    "       sda_anchors, sdb_anchors, sda_a_coords, sdb_a_coords'''\n",
    "    if gain.name.endswith('.'): # Adjust the name for finding the GESAMT aligned PDBs.\n",
    "\n",
    "        name = gain.name[:-1]\n",
    "    else:\n",
    "        name = gain.name\n",
    "    name = name.replace('(','-').replace(')','-').replace('\\'','')\n",
    "    \n",
    "    a_gesamt_file = f'../sda_template_aligned_files/sda_{gain_idx}.out'\n",
    "    b_gesamt_file = f'../sdb_template_aligned_files/sdb_{gain_idx}.out'\n",
    "    \n",
    "    # Get the precalculated anchors from the GainDomain object, split into helices and strand dictionaries.\n",
    "    _, gain_anchor_dict, _, _ = gain.create_indexing(anchors, anchor_occupation, anchor_dict,split_mode='double', silent=True)\n",
    "    gain_anchors =  {k[:-3]:v+gain.start for k,v in gain_anchor_dict.items()} #modify X#.50 --> X#\n",
    "    sda_gain_anchors = {k:v for k,v in gain_anchors.items() if \"H\" in k and v < gain.subdomain_boundary}\n",
    "    sdb_gain_anchors = {k:v for k,v in gain_anchors.items() if \"S\" in k and v > gain.subdomain_boundary}\n",
    "\n",
    "    #a_matches = find_anchor_matches(a_gesamt_file, sda_gain_anchors, isTarget=True)\n",
    "    #b_matches = find_anchor_matches(b_gesamt_file, sdb_gain_anchors, isTarget=True)\n",
    "    sda_actual_anchors = find_anchor_matches(a_gesamt_file, sda_anchors)\n",
    "    sdb_actual_anchors = find_anchor_matches(b_gesamt_file, sdb_anchors)\n",
    "    #print('SDA MATCHES (mobile --> template_res):', matches, '\\n\\nACTUAL ANCHORS (template --> mobile_res)', actual_anchors)\n",
    "    #print('SDB MATCHES (mobile --> template_res):', matches, '\\n\\nACTUAL ANCHORS (template --> mobile_res)', actual_anchors)\n",
    "\n",
    "    if mode == 'anchor-anchor':\n",
    "        # Here, the pre-defined anchor-anchor distances will be calculated.\n",
    "        _, a_dist = calculate_anchor_distances(sda_a_coords, f'../sda_template_aligned_pdbs/{name}_sda.pdb', sda_gain_anchors)\n",
    "        _, b_dist = calculate_anchor_distances(sdb_a_coords, f'../sdb_template_aligned_pdbs/{name}_sdb.pdb', sdb_gain_anchors)\n",
    "        all_dist = {**a_dist, **b_dist}\n",
    "\n",
    "    elif mode == 'template': \n",
    "        all_dist = {k:v[1] for k,v in {**sda_actual_anchors, **sdb_actual_anchors}.items() } #{'H1': (653, 1.04) , ...}\n",
    "    # From the anchor-residue or anchor-anchor distances, construct a distance matrix where unmatched entried will be assigned a penalty value.\n",
    "    else: \n",
    "        print('Unknown mode. Just returning matched anchors.')\n",
    "        return None, sda_actual_anchors, sdb_actual_anchors\n",
    "    # Fill a matrix with the individual distances, assign unmatched anchors a pre-set penalty value\n",
    "    distances = np.full(shape=(n_anch), fill_value=penalty)\n",
    "    for i, sse in enumerate(anchor_dict.values()):\n",
    "        #print(i, sse)\n",
    "        if sse in all_dist.keys():\n",
    "            if all_dist[sse] is not None:\n",
    "                distances[i] = all_dist[sse]\n",
    "            #print(all_dist[sse])\n",
    "            for i,val in enumerate(distances):\n",
    "                if penalty is not None and val > penalty: \n",
    "                    distances[i] = penalty\n",
    "\n",
    "    return distances, sda_actual_anchors, sdb_actual_anchors\n",
    "\n",
    "def plot_pca(distance_matrix, cluster_intervals, n_components, name, plot3D=False, save=True):\n",
    "    colorlist = ['blue','red','green','yellow','orange','purple','forestgreen','limegreen','firebrick']\n",
    "    #X = anchor_distance_matrix\n",
    "    X = distance_matrix\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X)\n",
    "    print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "    #print(pca.singular_values_)\n",
    "    X_r = pca.fit(X).transform(X)\n",
    "    print(X_r.shape)\n",
    "\n",
    "    fig = plt.figure(figsize=[5,5])\n",
    "    fig.set_facecolor('w')\n",
    "    if plot3D:\n",
    "        ax = ax = fig.add_subplot(projection='3d')\n",
    "        for i, interval in enumerate(cluster_intervals):\n",
    "            ax.scatter(X_r[interval[0]:interval[1],0], X_r[interval[0]:interval[1],1], X_r[interval[0]:interval[1],2], marker='o', s=8, c=colorlist[i])\n",
    "    else:\n",
    "        ax = fig.add_subplot()\n",
    "        for i, interval in enumerate(cluster_intervals):\n",
    "            ax.scatter(X_r[interval[0]:interval[1],0], X_r[interval[0]:interval[1],1], marker='o', s=8, c=colorlist[i])\n",
    "    #cbar = plt.colorbar()\n",
    "    #plt.figure()\n",
    "    #plt.imshow(anchor_distance_matrix[::30])\n",
    "    #print(np.max(anchor_distance_matrix))\n",
    "    #plt.colorbar()\n",
    "    ax.set_title(f'PCA of MiniBatchKMeans - {name}')\n",
    "    ax.set_xlabel('PC 0')\n",
    "    ax.set_ylabel('PC 1')\n",
    "    if plot3D:\n",
    "        ax.set_zlabel('PC 2')\n",
    "    if save:\n",
    "        plt.savefig(f'{name}_pca.png', dpi=300)\n",
    "        \n",
    "def cluster_k_means(matrix, list_of_gain_obj, n_cluster=9):\n",
    "    struc_list = [gain.name for gain in list_of_gain_obj]\n",
    "    clust = MiniBatchKMeans(n_clusters=n_cluster,\n",
    "                            random_state=0,\n",
    "                            batch_size=6,\n",
    "                            max_iter=10,\n",
    "                            n_init=\"auto\").fit(matrix)\n",
    "    #clust = AgglomerativeClustering(n_clusters=n_cluster, metric='euclidean', \n",
    "    #                        memory=None, connectivity=None, compute_full_tree='auto', linkage='complete', distance_threshold=None, compute_distances=True).fit(anchor_distance_matrix)\n",
    "    clustering=clust.labels_\n",
    "    #print(np.unique(clustering, return_counts=True))\n",
    "    n_struc, n_distances = matrix.shape # 14432, 21\n",
    "    #print(clustering.labels_)\n",
    "    new_order = np.zeros(shape=(n_struc), dtype=int)\n",
    "    current_num = 0\n",
    "    for i in range(n_cluster):\n",
    "        for j, cluster_id in enumerate(clustering):\n",
    "            if cluster_id == i :\n",
    "                new_order[j] = current_num\n",
    "                current_num += 1\n",
    "    #print(new_order)\n",
    "    remap_dict = {old_idx:new_idx for old_idx, new_idx in enumerate(new_order)}\n",
    "    inv_remap_dict = {v:k for k,v in remap_dict.items()}\n",
    "    reordered_matrix = np.zeros(shape=(n_struc,n_distances))\n",
    "    for x in range(n_struc):\n",
    "        new_x = remap_dict[x]\n",
    "        reordered_matrix[new_x,:] = matrix[x,:]\n",
    "\n",
    "    #fig = plt.figure(figsize=[20,1])\n",
    "    #fig.set_facecolor('w')\n",
    "    #plt.imshow(ordered_distances.transpose(), cmap='Greys')\n",
    "    #cbar = plt.colorbar()\n",
    "    #cbar.set_label('RMSD [$\\AA$]')\n",
    "    #plt.savefig('../test_largedist.png',dpi=300)\n",
    "\n",
    "    reordered_names = [struc_list[inv_remap_dict[k]] for k in range(n_struc)]\n",
    "    reordered_clusters = [clustering[inv_remap_dict[k]] for k in range(n_struc)]\n",
    "\n",
    "    _, cluster_starts = np.unique(reordered_clusters, return_index = True)\n",
    "    cluster_intervals = [(cluster_starts[k], cluster_starts[k+1]) for k in range(n_cluster-1)]\n",
    "    cluster_intervals.append((cluster_starts[-1], n_struc))\n",
    "    #print(cluster_intervals)\n",
    "\n",
    "    return reordered_matrix, cluster_intervals, reordered_names\n",
    "\n",
    "def read_gesamt_pairs(gesamtfile):\n",
    "    with open(gesamtfile) as f:\n",
    "        pair_lines = [line for line in f.readlines() if line.startswith(\"|\")][2:]\n",
    "        #|H- A:LEU 720 | <**1.21**> |H- A:LEU 633 |\n",
    "        #| - A:ALA 721 | <..1.54..> | + A:GLN 634 |\n",
    "        #| + A:GLU 722 | <..1.75..> | + A:PRO 635 |\n",
    "        #| + A:GLU 723 | <==2.03==> | + A:GLN 636 |\n",
    "        #| + A:ASN 724 | <..1.91..> | - A:ALA 637 |\n",
    "        #|H+ A:ARG 725 | <..2.08..> |H- A:LEU 638 |\n",
    "    # construct a data structure with indices of both sides (fixed, mobile)\n",
    "    mobile_pairs = {}\n",
    "    template_pairs = {}\n",
    "    for pair in pair_lines:\n",
    "        template_str, distance_str, mobile_str = pair[9:13].strip(), pair[19:23].strip(), pair[36:40].strip()\n",
    "        #print(template_str, distance_str, mobile_str, \"\\n\", pair)\n",
    "        # If either residue is empty, let the pair point to None\n",
    "        if len(template_str) == 0:\n",
    "            mobile_pairs[int(mobile_str)] = (None, None)\n",
    "            continue\n",
    "        if len(mobile_str) == 0:\n",
    "            template_pairs[int(template_str)] = (None, None)\n",
    "            continue\n",
    "        if len(distance_str) == 0:\n",
    "            dist = None\n",
    "        else:\n",
    "            dist = float(distance_str)\n",
    "        template_pairs[int(template_str)] = (int(mobile_str), dist)\n",
    "        mobile_pairs[int(mobile_str)] = (int(template_str), dist)\n",
    "    \n",
    "    return template_pairs, mobile_pairs\n",
    "\n",
    "def find_anchor_matches(file, anchor_dict, isTarget=False):\n",
    "    # Takes a gesamt file and an anchor dictionary either of the target or the template and returns the matched other residue with the pairwise distance\n",
    "    # as a dictionary: {'H1': (653, 1.04) , ...}\n",
    "    template_pairs, mobile_pairs = read_gesamt_pairs(file)\n",
    "    # Find the closest residue to template anchors\n",
    "    matched_residues = {}\n",
    "    if not isTarget:\n",
    "        parsing_dict = template_pairs\n",
    "    else:\n",
    "        parsing_dict = mobile_pairs\n",
    "\n",
    "    start, end = min(parsing_dict.keys()), max(parsing_dict.keys())\n",
    "    for anchor_name, anchor_res in anchor_dict.items():\n",
    "        # If the anchor lies outside the aligned segments, pass empty match (None, None)\n",
    "        if anchor_res < start or anchor_res > end:\n",
    "            matched_residues[anchor_name] = (None, None)\n",
    "            continue\n",
    "        matched_residues[anchor_name] = parsing_dict[anchor_res]\n",
    "\n",
    "    return matched_residues\n",
    "\n",
    "def get_anchor_coords(pdbfile, anchor_dict, multistate=False):\n",
    "    # Find the CA coordinates of the anchor residue in the template PDB, return dictionary with the coords for each labeled anchor\n",
    "    with open(pdbfile) as p:\n",
    "        if multistate:\n",
    "            data = p.read().split(\"ENDMDL\")[1]\n",
    "        if not multistate:\n",
    "            data = p.read()\n",
    "        mdl2data = [l for l in data.split(\"\\n\") if l.startswith(\"ATOM\")]\n",
    "        ca_data = [atom for atom in mdl2data if \" CA \" in atom]\n",
    "        #print(anchor_dict, pdbfile)\n",
    "    # Find the PDB coords of the CA of each atom and get them as a dict with  {'myanchor':(x,y,z), ...}\n",
    "    res_ca_dict = {int(line[22:26]):(float(line[30:38]), float(line[38:46]), float(line[46:54])) for line in ca_data}\n",
    "    coord_dict={}\n",
    "    for anchor_name, anchor_res in anchor_dict.items():\n",
    "        if anchor_res in res_ca_dict.keys():\n",
    "            coord_dict[anchor_name]=res_ca_dict[anchor_res]\n",
    "        else:\n",
    "            \"FALLBACK. Eliminiating Residue Anchor.\"\n",
    "            coord_dict[anchor_name]=(1000.0, 1000.0, 1000.0)\n",
    "    #coord_dict = {anchor_name:res_ca_dict[anchor_res] for anchor_name, anchor_res in anchor_dict.items()}\n",
    "    \n",
    "    return coord_dict\n",
    "\n",
    "def space_distance(coords1, coords2):\n",
    "    # Pythagorean distance of two sets of coords\n",
    "    return round(math.sqrt(abs(coords1[0]-coords2[0])**2 + abs(coords1[1]-coords2[1])**2 + abs(coords1[2]-coords2[2])**2 ),3)\n",
    "\n",
    "def calculate_anchor_distances(template_anchor_coords, mobile_pdb, mobile_anchors, threshold=10):\n",
    "    # template_anchor_coords are a precalculated set of coords to save calc time.\n",
    "    # Matches always the closest Helix and sheet anchors, respectively, regardless of label.\n",
    "    mobile_anchor_coords = get_anchor_coords(mobile_pdb, mobile_anchors, multistate=True)\n",
    "    #distances = pd.DataFrame()\n",
    "    t_anchor_names = list(template_anchor_coords.keys())\n",
    "    t_coord = list(template_anchor_coords.values())\n",
    "    #print(t_coord)\n",
    "    anchor_occupied = np.zeros(shape=(len(template_anchor_coords.keys())), dtype=bool)\n",
    "    min_dists = []\n",
    "    matched_anchors = []\n",
    "    double_keys = []\n",
    "\n",
    "    mob_coords = list(mobile_anchor_coords.values())\n",
    "    n_template_keys = len(list(template_anchor_coords.keys()))\n",
    "\n",
    "    for m_coord in mob_coords:\n",
    "        distances = [space_distance(m_coord, coords) for coords in t_coord]\n",
    "        min_idx = np.argmin(distances)\n",
    "        if anchor_occupied[min_idx] == True:\n",
    "            double_keys.append(t_anchor_names[min_idx])\n",
    "        anchor_occupied[min_idx] = True\n",
    "        matched_anchors.append(t_anchor_names[min_idx])\n",
    "        min_dists.append(distances[min_idx])\n",
    "\n",
    "    if len(double_keys) == 0:\n",
    "        return dict(zip(mobile_anchors.keys(), matched_anchors)), dict(zip(mobile_anchors.keys(), min_dists))\n",
    "\n",
    "    # If anchors are present multiple times, delete the further distant entry and make None, None\n",
    "    for doublet in double_keys:\n",
    "        #print(f'{doublet = }, {matched_anchors = }, {type(matched_anchors) = }')\n",
    "        indices = [i for i,x in enumerate(matched_anchors) if doublet == x]\n",
    "        minimum = np.argmin(np.array(min_dists)[indices])\n",
    "        indices.remove(indices[minimum])\n",
    "        # Only the non-minimum values will remain in the indices list that need to be re-evaluated\n",
    "        newdists = np.empty(shape=(n_template_keys))\n",
    "        for idx in indices:\n",
    "\n",
    "            for i, coord in enumerate(t_coord):\n",
    "                if anchor_occupied[i] == True:\n",
    "                    newdists[i] = None\n",
    "                    continue\n",
    "                newdists[i] = space_distance(list(mobile_anchor_coords.values())[idx], coord)\n",
    "            newmindist = np.min(newdists)\n",
    "            if newmindist < threshold:\n",
    "                print('Found Alternative')\n",
    "                anchor_occupied[np.argmin(newdists)] = True\n",
    "                min_dists[idx] = newmindist\n",
    "                matched_anchors[idx] = list(mobile_anchor_coords.keys())[idx]\n",
    "            else: \n",
    "                #print('Removed double occurrence')\n",
    "                min_dists[idx] = None\n",
    "                matched_anchors[idx] = ''\n",
    "\n",
    "    return dict(zip(mobile_anchors.keys(), matched_anchors)), dict(zip(mobile_anchors.keys(), min_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRECALC\n",
    "#sda_anchors = {'H1':309, 'H2': 324, 'H3': 358, 'H4':385, 'H5':410, 'H6':421, 'H8':438} #A0A7K7IHI9 -- H7: None\n",
    "#sdb_anchors = {'S1':621, 'S2':628, 'S3':643, 'S4':669, 'S5':688,  'S7':717, 'S8':734, 'S9':750, 'S10':764, 'S11':770, 'S12':782, 'S13':793} #A0A3P9I6M5 S6: None\n",
    "# IDENTITY\n",
    "# pre:  'H1': 309, 'H2': 324, 'H3': 358, 'H4': 385, 'H5': 410, 'H6': 421, 'H8': 438\n",
    "# cons:            'H1': 317, 'H2': 358, 'H3': 377, 'H4': 415, 'H6': 425, 'H7': 443,\n",
    "\n",
    "# pre:  'S1': 621, 'S2': 628, 'S3': 643, 'S4': 669,                         'S7': 717,  'S8': 734,  'S9': 750, 'S10': 764, 'S11': 770, 'S12': 782, 'S13': 793\n",
    "# cons: 'S1': 621, 'S3': 629, 'S4': 644, 'S5': 671, 'S7': 697, 'S10': 712, 'S11': 717, 'S13': 736, 'S14': 753, 'S15': 764, 'S16': 770, 'S18': 784, 'S19': 793\n",
    "\n",
    "sda_anchors = {'H1': 317, 'H2': 358, 'H3': 377, 'H4': 415, 'H6': 425, 'H7': 443} # 6 Helices\n",
    "sdb_anchors = {'S1': 621, 'S3': 629, 'S4': 644, 'S5': 671, 'S7': 697, 'S10': 712, 'S11': 717, 'S13': 736, 'S14': 753, 'S15': 764, 'S16': 770, 'S18': 784, 'S19': 793} # 13 Strands\n",
    "sda_a_coords = get_anchor_coords('../sda_template.pdb', sda_anchors)\n",
    "sdb_a_coords = get_anchor_coords('../sdb_template.pdb', sdb_anchors)\n",
    "\"\"\"\n",
    "for target_protein:\n",
    "    # Get residues that are closest to the anchor\n",
    "    matched_anchors = parse_gesamt_file(file, template_anchors)\n",
    "    # Calculate distances between the Aligned anchor residues\n",
    "    anchor_distances = calculate_anchor_distances(template_anchor_coords, aligned_pdb)\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_list = [tf.get_agpcr_type(gain.name) for gain in valid_collection.collection]\n",
    "name_list = [gain.name for gain in valid_collection.collection]\n",
    "subfam_list = [x[0] for x in fam_list]\n",
    "receptors, counts  = np.unique(fam_list, return_counts=True)\n",
    "r_list = list(zip(receptors,counts))\n",
    "print(r_list)\n",
    "print(receptors)\n",
    "fam_counts = {}\n",
    "for prot in fam_list:\n",
    "    fam = prot[0]\n",
    "    if fam not in fam_counts.keys():\n",
    "        fam_counts[fam] = 0\n",
    "    fam_counts[fam] += 1\n",
    "\n",
    "print(fam_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match everything for each subfamily.\n",
    "#print(subfam_list)\n",
    "y = len(r_list)\n",
    "n_anch = 26\n",
    "n_helix = 9\n",
    "n_sheets = 17\n",
    "allsse = [f\"H{i+1}\" for i in range(n_helix)]+[f\"S{j+1}\" for j in range(n_sheets)]\n",
    "anchor_index = {k:i for i, k in enumerate(allsse)}\n",
    "all_anchor_averages = np.full(shape=(y,n_anch), fill_value=None)\n",
    "all_anchor_occupancy = np.zeros(shape=(y,n_anch))\n",
    "\n",
    "assigned_anchor_freq = np.zeros(shape=(len(receptors),n_anch))\n",
    "# famstring\n",
    "for fam_idx, r in enumerate(receptors):# in enumerate('ABCDEFGLVX'):\n",
    "    print(r)\n",
    "    gain_subset = [ gain for i, gain in enumerate(valid_collection.collection) if fam_list[i] == r]#subfam_list[i]==r ]\n",
    "    gain_idx_list = [ i for i,gain in enumerate(fam_list) if gain == r ]\n",
    "    print(len(gain_idx_list), len(gain_subset))\n",
    "\n",
    "    element_occupation = {k:0 for k in allsse}\n",
    "    n_hels = np.zeros(shape=(len(gain_subset)))\n",
    "    n_sheets = np.zeros(shape=(len(gain_subset)))\n",
    "    for i, gain in enumerate(gain_subset):\n",
    "        _, cd, _, _ = sse_func.create_indexing(gain, anchors, anchor_occupation, anchor_dict,split_mode='double', silent=True)\n",
    "        h,s = 0,0\n",
    "        for k in cd.keys():\n",
    "            element_occupation[k[:-3]] += 1 \n",
    "            if k.startswith('H'): h += 1\n",
    "            elif k.startswith('S'): s +=1\n",
    "        n_hels[i] = h\n",
    "        n_sheets[i] = s\n",
    "    print(np.unique(n_hels, return_counts=True))\n",
    "    print(np.unique(n_sheets, return_counts=True))\n",
    "    for key, val in element_occupation.items():\n",
    "        assigned_anchor_freq[fam_idx, anchor_index[key]] = float(val)/len(gain_subset)\n",
    "\n",
    "    fam_distances, fam_sda_anchors, fam_sdb_anchors = gain_set_to_template(gain_subset, gain_idx_list, anchors, anchor_occupation, anchor_dict, mode='template', penalty=None, n_anch=n_anch)#, print_matches=True)\n",
    "    mean_dist = np.empty(shape=(n_anch))\n",
    "    occ = np.zeros(shape=(n_anch))\n",
    "    \n",
    "    for j in range(n_anch):\n",
    "        occ_values = np.array([d for d in fam_distances[:,j] if d is not None])\n",
    "        if len(occ_values) != 0:\n",
    "            mean_dist[j] = round(np.mean(occ_values), 3)\n",
    "            occ[j] = round(np.count_nonzero(fam_distances[:,j])/len(gain_idx_list), 3)\n",
    "    all_anchor_averages[fam_idx,:] = mean_dist #np.mean(fam_distances, axis=0)\n",
    "    all_anchor_occupancy[fam_idx,:] = occ\n",
    "    print(\"_\"*30)\n",
    "    #reordered_t_distances, t_cluster_intervals, reordered_t_names = cluster_k_means(fam_distances, gain_subset, 9)\n",
    "\n",
    "    #plot_pca(reordered_t_distances, t_cluster_intervals, n_components=2, plot3D=False, name=f'PCA_ADGR{fam_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[6,10], facecolor='w')\n",
    "#plt.yticks(ticks = range(ydim), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "plt.yticks(ticks = range(len(r_list)), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "plt.xticks(ticks = range(n_anch), labels= allsse, rotation=90)\n",
    "plt.imshow(assigned_anchor_freq, cmap='summer')\n",
    "cbar = plt.colorbar(shrink=0.5)\n",
    "cbar.set_label('Relative Occupancy')\n",
    "plt.vlines(6.5,-0.5,len(r_list)-0.5, color='k', linewidth=1.5)\n",
    "ydim = len(r_list)\n",
    "for y in range(ydim):\n",
    "    for x in range(n_anch):\n",
    "        if assigned_anchor_freq[y,x] > 0.001:\n",
    "            pass\n",
    "        else:\n",
    "            plt.text(x,y,'x', horizontalalignment='center', verticalalignment='center', fontsize=18,color='k')\n",
    "plt.savefig('assigned_cons_anchor_occupancy.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_anchor_averages.shape)\n",
    "#allsse =[f'H{a}' for a in range(1,9)]+[f'S{a}' for a in range(1,14)]\n",
    "allsse =[f'H{a}' for a in range(1,7)]+[f'S{a}' for a in range(1,15)]\n",
    "df = pd.DataFrame(data=all_anchor_averages, index = r_list, columns=allsse)\n",
    "print(df)\n",
    "docc = pd.DataFrame(data=all_anchor_occupancy, index = r_list, columns=allsse)\n",
    "print(docc)\n",
    "element_occupation = {k:0 for k in allsse}\n",
    "n_hels = {np.zeros(shape=14432)}\n",
    "n_sheets = np.zeros(shape=14432)\n",
    "for i, gain in enumerate(valid_collection.collection):\n",
    "    _, cd, _, _ = gain.create_indexing(anchors, anchor_occupation, anchor_dict,split_mode='double', silent=True)\n",
    "    h,s = 0,0\n",
    "    for k in cd.keys():\n",
    "        element_occupation[k[:-3]] += 1 \n",
    "        if k.startswith('H'): h += 1\n",
    "        elif k.startswith('S'): s +=1\n",
    "    n_hels[i] = h\n",
    "    n_sheets[i] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(element_occupation)\n",
    "print(np.unique(n_hels, return_counts=True))\n",
    "print(np.unique(n_sheets, return_counts=True))\n",
    "print(anchors, anchor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "ydim = 40\n",
    "fig = plt.figure(figsize=[8,ydim/3], facecolor='w')\n",
    "plt.imshow(docc, cmap='spring')\n",
    "#plt.yticks(ticks = range(ydim), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "plt.yticks(ticks = range(ydim), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "plt.xticks(ticks = range(n_anch), labels= allsse, rotation=90)\n",
    "\n",
    "all_anchor_averages # 10,21\n",
    "for y in range(ydim):\n",
    "    for x in range(n_anch):\n",
    "        if all_anchor_averages[y,x] > 0.0001:\n",
    "            plt.text(x,y, round(all_anchor_averages[y,x], 2), horizontalalignment='center', verticalalignment='center', fontsize=7,color='k', rotation=45)\n",
    "cbar = plt.colorbar(shrink=float(8/ydim))\n",
    "cbar.set_label('Relative Occupancy')\n",
    "plt.vlines(6.5,-0.5,ydim-0.5, color='k', linewidth=1.5)\n",
    "plt.savefig('identity_receptor_anchor_occupancy.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=[8,ydim/3], facecolor='w')\n",
    "#plt.imshow(df, cmap='summer')\n",
    "im_data = np.zeros(shape=(ydim, n_anch))\n",
    "\n",
    "print(all_anchor_averages.shape)\n",
    "#plt.yticks(ticks = range(10), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "plt.yticks(ticks = range(ydim), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "plt.xticks(ticks = range(n_anch), labels= allsse, rotation=90)\n",
    "#all_anchor_averages # 10,21\n",
    "for y in range(ydim):\n",
    "    for x in range(n_anch):\n",
    "        if all_anchor_averages[y,x] > 0.001:\n",
    "            im_data[y,x] = all_anchor_averages[y,x]\n",
    "        else:\n",
    "            plt.text(x,y,'x', horizontalalignment='center', verticalalignment='center', fontsize=20,color='k')\n",
    "            #patches.Rectangle((x,y), 1, 1, linewidth=0.5, edgecolor='k', facecolor='w')\n",
    "plt.imshow(im_data, cmap='summer', vmax=3)\n",
    "            #plt.text(x,y, round(all_anchor_averages[y,x], 2), horizontalalignment='center', verticalalignment='center', fontsize=7,color='k', rotation=45)\n",
    "cbar = plt.colorbar(shrink=float(8)/ydim)\n",
    "cbar.set_label(r'Closest Anchor Residue Distance [$\\AA$]')\n",
    "plt.vlines(6.5,-0.5,ydim-0.5, color='k', linewidth=1.5)\n",
    "plt.savefig('identity_receptor_anchor_distance.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "ydim = 40\n",
    "fig = plt.figure(figsize=[8,ydim/3], facecolor='w')\n",
    "\n",
    "#plt.yticks(ticks = range(ydim), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "plt.yticks(ticks = range(ydim), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "plt.xticks(ticks = range(n_anch), labels= allsse, rotation=90)\n",
    "occ_values = df.to_numpy()\n",
    "is_off = np.zeros(shape=(40,26))\n",
    "print(docc.shape, all_anchor_averages.shape)\n",
    "#all_anchor_averages # 10,21\n",
    "for y in range(ydim):\n",
    "    for x in range(n_anch):\n",
    "        if all_anchor_averages[y,x] is not None and all_anchor_averages[y,x] > 1.5 and occ_values[y,x] > 0.1:\n",
    "            is_off[y,x] = 1\n",
    "            plt.text(x,y, round(all_anchor_averages[y,x], 2), horizontalalignment='center', verticalalignment='center', fontsize=7,color='k', rotation=45)\n",
    "plt.imshow(is_off, cmap='spring')\n",
    "cbar = plt.colorbar(shrink=float(8/ydim))\n",
    "cbar.set_label('Relative Occupancy')\n",
    "plt.vlines(6.5,-0.5,ydim-0.5, color='k', linewidth=1.5)\n",
    "#plt.savefig('identity_receptor_anchor_occupancy.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_stats = np.zeros(shape = (n_anch, 2))\n",
    "\n",
    "for fam_idx, r in enumerate(receptors):# in enumerate('ABCDEFGLVX'):\n",
    "    print(r)\n",
    "    gain_subset = [ gain for i, gain in enumerate(valid_collection.collection) if fam_list[i] == r]#subfam_list[i]==r ]\n",
    "    gain_idx_list = [ i for i,gain in enumerate(fam_list) if gain == r ]\n",
    "    n_sse = [[len(gain.sda_helices), len(gain.sdb_sheets)] for gain in gain_subset] # (n_struc, 2)\n",
    "    n_strucs = np.mean(np.array(n_sse), axis=0)\n",
    "    print(r, round(n_strucs[0], 2), round(n_strucs[1],2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new Anchor management.\n",
    "Use the \"max\" SDA / SDB template for generating the new anchors \n",
    "For each of the SSE \n",
    "  get the center residue index\n",
    "  For each GAIN\n",
    "      Find closest residue to the center, note down the sequence, start, end of the matched SSE\n",
    "  Run MAFFT with each of the gathered sequences\n",
    "  For each MAFFT\n",
    "      Find the most conserved residue (Identity matrix)\n",
    "      Set as new Anchor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = ['A0A2I4CCH8', \n",
    "             'F6QI92', \n",
    "             'A0A2K5Y1I7', \n",
    "             'A0A093HFD2', \n",
    "             'A0A2K5MG19', \n",
    "             'A0A7N6BTD2']\n",
    "sda_centers = {}\n",
    "for gain in valid_collection.collection:\n",
    "    \"\"\"if gain.name.split(\"-\")[0] in templates: #SDA\n",
    "        hel_centers = []\n",
    "        for hel in gain.sda_helices: # each hel is a tuple\n",
    "            hel_centers.append( gain.start + int((hel[0]+hel[1])/2) )\n",
    "        hel_keys = [f'H{i+1}' for i in range(len(hel_centers))]\n",
    "        sda_centers = dict(zip(hel_keys, hel_centers))\"\"\"\n",
    "    if gain.name.split(\"-\")[0] in templates: # if gain.name[:10] =='A0A3P9I6M5':\n",
    "        sheet_centers = []\n",
    "        _, cd, _, _ = sse_func.create_indexing(gain, anchors=precalc_anchors, anchor_occupation=precalc_anchor_occupation, anchor_dict=precalc_anchor_dict, \n",
    "                                               debug=False, silent=True)\n",
    "        print(gain.name, cd)\n",
    "        for sheet in gain.sdb_sheets: # each hel is a tuple\n",
    "            #if sheet[1] - sheet[0] < 3:\n",
    "            #    print(gain.start+sheet[0], gain.start+sheet[1])\n",
    "            sheet_centers.append( gain.start + int((sheet[0]+sheet[1])/2) )\n",
    "        sheet_keys = [f'S{i+1}' for i in range(len(sheet_centers))]\n",
    "        sdb_centers = dict(zip(sheet_keys, sheet_centers))\n",
    "# Manually curated the centers to exclude two small strands in the CD between S6/S7 @ 707-708 and 711-713, respectively (low pLDDT here).\n",
    "# sda_centers = {'H1': 313, 'H2': 328, 'H3': 359, 'H4': 383, 'H5': 409, 'H6': 424, 'H7': 444}\n",
    "# sdb_centers = {'S1': 622, 'S2': 631, 'S3': 645, 'S4': 658, 'S5': 670, 'S6': 695, 'S7': 719, 'S8': 736, 'S9': 752, 'S10': 765, 'S11': 771, 'S12': 782, 'S13': 793}\n",
    "\n",
    "# Find closest residue to the center (GESAMT), note down the sequence, start, end of the matched SSE; write to FASTA\n",
    "    # A dict of dicts --> for each key, there is a dictionary inside sse_seqs['H1'][gain.name]:'seqlist'\n",
    "all_keys = list({**sda_centers, **sdb_centers}.keys())\n",
    "sse_seqs = {k:{} for k in all_keys}\n",
    "sse_extents = {k:{} for k in all_keys}\n",
    "unmatched = {k:0 for k in all_keys}\n",
    "unstructured = {k:0 for k in all_keys}\n",
    "\n",
    "for i, gain in enumerate(valid_collection.collection):\n",
    "        a_gesamt_file = f'../sda_template_aligned_files/sda_{i}.out'\n",
    "        b_gesamt_file = f'../sdb_template_aligned_files/sdb_{i}.out'\n",
    "\n",
    "        sda_matches = find_anchor_matches(a_gesamt_file, sda_centers, isTarget=False)\n",
    "        sdb_matches = find_anchor_matches(b_gesamt_file, sdb_centers, isTarget=False)\n",
    "        #print(sda_matches, sdb_matches)\n",
    "        hel_extents = np.full(shape = (gain.end-gain.start+1), fill_value=100)\n",
    "        she_extents = np.full(shape = (gain.end-gain.start+1), fill_value=100)\n",
    "        # Establish two matrices to match the respective residue to the index of its helix/sheet for easier matching\n",
    "        for i,element in enumerate(gain.sda_helices):\n",
    "            hel_extents[element[0]:element[1]] = i\n",
    "        for i,element in enumerate(gain.sdb_sheets):\n",
    "            she_extents[element[0]:element[1]] = i\n",
    "        # Match the corresponding closest residue to find the associated SSE with start, end and sequence\n",
    "        for sse, match in sda_matches.items():\n",
    "            if match[0] is None:\n",
    "                unmatched[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_index = hel_extents[match[0]-gain.start]\n",
    "\n",
    "            if sse_index == 100:\n",
    "                unstructured[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_extents[sse][gain.name] = gain.sda_helices[sse_index]\n",
    "            sse_seqs[sse][gain.name] = gain.sequence[gain.sda_helices[sse_index][0]:gain.sda_helices[sse_index][1]]\n",
    "        \n",
    "        for sse, match in sdb_matches.items():\n",
    "            if match[0] is None:\n",
    "                unmatched[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_index = she_extents[match[0]-gain.start]\n",
    "\n",
    "            if sse_index == 100:\n",
    "                unstructured[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_extents[sse][gain.name] = gain.sdb_sheets[sse_index]\n",
    "            sse_seqs[sse][gain.name] = gain.sequence[gain.sdb_sheets[sse_index][0]:gain.sdb_sheets[sse_index][1]]\n",
    "        \n",
    "for sse in all_keys:\n",
    "    with open(f'../sse_aln/{sse}.seqs.fa','w') as fa:\n",
    "        for name, seq in sse_seqs[sse].items():\n",
    "            fa.write(f'>{name}\\n{\"\".join(seq)}\\n')\n",
    "\n",
    "print(unmatched, '\\n', unstructured)\n",
    "#   Run MAFFT with each of the gathered sequences\n",
    "#   For each MAFFT\n",
    "#       Find the most conserved residue (Identity matrix)\n",
    "#       Set as new Anchor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for gain in valid_collection.collection:\n",
    "\n",
    "    if gain.name[:10] == 'A0A7K7IHI9': #SDA\n",
    "        sda_gain = gain\n",
    "    if gain.name[:10] =='A0A3P9I6M5': # SDB\n",
    "        sdb_gain = gain\n",
    "for i,k in enumerate(sda_centers.keys()):\n",
    "    kfile = glob.glob(f\"../sse_aln/{k}.aln.fa\")[0]\n",
    "    with open(kfile) as alnf:\n",
    "        x = alnf.readlines()[1].strip(\" \\n\")\n",
    "        kcutoff = len(x)\n",
    "    print(kcutoff)\n",
    "    aln = sse_func.read_alignment(kfile, cutoff=kcutoff)\n",
    "    h = sda_gain.sda_helices[i]\n",
    "    aln_matrix = np.array([list(seq) for seq in aln.values()])\n",
    "    kquality, kocc = calc_identity(aln_matrix)\n",
    "    '''    kquality = []\n",
    "    kocc = []\n",
    "    for col in range(aln_matrix.shape[1]):\n",
    "        chars, count = np.unique(aln_matrix[:,col], return_counts=True)\n",
    "        if chars[0] == '-':\n",
    "            q = count[1]\n",
    "        else:\n",
    "            q = count[0]\n",
    "        x = np.where(chars == '-')[0][0]\n",
    "        kocc.append(14435 - count[x])\n",
    "        kquality.append(q)'''\n",
    "    template_aln_seq = aln[sda_gain.name]\n",
    "    template_res_idx = np.argmax(kquality)\n",
    "    print(template_aln_seq, template_res_idx)\n",
    "    template_index = template_aln_seq[:template_res_idx+1]\n",
    "    t_res = template_aln_seq[template_res_idx]\n",
    "    print(template_index, t_res)\n",
    "    new = template_index\n",
    "\n",
    "    fig = plt.figure(figsize=[4,2], facecolor='w')\n",
    "    plt.bar(range(kcutoff), kquality)\n",
    "    plt.title(f'SDA TEMPLATE : {k}')\n",
    "    plt.xticks(ticks = range(kcutoff), labels=template_aln_seq, fontsize=5)\n",
    "    plt.savefig(f'../sse_aln/{k}.template1.png', dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "for i,k in enumerate(sdb_centers.keys()):\n",
    "    kfile = glob.glob(f\"../sse_aln/{k}.aln.fa\")[0]\n",
    "    with open(kfile) as alnf:\n",
    "        x = alnf.readlines()[1].strip(\" \\n\")\n",
    "        kcutoff = len(x)\n",
    "    print(kcutoff)\n",
    "    aln = sse_func.read_alignment(kfile, cutoff=kcutoff)\n",
    "    h = sdb_gain.sdb_sheets[i]\n",
    "    aln_matrix = np.array([list(seq) for seq in aln.values()])\n",
    "    kquality = []\n",
    "    kocc = []\n",
    "    kquality, kocc = calc_identity(aln_matrix)\n",
    "        \n",
    "    template_aln_seq = aln[sdb_gain.name]\n",
    "    template_res_idx = np.argmax(kquality)\n",
    "    print(template_aln_seq, template_res_idx)\n",
    "    template_index = template_aln_seq[:template_res_idx+1]\n",
    "    t_res = template_aln_seq[template_res_idx]\n",
    "    print(template_index, t_res)\n",
    "    new = template_index\n",
    "\n",
    "    fig = plt.figure(figsize=[4,2], facecolor='w')\n",
    "    plt.bar(range(kcutoff), kquality)\n",
    "    plt.title(f'SDB TEMPLATE : {k}')\n",
    "    plt.xticks(ticks = range(kcutoff), labels=template_aln_seq, fontsize=5)\n",
    "    plt.savefig(f'../sse_aln/{k}.template1.png', dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed671274c087d00ab9ecb8e0fee95cd6aaf5c323b152e6f5d51e6ecc9c97d119"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
