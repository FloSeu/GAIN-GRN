{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for the creation of a GainCollection object. The input are the AlphaFold/Colabfold models  and they are validated and filtered, leaving only validly detected GAIN domains for use in the indexing scheme and template selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES, see README.txt\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FixedLocator)\n",
    "import logomaker\n",
    "# LOCAL IMPORTS\n",
    "from indexing_classes import GPCRDBIndexing\n",
    "from gain_classes import GainCollection\n",
    "import sse_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_collection = np.load('../data/valid_collection.q.pkl', allow_pickle=True)\n",
    "human_collection = np.load('../data/human_collection.q.pkl', allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"stal_indexing\" Indexing and the \"validCollection\" Collection, we can query the data to give us statistics about the anchors and individual SSE composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in_pdbs = glob.glob(\"/home/hildilab/projects/agpcr_nom/human_31/aligned/subd_B/trunc_pdbs/*.pdb\")\n",
    "# For each SSE, get the number of structures having this anchor and stats of which residue is the .50 anchor (via np.where -> anchor)\n",
    "# Get statistics of the length of each SSE\n",
    "# Get statistics of loop lengths.\n",
    "# This may also be subqueued to families...\n",
    "\n",
    "stal_indexing=pd.read_pickle('../stal_indexing.r4.pkl')\n",
    "valid_collection=pd.read_pickle('../valid_collection.q.pkl')\n",
    "def get_loops(indexing_dir):\n",
    "    # Returns a named dict with loop lengths, i.e. {\"H1-H2\":13, \"H8-S1\":12}\n",
    "    inverted_dir = {sse[0] : (sse[1],ki) for ki, sse in indexing_dir.items()} # The begin of each sse is here {0:(13, \"H2\")}\n",
    "    loop_dir = {}\n",
    "    ordered_starts = sorted(inverted_dir.keys())\n",
    "    for i, sse_start in enumerate(ordered_starts):\n",
    "        if i == 0: \n",
    "            continue # Skip the first and go from the second SSE onwards, looking in N-terminal direction.\n",
    "        c_label = inverted_dir[sse_start][1]\n",
    "        n_end, n_label = inverted_dir[ordered_starts[i-1]]\n",
    "        loop_dir[f\"{n_label}-{c_label}\"] = sse_start - n_end - 1\n",
    "    return loop_dir\n",
    "\n",
    "def get_sse_len(intervals, total_keys):\n",
    "    # Returns a dict with the length of each SSE in respective GAIN domain.\n",
    "    len_dir = {x:0 for x in total_keys}\n",
    "    for ki in intervals.keys():\n",
    "        start = intervals[ki][0]\n",
    "        end = intervals[ki][1]\n",
    "        len_dir[ki] = end - start + 1\n",
    "    return len_dir\n",
    "\n",
    "def get_pos_res(pos_dir, gain):\n",
    "    # Returns a dict with the One-Letter-Code of each SSE position in the respective GAIN domain.\n",
    "    pos_res = {k : gain.sequence[pos_dir[k]] for k in pos_dir.keys()}\n",
    "    return pos_res\n",
    "\n",
    "def match_dirs(single_dir, collection_dir, exclude=[]):\n",
    "    for k, v in single_dir.items():\n",
    "        if v in exclude:\n",
    "            continue\n",
    "        if k not in collection_dir.keys():\n",
    "            collection_dir[k] = [v]\n",
    "            continue\n",
    "        collection_dir[k].append(v)\n",
    "    return collection_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = [gain.sequence for gain in valid_collection.collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#print(dir(all_base))\n",
    "newkeys = ['H1','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13','S14','GPS']\n",
    "loop_lengths = {}\n",
    "sse_lengths = {}\n",
    "center_residues = {}\n",
    "sse_matrix = np.zeros(shape=(len(stal_indexing.total_keys),len(stal_indexing.total_keys)))\n",
    "for idx in range(stal_indexing.length):\n",
    "    #Sanity Check - Do the identifiers match? Yes, they do.\n",
    "    #if all_base.names[idx].split(\"-\")[0] != valid_collection.collection[idx].name.split(\"-\")[0]:\n",
    "    #    print(all_base.names[idx].split(\"-\")[0], valid_collection.collection[idx].name.split(\"-\")[0])\n",
    "    #    raise IndexError\n",
    "    loop_lengths = match_dirs(get_loops(all_base.indexing_dirs[idx]), loop_lengths)\n",
    "    sse_lengths = match_dirs(get_sse_len(stal_indexing.intervals[idx], stal_indexing.total_keys), sse_lengths, exclude=[0])\n",
    "\n",
    "    present_sse = stal_indexing.indexing_dirs[idx].keys()\n",
    "    for i, kk in enumerate(newkeys):\n",
    "        for j in range(i,len(newkeys)):\n",
    "            if kk in present_sse and newkeys[j] in present_sse:\n",
    "                sse_matrix[j,i] += 1\n",
    "\n",
    "print(loop_lengths)\n",
    "\n",
    "# Plot the element occupancy for each of the SSE elements as a 2D matrix\n",
    "plt.imshow(sse_matrix, cmap='gist_yarg')\n",
    "plt.xticks(ticks= range(len(newkeys)), labels=newkeys, rotation=90)\n",
    "plt.yticks(ticks= range(len(newkeys)), labels=newkeys)\n",
    "plt.xlim(-0.5,20.5)\n",
    "plt.ylim(20.5,-0.5)\n",
    "cbar = plt.colorbar(shrink=0.5)\n",
    "plt.savefig(\"occ_map.svg\")\n",
    "#print(f\"\\tHas H4 and H5: {len(has4and5)}\\n\\tHas H5 and H6: {len(has5and6)}\\n\\tHas H4 and H6: {len(has4and6)}\\n\\tHas H4,5,6: {len(has_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(datarow, color, name, length):\n",
    "    max = np.max(datarow)\n",
    "    try: \n",
    "        dens = stats.gaussian_kde(datarow)\n",
    "    except:\n",
    "        print(np.unique(datarow))\n",
    "        return\n",
    "    fig = plt.figure(figsize=[4,2])\n",
    "    fig.set_facecolor('w')\n",
    "    n, x, _ = plt.hist(datarow, bins=np.linspace(0,max,max+1), histtype=u'step', density=True, color='white',alpha=0)\n",
    "    plt.plot(x, dens(x),linewidth=2,color=color,alpha=1)\n",
    "    plt.fill_between(x,dens(x), color=color,alpha=0.1)\n",
    "    ax = plt.gca()\n",
    "    ymax = ax.get_ylim()[1]\n",
    "    val_string = f'{round(np.average(datarow),2)}±{round(np.std(datarow),2)}'\n",
    "    plt.text(max, ymax*0.95, name, horizontalalignment='right', fontsize=14, verticalalignment='top')\n",
    "    plt.text(max, ymax*0.8, val_string, horizontalalignment='right', fontsize=14, verticalalignment='top')\n",
    "    plt.text(max, ymax*0.65, f\"{round(len(datarow)/length*100, 1)}%\", horizontalalignment='right', fontsize=14, verticalalignment='top')\n",
    "    plt.xlabel('Element Length [Residues]')\n",
    "    plt.ylabel('Relative density [AU]')\n",
    "    plt.savefig(f'{name}_hist.svg')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def parse_conservation(datarow, length):\n",
    "    total = len(datarow)\n",
    "    letters, counts = np.unique(np.array(datarow), return_counts=True)\n",
    "\n",
    "    resid_counts = {}\n",
    "    for i, res in enumerate(letters):\n",
    "            resid_counts[int(counts[i])] = res\n",
    "    \n",
    "    sorted_counts = sorted(resid_counts.keys())[::-1]\n",
    "\n",
    "    occupancy = round(total/length*100, 1)\n",
    "    conserv_string = []\n",
    "    residue_occupancies = [ int( x*100 / total ) for x in sorted_counts]\n",
    "    for idx, occ in enumerate(residue_occupancies):\n",
    "        if occ >= 5: conserv_string.append(f\"{resid_counts[sorted_counts[idx]]}:{occ}%\")\n",
    "\n",
    "    return occupancy, \", \".join(conserv_string)\n",
    "# HISTOGRAMS FOR ALL SSE Lengths.\n",
    "\n",
    "newkeys = ['H1','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13','S14','GPS']\n",
    "\n",
    "# Statistics for SSE extents\n",
    "with open('adj.sse_lengths.tsv','w') as lenfile:\n",
    "    lenfile.write(\"Element\\tLength\\n\")\n",
    "    for ki in newkeys:\n",
    "        datarow = sse_lengths[ki]\n",
    "        if \"S\" in ki: c = 'xkcd:orange'\n",
    "        else: c = 'xkcd:denim'\n",
    "        print(len(datarow))\n",
    "        plot_hist(datarow, c, ki, stal_indexing.length)\n",
    "        val_string = f'{round(np.average(datarow),1)} ± {round(np.std(datarow),1)}'\n",
    "        lenfile.write(f\"{ki}\\t{val_string}\\n\")\n",
    "\n",
    "# Statistics for SSE residue conservation\n",
    "with open(\"adj.sse_stats.tsv\", \"w\") as statfile:\n",
    "    statfile.write(\"Element\\tOccupation\\tConsensus\\n\")\n",
    "    for sse in newkeys:\n",
    "        datarow = center_res[f\"{sse}.50\"]\n",
    "        occ, residues = parse_conservation(datarow, all_base.length)\n",
    "        statfile.write(f\"{sse}\\t{occ}%\\t{residues}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TSV file with the individual pLDDT values.\n",
    "import json\n",
    "\n",
    "jsons = glob.glob('/home/hildilab/projects/agpcr_nom/*_output/batch*/*rank_1*scores.json')\n",
    "\n",
    "import re\n",
    "identifiers = []\n",
    "with open('all_plddt.tsv', 'w') as c:\n",
    "    c.write('Identifier\\tplddt_values\\n')\n",
    "    for j in jsons:\n",
    "        identifier = re.findall(r'\\/[\\w]+-', j)[0][1:-1]\n",
    "        if identifier in identifiers:\n",
    "            continue\n",
    "        identifiers.append(identifier)\n",
    "        with open(j) as jx:\n",
    "            data = json.load(jx)\n",
    "        c.write(f\"{identifier}\\t{','.join(['{:.2f}'.format(k) for k in data['plddt']])}\\n\")\n",
    "print(len(identifiers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def construct_identifiers(indexing_dir, center_dir, plddt_values, max_id_dir, name, gain_start, seq=None):\n",
    "    # construct a dictionary where for each identifier (i.e. H6.49), the total occ. is plotted\n",
    "    # This enables a statistical evaluation for each GRN labeled position.\n",
    "    seq=list(seq)\n",
    "    if seq is not None:\n",
    "        seq.append(\"X\") # Sometimes, unknown residues are present in the FASTA file.\n",
    "    id_dir = {}\n",
    "    plddts = {}\n",
    "    sse_seq = {}\n",
    "    for sse in indexing_dir.keys():\n",
    "\n",
    "        if sse == 'GPS' :\n",
    "            continue\n",
    "\n",
    "        start = indexing_dir[sse][0]\n",
    "        end = indexing_dir[sse][1]\n",
    "\n",
    "        if end-start > 45: \n",
    "            # Some models have segments which are excessively long and correspond to low pLDDT values as well. \n",
    "            # That is why manually, a limit of 45 residues was imposed.\n",
    "            print(f\"NOTE: SKIPPING TOO LONG SSE WITH LENGTH {end-start}\\n{name}: {sse}\")\n",
    "            continue\n",
    "\n",
    "        center_res = center_dir[f\"{sse}.50\"]\n",
    "        first_res = 50 - center_res + start\n",
    "\n",
    "        for k in range(end-start+1):\n",
    "            if sse not in max_id_dir.keys():\n",
    "                max_id_dir[sse] = []\n",
    "            if first_res+k not in max_id_dir[sse]:\n",
    "                max_id_dir[sse].append(first_res+k)\n",
    "\n",
    "        id_dir[sse] = [first_res+k for k in range(end-start+1)]\n",
    "        plddts[sse] = [plddt_values[k] for k in range(start, end+1)]\n",
    "\n",
    "        if seq is not None:\n",
    "            sse_seq[sse] = [seq[k] for k in range(start-gain_start, end-gain_start+1)]\n",
    "\n",
    "    if seq is None:\n",
    "        sse_seq = None\n",
    "\n",
    "    return max_id_dir, id_dir, plddts, sse_seq\n",
    "\n",
    "def get_plddt_dir(file='all_plddt.tsv'):\n",
    "    # Load the pLDDT file into a dictionary\n",
    "    plddt_dir = {}\n",
    "    with open(file) as f:\n",
    "        data = [l.strip() for l in f.readlines()[1:]]\n",
    "        for l in data:\n",
    "            i,v  = tuple(l.split(\"\\t\"))\n",
    "            plddt_dir[i] = [float(val) for val in v.split(\",\")]\n",
    "    return plddt_dir\n",
    "\n",
    "def make_id_list(id_dir):\n",
    "    # From the dictionary, convert to label strings, i.e. \"S6.50\"\n",
    "    id_list = []\n",
    "    for sse in id_dir.keys():\n",
    "        for res in id_dir[sse]:\n",
    "            id_list.append(f\"{sse}.{res}\")\n",
    "    return id_list\n",
    "\n",
    "def compact_label_positions(id_collection, plddt_collection, sse_keys, return_unique=True):\n",
    "    # Here, we stack the labeled GRN positions on top of one another to created a GRN-based multiple sequence alignment for evaluation.\n",
    "    label_plddts = {}\n",
    "    print(\"compact_label_positions:\", sse_keys)\n",
    "    for sse in sse_keys:\n",
    "        label_plddts[sse] = {}\n",
    "\n",
    "    for i in range(len(id_collection)):\n",
    "        gain_positions = id_collection[i]\n",
    "        plddt_positions = plddt_collection[i]\n",
    "        #print(plddt_positions.keys())\n",
    "        for sse in gain_positions.keys():\n",
    "            if not return_unique and \".\" in sse:\n",
    "                print(\"Found unique segment\", sse)\n",
    "                sse_id = sse.split(\".\")[0]\n",
    "            else:\n",
    "                sse_id = sse\n",
    "            for j, pos in enumerate(gain_positions[sse]):\n",
    "                pos = int(pos)\n",
    "                if pos < 10 or pos > 90:\n",
    "                    continue\n",
    "                #print(pos, label_plddts[sse_id].keys())\n",
    "                if pos not in label_plddts[sse_id].keys():\n",
    "                    label_plddts[sse_id][pos] = [plddt_positions[sse][j]]\n",
    "                else:\n",
    "                    label_plddts[sse_id][pos].append(plddt_positions[sse][j])\n",
    "\n",
    "    return label_plddts\n",
    "\n",
    "def construct_id_occupancy(intervals, center_dirs, length, plddt_dir, names, seqs):\n",
    "    newkeys = ['H1','H1.D1','H1.E1', 'H1.F4','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13','S14']\n",
    "    id_collection = []\n",
    "    plddt_collection = []\n",
    "    seq_collection = []\n",
    "    all_id_dir = {x:[] for x in newkeys}\n",
    "    for k in range(length):\n",
    "        identifier = names[k].split(\"-\")[0]\n",
    "        plddt_values = plddt_dir[identifier]\n",
    "        gain_start = valid_collection.collection[k].start\n",
    "        #print(identifier, valid_collection.collection[k].name, len(plddt_values))\n",
    "        #print(\"INPUT:\", k, intervals[k], center_dirs[k], plddt_values, all_id_dir, names[k], seqs[k], sep='\\n')\n",
    "        all_id_dir, id_dir, plddts, sse_seq = construct_identifiers(intervals[k], center_dirs[k], plddt_values, all_id_dir, names[k], gain_start, seqs[k])\n",
    "        #print(\"OUTPUT:\", all_id_dir, id_dir, plddts, sse_seq, sep='\\n')\n",
    "        id_collection.append(id_dir)\n",
    "        #print(id_dir)\n",
    "        plddt_collection.append(plddts)\n",
    "        seq_collection.append(sse_seq)\n",
    "    print(\"Completed creating value collection.\")\n",
    "    print(id_collection[0])\n",
    "    print(plddt_collection[0])\n",
    "    print(valid_collection.collection[0].name)\n",
    "    # Here, parse through the id_dirs to count the occurrence of positions per SSE\n",
    "    # Dictionary to map any label identifier to a respective position.\n",
    "    id_map = {}\n",
    "    i = 0\n",
    "    for sse in newkeys:\n",
    "        for res in all_id_dir[sse]:\n",
    "            id_map[f'{sse}.{res}'] = i \n",
    "            i += 1\n",
    "    \n",
    "    max_id_list = []\n",
    "    for i, id_dict in enumerate(id_collection):\n",
    "        max_id_list.append(make_id_list(id_dict))\n",
    "    flat_id_list = np.array([item for sublist in max_id_list for item in sublist])\n",
    "    print(\"Finished constructing flat_id_list.\")\n",
    "    labels, occ = np.unique(flat_id_list, return_counts=True)\n",
    "    occ_dict = dict(zip(labels,occ))\n",
    "\n",
    "    # Transform occ_dict to the same format as label_plddts (one dict per sse):\n",
    "    label_occ = {}\n",
    "\n",
    "    for sse in newkeys:\n",
    "        print(sse)\n",
    "        label_occ[sse] = {int(k[-2:]):v for k,v in occ_dict.items() if k[:-3] == sse}\n",
    "    \n",
    "    label_plddts = compact_label_positions(id_collection, plddt_collection, newkeys, return_unique=True)\n",
    "    label_seq = compact_label_positions(id_collection, seq_collection, newkeys, return_unique=True)\n",
    "    \n",
    "    return label_plddts, label_occ, label_seq\n",
    "\n",
    "plddt_dir = get_plddt_dir()\n",
    "\n",
    "plddt_values, occ_values, label_seq = construct_id_occupancy(stal_indexing.intervals, \n",
    "                                                             stal_indexing.center_dirs, \n",
    "                                                             stal_indexing.length, \n",
    "                                                             plddt_dir, \n",
    "                                                             stal_indexing.names, \n",
    "                                                             seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(occ_values)\n",
    "print(stal_indexing.indexing_dirs[0])\n",
    "\n",
    "# get the occupancy for GPS\n",
    "gps_occ = {\"GPS-2\":0,\"GPS-1\":0,\"GPS+1\":0}\n",
    "gps_seq = {\"GPS-2\":[],\"GPS-1\":[],\"GPS+1\":[]}\n",
    "for idx, pos_dir in enumerate(stal_indexing.indexing_dirs):\n",
    "    gain_start = valid_collection.collection[idx].start\n",
    "    current_seq = seqs[idx]\n",
    "    for k in gps_occ.keys():\n",
    "        if k in pos_dir.keys() and pos_dir[k] is not None:\n",
    "            gps_occ[k] += 1\n",
    "            gps_seq[k].append(current_seq[pos_dir[k]-gain_start])\n",
    "\n",
    "print(gps_occ)\n",
    "print(gps_seq)\n",
    "\n",
    "#print(len(seqs))\n",
    "print(label_seq[\"S4\"])\n",
    "label_seq[\"GPS\"] = gps_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT THE POSITION OCCUPANCY AND THE AVERAGE PLDDT PER POSITION. with plddt_values, occ_values\n",
    "newkeys = ['H1','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13','S14','GPS']\n",
    "#newkeys = ['H1','H1.D1','H1.E1', 'H1.F4','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13','S14','GPS']\n",
    "for sse in newkeys:\n",
    "    # Transform the values first\n",
    "    pp = plddt_values[sse]\n",
    "    #print(occ_values[sse])\n",
    "    av_pp = {k:np.average(np.array(v))/100 for k,v in pp.items()}\n",
    "    #print(av_pp)\n",
    "    norm_occ = {k:v/14435 for k,v in occ_values[sse].items()}\n",
    "    xax = sorted(av_pp.keys())\n",
    "    y_pp = [av_pp[x] for x in xax]\n",
    "    y_occ = [norm_occ[x] for x in xax]\n",
    "    norm_pp = np.array(y_pp)*np.array(y_occ)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[5,2])\n",
    "    fig.set_facecolor('w')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1)) #AutoMinorLocator())\n",
    "    ax.xaxis.set_major_locator(FixedLocator([a for a in range(2,100,3)]))#MultipleLocator(3)))\n",
    "    ax.tick_params(which='both', width=2)\n",
    "    ax.tick_params(which='major', length=8)\n",
    "    ax.tick_params(which='minor', length=6)\n",
    "    plt.bar(xax,y_pp, color='silver', alpha=0.7)\n",
    "    plt.plot(xax, y_occ, color='dodgerblue')\n",
    "    plt.bar(xax, norm_pp, color='xkcd:lightish red', alpha=0.1)\n",
    "    plt.title(f'Element Composition ({sse})')\n",
    "    plt.yticks(ticks = [0, 0.2, 0.4, 0.6, 0.8, 1], labels = ['0%', '20%', '40%', '60%', '80%', '100%'])\n",
    "    #plt.ylabel('')\n",
    "    ax.set_xticklabels([f'{sse}.{str(int(v))}' for v in ax.get_xticks()], rotation=90)\n",
    "    plt.savefig(f'../fig/r4stal/{sse}_stats2.svg', bbox_inches='tight')\n",
    "    #plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the occupancy of certain positions:\n",
    "enriched_positions = ['H1.50','H1.54','H1.57','H1.61','H2.56','H2.57','H2.60','H2.61','H3.36','H3.43',\n",
    "'H3.44','H3.51','H3.53','H3.56','H4.38','H4.41','H4.51','H5.37','H5.38','H5.42','H5.44',\n",
    "'H5.48','H5.50','H5.59','H6.42','H6.54','H6.56','H7.40','H7.51','H8.46','H8.58','H8.60',\n",
    "'S1.48','S2.47','S2.51','S2.53','S2.58','S3.53','S3.55','S5.56','S6.48','S6.52','S6.55',\n",
    "'S7.50','S8.45','S8.52','S8.57','S10.53','S11.50','S12.54','S12.55','S13.47','S13.50','S13.52','S14.48','S14.50']\n",
    "for sse in newkeys:\n",
    "    sub_positions = [k for k in enriched_positions if f'{sse}.' in k]\n",
    "    # Transform the values first\n",
    "    #pp = plddt_values[sse]\n",
    "    #print(occ_values[sse])\n",
    "    #av_pp = {k:np.average(np.array(v))/100 for k,v in pp.items()}\n",
    "    #print(av_pp)\n",
    "    norm_occ = {f'{sse}.{k}':v/14435 for k,v in occ_values[sse].items()}\n",
    "    #print(norm_occ)\n",
    "    for k in sub_positions:\n",
    "        print(round(norm_occ[k],2),k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE A FULL DATAFRAME FOR THE LABELED POSITIONS AND THEIR RESPECTIVE AA FREQUENCIES FOR LOGOPLOTS\n",
    "sse_aa_freqs = {}\n",
    "aastr = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "cols = {aa:i for i,aa in enumerate(aastr)}\n",
    "for sse in newkeys:\n",
    "    sse_dict = label_seq[sse]\n",
    "    aafreqs = np.zeros(shape=(len(sse_dict.keys()), 21))\n",
    "    for p_index, pos in enumerate(sorted(sse_dict.keys())):\n",
    "        aas, freq = np.unique(np.array(sse_dict[pos]), return_counts=True)\n",
    "        for i, aa in enumerate(aas):\n",
    "            aafreqs[p_index, cols[aa]] = freq[i]/14435\n",
    "    sse_aa_freqs[sse] = aafreqs\n",
    "root_path+\"human_31/trunc_pdbs/\"\n",
    "gps_aa_freqs = {}\n",
    "aastr = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "cols = {aa:i for i,aa in enumerate(aastr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the sequence composition for each cancer-enriched-position\n",
    "\n",
    "for sse in newkeys:\n",
    "    sub_positions = [k for k in enriched_positions if f'{sse}.' in k]\n",
    "    lframe = pd.DataFrame(data=sse_aa_freqs[sse], columns=[c for c in aastr], index = sorted(plddt_values[sse].keys()))\n",
    "    #print(lframe)\n",
    "    for pos in sub_positions:\n",
    "        idx = int(pos[-2:]) # get the row number in the SSE\n",
    "        res_data = lframe.loc[[idx]]\n",
    "        total_freqs = res_data.sum(axis=1).to_list()[0]\n",
    "        #print(f\"{total_freqs = }, {type(total_freqs)}\")\n",
    "\n",
    "        norm_freq_dict = {round(freq.to_list()[0]/total_freqs, 5) : aa for aa, freq in res_data.items()}\n",
    "        sorted_norm_freqs = sorted(norm_freq_dict.keys())[::-1]\n",
    "        #print(pos)\n",
    "        xstring = ''\n",
    "        for k in sorted_norm_freqs[:3]: \n",
    "            xstring = xstring +f'{norm_freq_dict[k]}:{round(k*100)}%,'\n",
    "        print(pos, xstring)\n",
    "\n",
    "            # normalize frequency with the total sum\n",
    "\n",
    "\"\"\"for sse in newkeys:\n",
    "    for pos in enriched_positions:\n",
    "        datarow = center_res[pos]\n",
    "        occ, residues = parse_conservation(datarow, all_base.length)\n",
    "        print(f\"{sse}\\t{occ}%\\t{residues}\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGOPLOTS FOR THE ELEMENTS\n",
    "\n",
    "print()\n",
    "print(sse_aa_freqs[\"GPS\"])\n",
    "plddt_values[\"GPS\"] = {\"GPS-2\":100,\"GPS-1\":100,\"GPS+1\":100}\n",
    "for sse in [\"GPS\"]: #newkeys:\n",
    "    if sse == \"GPS\":\n",
    "        lframe = pd.DataFrame(data=sse_aa_freqs[sse], columns=[c for c in aastr], index = [0,1,2])\n",
    "    else:\n",
    "        lframe = pd.DataFrame(data=sse_aa_freqs[sse], columns=[c for c in aastr], index = sorted(plddt_values[sse].keys()))\n",
    "\n",
    "    # Note down the first and last row where the occupation threshold is met.\n",
    "    firstval = None\n",
    "    for i, r in lframe.iterrows():\n",
    "        if np.sum(r) > 0.05: \n",
    "            if firstval is None:\n",
    "                firstval = i\n",
    "            lastval = i\n",
    "    print(firstval, lastval)\n",
    "    subframe = lframe.truncate(before=firstval, after=lastval)\n",
    "    #x_offset = sorted(plddt_values[sse].keys())[0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[5,2])\n",
    "    cons_logo = logomaker.Logo(subframe,\n",
    "                                ax=ax,\n",
    "                                color_scheme='chemistry',\n",
    "                                show_spines=False,\n",
    "                                font_name='DejaVu Sans Mono')\n",
    "\n",
    "    fig.set_facecolor('w')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1)) #AutoMinorLocator())\n",
    "    ax.xaxis.set_major_locator(FixedLocator([a for a in range(2,100,3)]))#MultipleLocator(3))\n",
    "    ax.tick_params(which='both', width=2)\n",
    "    ax.tick_params(which='major', length=8)\n",
    "    ax.tick_params(which='minor', length=6)\n",
    "    ax.set_xticklabels([f'{sse}.{str(int(v))}' for v in ax.get_xticks()], rotation=90)\n",
    "    cons_logo.draw()\n",
    "    fig.tight_layout()\n",
    "    fig.set_facecolor('w')\n",
    "    plt.savefig(f\"../fig/r4stal/conslogo_{sse}.svg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loop_stats(indexing_dir, sequence):\n",
    "    # Returns a named dict with loop lengths, i.e. {\"H1-H2\":13, \"H8-S1\":12}\n",
    "    inverted_dir = {sse[0] : (sse[1],ki) for ki, sse in indexing_dir.items() if \"GPS\" not in ki} # The begin of each sse is here {0:(13, \"H2\")}\n",
    "    loop_loc = {}\n",
    "    loop_dir = {}\n",
    "    ordered_starts = sorted(inverted_dir.keys())\n",
    "    for i, sse_start in enumerate(ordered_starts):\n",
    "        if i == 0: \n",
    "            continue # Skip the first and go from the second SSE onwards, looking in N-terminal direction.\n",
    "        c_label = inverted_dir[sse_start][1]\n",
    "        n_end, n_label = inverted_dir[ordered_starts[i-1]]\n",
    "        loop_loc[f\"{n_label}-{c_label}\"] = (n_end, sse_start-1)\n",
    "        loop_dir[f\"{n_label}-{c_label}\"] = sequence[n_end+1:sse_start] # The one-letter-coded seqeuence. Will be a list of lists\n",
    "    return loop_loc, loop_dir\n",
    "\n",
    "loop_lengths = {}\n",
    "loop_seqs = {}\n",
    "loop_seq = {}\n",
    "\n",
    "loop_info = {}\n",
    "#[loop_info[loop] = {} for loop in loop_seqs.keys()] # into each of these keys, any entry is composed of \"name\":$name, \"sequence\":$seq\n",
    "\n",
    "for idx in range(all_base.length):\n",
    "    curr_name = all_base.names[idx]\n",
    "    start = valid_collection.collection[idx].start\n",
    "    i_loc, i_dir = get_loop_stats(all_base.indexing_dirs[idx], valid_collection.collection[idx].sequence)\n",
    "    for k, seq in i_dir.items():\n",
    "        if k not in loop_info.keys():\n",
    "            loop_info[k] = []\n",
    "        loop_info[k].append({'name':f'{all_base.names[idx]}_{i_loc[k][0]+start}-{i_loc[k][1]+start}', 'sequence':''.join(seq)})\n",
    "    #print(i_len)\n",
    "    #loop_lengths = match_dirs(i_len, loop_lengths)\n",
    "    #loop_seqs = match_dirs(i_dir, loop_seqs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the collected loop sequences to a FASTA file for later alignment.\n",
    "def loop2fasta(outfile, itemlist):\n",
    "    with open(outfile, 'w') as out:\n",
    "        for subdict in itemlist:\n",
    "            out.write(f\">{subdict['name']}\\n{subdict['sequence']}\\n\")\n",
    "    print(\"Done with\", outfile)\n",
    "\n",
    "for loop in loop_info.keys():\n",
    "    loop2fasta(f\"../loops/{loop}.fa\", loop_info[loop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extents = {}\n",
    "for gain in valid_collection.collection:\n",
    "    extents[gain.name.split(\"-\")[0]] = [str(gain.start+1), str(gain.subdomain_boundary+1), str(gain.end+1)] # make it compatible with ONE-indexed PDBs.\n",
    "\n",
    "import json\n",
    "with open('domain_extents.json', 'w') as j:\n",
    "    dump = json.dumps(extents)\n",
    "    j.write(dump)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "efcc3436bf700bf51081b251413b556e30c22be82f452601745119c8a669a2f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
