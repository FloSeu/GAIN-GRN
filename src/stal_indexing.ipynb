{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook to construct the complete GAIN GRN indexing on a collection of GAIN domains.\n",
    "Requirements:\n",
    "> - GESAMT binary\n",
    "> - STRIDE set of files (one for each entry in the dataset, here we use float-modified STRIDE files for the outliers)\n",
    "> - A Folder of template PDBs\n",
    "> - template_data.json with all information about the template elements and centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logomaker\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FixedLocator)\n",
    "# LOCAL IMPORTS\n",
    "import sse_func\n",
    "import template_finder as tf\n",
    "from indexing_classes import StAlIndexing\n",
    "\n",
    "gesamt_bin = \"/home/hildilab/lib/xtal/ccp4-8.0/ccp4-8.0/bin/gesamt\"\n",
    "\n",
    "def find_pdb(name, pdb_folder):\n",
    "    identifier = name.split(\"-\")[0]\n",
    "    target_pdb = glob.glob(f\"{pdb_folder}/*{identifier}*.pdb\")[0]\n",
    "    return target_pdb\n",
    "\n",
    "def find_offsets(fasta_file, accessions, sequences):\n",
    "    # FIND THE ACTUAL OFFSET OF RESIDUES BETWEEN AN ALPHAFOLD MODEL AND THE UNIPROT ENTRY PROTEIN\n",
    "    #   - searches through the accessions in the big sequence file,\n",
    "    #   - finds the start for the provided sequence\n",
    "    with open(fasta_file,\"r\") as fa:\n",
    "        fa_data = fa.read()\n",
    "        fasta_entries = fa_data.split(\">\")\n",
    "    seqs = []\n",
    "    headers = []\n",
    "    offsets = []\n",
    "    for seq in fasta_entries:\n",
    "        # Fallback for too short sequences\n",
    "        if len(seq) < 10: \n",
    "            continue\n",
    "        data = seq.strip().split(\"\\n\")\n",
    "        headers.append(data[0].split(\"|\")[1]) # This is only the UniProtKB Accession Number and will be matched EXACTLY\n",
    "        seqs.append(\"\".join(data[1:]))\n",
    "    \n",
    "    heads = np.array(headers)\n",
    "    for idx, accession in enumerate(accessions):\n",
    "        seq_idx = np.where(heads == accession)[0][0]\n",
    "        offset = \"\".join(seqs[seq_idx]).find(\"\".join(sequences[idx]))\n",
    "        print(seqs[seq_idx], \"\".join(sequences[idx]), offset, sep=\"\\n\")\n",
    "        offsets.append(offset)\n",
    "    \n",
    "    return offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the GainCollection objects to be indexed. Here, we have the whole 14435 structure set (valid_collection) and the 31 structure set (human_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_collection = pd.read_pickle(\"../valid_collection.q.pkl\")\n",
    "human_collection = pd.read_pickle(\"../human_collection.q.pkl\")\n",
    "\n",
    "xlist = [\"A0A2Y9F628\",\n",
    "        \"A0A7K6E127\",\n",
    "        \"A0A1A7WJQ6\",\n",
    "        \"A0A2I2YJG7\",\n",
    "        \"G1TKX5\",\n",
    "        \"A0A7L3N0A5\",\n",
    "        \"W5PQ70\",\n",
    "        \"A0A2K5Y1I7\",\n",
    "        \"A0A452HCU9\",\n",
    "        \"A0A7L3KTA8\",\n",
    "        \"A0A6Q2XYK2\",\n",
    "        \"A0A7L3GD10\",\n",
    "        \"A0A6J3IBI5\",\n",
    "        \"A0A3P8S994\",\n",
    "        \"A0A6J3IBI5\",\n",
    "        \"A0A2I4CCH8\"\n",
    "]\n",
    "\n",
    "seqs = []\n",
    "accs = []\n",
    "for gain in valid_collection.collection:\n",
    "    if gain.name.split(\"-\")[0] in xlist:\n",
    "        print(gain.name)\n",
    "        seqs.append(gain.sequence)\n",
    "        accs.append(gain.name.split(\"-\")[0])\n",
    "\n",
    "xoffsets = find_offsets(\"../agpcr_celsr.fasta\", accs, seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For testing, in this cell an individual indexing can be constructed. \n",
    "Setting _debug=True_ will result in a large amount of information being printed, enabling the tracing of errors and irregularities during the assignment process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gain in enumerate(human_collection.collection):\n",
    "    #if \"Q8IZF6\" not in gain.name: # Specify a Uniprot identifier here.\n",
    "    #    continue\n",
    "    file_prefix = f\"../human_gesamt/f3_test{gain.name}\" # a temp folder where calculations and outputs will be stored.\n",
    "    print(\"_\"*30, f\"\\n{i} {gain.name}\")\n",
    "    element_intervals, element_centers, residue_labels, unindexed_elements, params = tf.assign_indexing(gain, \n",
    "                                                                                                file_prefix=file_prefix, \n",
    "                                                                                                gain_pdb=find_pdb(gain.name, '../all_pdbs'), \n",
    "                                                                                                template_dir='../r4_template_pdbs/',\n",
    "                                                                                                template_json='template_data.json',\n",
    "                                                                                                outlier_cutoff=5.0,\n",
    "                                                                                                gesamt_bin=gesamt_bin,\n",
    "                                                                                                debug=True, \n",
    "                                                                                                create_pdb=True,\n",
    "                                                                                                hard_cut={\"S2\":7,\"S7\":3,\"H5\":3},\n",
    "                                                                                                patch_gps=True\n",
    "                                                                                                )\n",
    "    # This denotes the priority line of splitting.\n",
    "    split_modes = {\n",
    "        0:\"No Split.\",\n",
    "        1:\"Split by coiled residue.\",\n",
    "        2:\"Split by disordered residue.\",\n",
    "        #3:\"Split by Proline/Glycine\",\n",
    "        4:\"Split by hard cut.\",\n",
    "        5:\"Overwrite by anchor priority.\"\n",
    "    }\n",
    "    #print(gain.name, gain.subdomain_boundary)\n",
    "    #if params[\"split_mode\"] > 0:\n",
    "    #    print(params[\"split_mode\"], split_modes[params[\"split_mode\"]])\n",
    "    #print(element_intervals, element_centers, residue_labels, unindexed_elements, sep=\"\\n\")\n",
    "    print(unindexed_elements, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, the full __StAlIndexing__ may be constructed test-wise, or by default the pickle of the Indexing is loaded. \n",
    "Keep in mind that within this jupyter notebook - due to its handling of multiprocessig.Pool - the number of threads is limited to 1 and this takes a while for the full set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessions = [gain.name.split(\"-\")[0].split(\"_\")[0] for gain in valid_collection.collection]\n",
    "sequences = [\"\".join(gain.sequence) for gain in valid_collection.collection]\n",
    "\n",
    "fasta_offsets = find_offsets(\"../agpcr_celsr.fasta\",\n",
    "                                 accessions, \n",
    "                                 sequences)\n",
    "\n",
    "# Pseudocenter cases: cases, where the segment center is NOT part of the segment in question. Therefore, an alternative indexing method is applied,\n",
    "# using a \"pseudocenter\", a residue which matches the segment, but is not the .50 residue.\n",
    "ps_file = \"../pseudocenters.csv\"\n",
    "open(ps_file,\"w\").write(f\"GAIN,res,elem\\n\")\n",
    "\n",
    "# Careful when running, this takes a lot of time to calculate on single-thread. use run_indexing.py for fast multithreaded calculation.\n",
    "stal_indexing = StAlIndexing(valid_collection.collection, \n",
    "                             prefix=\"../test_stal_indexing/test\", \n",
    "                             pdb_dir='../all_pdbs/',\n",
    "                             template_json='template_data.json',\n",
    "                             gesamt_bin=gesamt_bin, \n",
    "                             template_dir='../r4_template_pdbs/', \n",
    "                             fasta_offsets=fasta_offsets,\n",
    "                             n_threads=1,\n",
    "                             pseudocenters=ps_file,\n",
    "                             debug=False)\n",
    "#with open(\"stal_indexing.pkl\",\"wb\") as save:\n",
    "#    pickle.dump(stal_indexing, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-calculated STAL indexing, which saves some time.\n",
    "stal_indexing = pd.read_pickle(\"../stal_indexing.r4.pkl\")\n",
    "\n",
    "header, matrix = stal_indexing.construct_data_matrix(unique_sse=False)\n",
    "stal_indexing.data2csv(header, matrix, \"stal_indexing.r4.csv\")\n",
    "header, matrix = stal_indexing.construct_data_matrix(unique_sse=True)\n",
    "stal_indexing.data2csv(header, matrix, \"stal_indexing.r4u.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Analysis and Graphical statistics for StAlIndexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Block for parsing information\n",
    "def get_loops(indexing_dir):\n",
    "    # Returns a named dict with loop lengths, i.e. {\"H1-H2\":13, \"H8-S1\":12}\n",
    "    inverted_dir = {sse[0] : (sse[1],ki) for ki, sse in indexing_dir.items()} # The begin of each sse is here {0:(13, \"H2\")}\n",
    "    loop_dir = {}\n",
    "    ordered_starts = sorted(inverted_dir.keys())\n",
    "    for i, sse_start in enumerate(ordered_starts):\n",
    "        if i == 0: \n",
    "            continue # Skip the first and go from the second SSE onwards, looking in N-terminal direction.\n",
    "        c_label = inverted_dir[sse_start][1]\n",
    "        n_end, n_label = inverted_dir[ordered_starts[i-1]]\n",
    "        loop_dir[f\"{n_label}-{c_label}\"] = sse_start - n_end - 1\n",
    "    return loop_dir\n",
    "\n",
    "def get_sse_len(indexing_dir, total_keys):\n",
    "    # Returns a dict with the length of each SSE in respective GAIN domain.\n",
    "    len_dir = {x:0 for x in total_keys}\n",
    "    for ki in indexing_dir.keys():\n",
    "        start = indexing_dir[ki][0]\n",
    "        end = indexing_dir[ki][1]\n",
    "        len_dir[ki] = end - start + 1\n",
    "    return len_dir\n",
    "\n",
    "def get_pos_res(pos_dir, gain):\n",
    "    # Returns a dict with the One-Letter-Code of each SSE position in the respective GAIN domain.\n",
    "    pos_res = {k : gain.sequence[v-gain.start] for k,v in pos_dir.items() if v is not None and v-gain.start < len(gain.sequence)}\n",
    "    return pos_res\n",
    "\n",
    "def match_dirs(single_dir, collection_dir, exclude=[]):\n",
    "    for k, v in single_dir.items():\n",
    "        if v in exclude:\n",
    "            continue\n",
    "        if k not in collection_dir.keys():\n",
    "            collection_dir[k] = [v]\n",
    "            continue\n",
    "        collection_dir[k].append(v)\n",
    "    return collection_dir\n",
    "\n",
    "def plot_hist(datarow, color, name, length):\n",
    "    max = np.max(datarow)\n",
    "    try: \n",
    "        dens = stats.gaussian_kde(datarow)\n",
    "    except:\n",
    "        print(np.unique(datarow))\n",
    "        return\n",
    "    fig = plt.figure(figsize=[4,2])\n",
    "    fig.set_facecolor('w')\n",
    "    n, x, _ = plt.hist(datarow, bins=np.linspace(0,max,max+1), histtype=u'step', density=True, color='white',alpha=0)\n",
    "    plt.plot(x, dens(x),linewidth=2,color=color,alpha=1)\n",
    "    plt.fill_between(x,dens(x), color=color,alpha=0.1)\n",
    "    ax = plt.gca()\n",
    "    ymax = ax.get_ylim()[1]\n",
    "    val_string = f'{round(np.average(datarow),2)}±{round(np.std(datarow),2)}'\n",
    "    plt.text(max, ymax*0.95, name, horizontalalignment='right', fontsize=14, verticalalignment='top')\n",
    "    plt.text(max, ymax*0.8, val_string, horizontalalignment='right', fontsize=14, verticalalignment='top')\n",
    "    plt.text(max, ymax*0.65, f\"{round(len(datarow)/length*100, 1)}%\", horizontalalignment='right', fontsize=14, verticalalignment='top')\n",
    "    plt.xlabel('Element Length [Residues]')\n",
    "    plt.ylabel('Relative density [AU]')\n",
    "    plt.savefig(f'{name}_hist.svg')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def parse_conservation(datarow, length):\n",
    "    total = len(datarow)\n",
    "    letters, counts = np.unique(np.array(datarow), return_counts=True)\n",
    "\n",
    "    resid_counts = {}\n",
    "    for i, res in enumerate(letters):\n",
    "            resid_counts[int(counts[i])] = res\n",
    "    \n",
    "    sorted_counts = sorted(resid_counts.keys())[::-1]\n",
    "\n",
    "    occupancy = round(total/length*100, 1)\n",
    "    conserv_string = []\n",
    "    residue_occupancies = [ int( x*100 / total ) for x in sorted_counts]\n",
    "    for idx, occ in enumerate(residue_occupancies):\n",
    "        if occ >= 5: conserv_string.append(f\"{resid_counts[sorted_counts[idx]]}:{occ}%\")\n",
    "\n",
    "    return occupancy, \", \".join(conserv_string)\n",
    "\n",
    "def construct_identifiers(intervals:dict, center_dir:dict, plddt_values:dict, max_id_dir:dict, name:str, seq=None, gain_start=0, debug=False):\n",
    "    id_dir = {}\n",
    "    plddts = {}\n",
    "    sse_seq = {}\n",
    "    if debug:\n",
    "        print(\"DEBUG\",f\"{len(plddt_values) = }\", f\"{len(seq) = }\", f\"{gain_start = }\", sep=\"\\n\\t\")\n",
    "    for sse in intervals.keys():\n",
    "        if sse == 'GPS' :\n",
    "            continue\n",
    "        start = intervals[sse][0]\n",
    "        end = intervals[sse][1]\n",
    "        if end-start > 45:\n",
    "            print(f\"NOTE: SKIPPDING TOO LONG SSE WITH LENGTH {end-start}\\n{name}: {sse}\")\n",
    "            continue\n",
    "        center_resid = center_dir[f\"{sse}.50\"]\n",
    "        first_resid = 50 - center_resid + start\n",
    "        for k in range(end-start+1):\n",
    "            if sse not in max_id_dir.keys():\n",
    "                max_id_dir[sse] = []\n",
    "            if first_resid+k not in max_id_dir[sse]:\n",
    "                max_id_dir[sse].append(first_resid+k)\n",
    "        id_dir[sse] = [first_resid+k for k in range(end-start+1)]\n",
    "        plddts[sse] = [plddt_values[k] for k in range(start, end+1)]\n",
    "        if seq is not None:\n",
    "            sse_seq[sse] = [seq[k-gain_start] for k in range(start, end+1) if k-gain_start<len(seq)]\n",
    "    if seq is None:\n",
    "        sse_seq = None\n",
    "    return max_id_dir, id_dir, plddts, sse_seq\n",
    "\n",
    "def get_plddt_dir(file='all_plddt.tsv'):\n",
    "    plddt_dir = {}\n",
    "    with open(file) as f:\n",
    "        data = [l.strip() for l in f.readlines()[1:]]\n",
    "        for l in data:\n",
    "            i,v  = tuple(l.split(\"\\t\"))\n",
    "            plddt_dir[i] = [float(val) for val in v.split(\",\")]\n",
    "    return plddt_dir\n",
    "\n",
    "def make_id_list(id_dir):\n",
    "    id_list = []\n",
    "    for sse in id_dir.keys():\n",
    "        for res in id_dir[sse]:\n",
    "            id_list.append(f\"{sse}.{res}\")\n",
    "    return id_list #np.array(id_list)\n",
    "\n",
    "def compact_label_positions(id_collection, plddt_collection, sse_keys, debug=False):\n",
    "    # Stacks label positions on one another\n",
    "    label_plddts = {}\n",
    "    for sse in sse_keys:\n",
    "        label_plddts[sse] = {}\n",
    "\n",
    "    for i in range(len(id_collection)):\n",
    "        gain_positions = id_collection[i]\n",
    "        plddt_positions = plddt_collection[i]\n",
    "        if debug: \n",
    "            print(i,gain_positions, plddt_positions, sep=\"\\n\")\n",
    "        for sse, v in gain_positions.items():\n",
    "            if v == []:\n",
    "                continue\n",
    "            for j, pos in enumerate(v):\n",
    "                pos = int(pos)\n",
    "                if j >= len(plddt_positions[sse]):\n",
    "                    continue\n",
    "                if pos not in label_plddts[sse].keys():\n",
    "                    label_plddts[sse][pos] = [plddt_positions[sse][j]]\n",
    "                else:\n",
    "                    label_plddts[sse][pos].append(plddt_positions[sse][j])\n",
    "\n",
    "    return label_plddts\n",
    "\n",
    "def construct_id_occupancy(indexing_dirs, center_dirs, length, plddt_dir, names, seqs, starts:list, debug=False):\n",
    "    newkeys = ['H1','H1.D1','H1.E1','H1.F4','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13','S14']\n",
    "    id_collection = []\n",
    "    plddt_collection = []\n",
    "    seq_collection = []\n",
    "    all_id_dir = {x:[] for x in newkeys}\n",
    "    for k in range(length):\n",
    "        identifier = names[k].split(\"-\")[0]\n",
    "        plddt_values = plddt_dir[identifier]\n",
    "        all_id_dir, id_dir, plddts, sse_seq = construct_identifiers(indexing_dirs[k], center_dirs[k], plddt_values, all_id_dir, names[k], seqs[k], starts[k], debug=debug)\n",
    "        #print(k, sse_seq)\n",
    "        id_collection.append(id_dir)\n",
    "        #print(id_dir)\n",
    "        plddt_collection.append(plddts)\n",
    "        seq_collection.append(sse_seq)\n",
    "    print(\"Completed creating value collection.\")\n",
    "    print(id_collection[0])\n",
    "    print(plddt_collection[0])\n",
    "\n",
    "    # Here, parse through the id_dirs to count the occurrence of positions per SSE\n",
    "    # Dictionary to map any label identifier to a respective position.\n",
    "    id_map = {}\n",
    "    i = 0\n",
    "    for sse in newkeys:\n",
    "        for res in all_id_dir[sse]:\n",
    "            id_map[f'{sse}.{res}'] = i \n",
    "            i += 1\n",
    "    \n",
    "    max_id_list = []\n",
    "    for i, id_dict in enumerate(id_collection):\n",
    "        max_id_list.append(make_id_list(id_dict))\n",
    "    flat_id_list = np.array([item for sublist in max_id_list for item in sublist])\n",
    "    print(\"Finished constructing flat_id_list.\")\n",
    "    labels, occ = np.unique(flat_id_list, return_counts=True)\n",
    "    # Parse through labels, occ to generate the sse-specific data\n",
    "    occ_dict = {labels[u]:occ[u] for u in range(len(labels))}\n",
    "    # Transform occ_dict to the same format as label_plddts (one dict per sse):\n",
    "    label_occ = {}\n",
    "    for sse in newkeys:\n",
    "        label_occ[sse] = {int(k[-2:]):v for k,v in occ_dict.items() if sse in k}\n",
    "    #print(labels, occ)\n",
    "    label_plddts = compact_label_positions(id_collection, plddt_collection, newkeys, debug=debug)\n",
    "    label_seq = compact_label_positions(id_collection, seq_collection, newkeys, debug=debug)\n",
    "    #print(labels)\n",
    "    return label_plddts, label_occ, label_seq\n",
    "    #[print(k, len(v)) for k,v in label_plddts.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct an Occupancy Matrix of each element in correspondence with each other element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCCUPANCY MATRIX\n",
    "print(stal_indexing.total_keys)\n",
    "#print(dir(stal_indexing))\n",
    "newkeys = ['H1','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13','S14']\n",
    "# for unique element assignments, use the keys below. These have a really low frequency though.\n",
    "#newkeys = ['H1','H1.D1','H1.E1','H1.F4','H2','H3','H4','H5','H6','S1','S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S12','S13','S14']\n",
    "\n",
    "loop_lengths = {}\n",
    "sse_lengths = {}\n",
    "center_residues = {}\n",
    "sse_matrix = np.zeros(shape=(len(stal_indexing.total_keys),len(stal_indexing.total_keys)))\n",
    "for idx in range(stal_indexing.length):\n",
    "    #Sanity Check - Do the identifiers match? Yes, they do.\n",
    "    #if stal_indexing.names[idx].split(\"-\")[0] != valid_collection.collection[idx].name.split(\"-\")[0]:\n",
    "    #    print(stal_indexing.names[idx].split(\"-\")[0], valid_collection.collection[idx].name.split(\"-\")[0])\n",
    "    #    raise IndexError\n",
    "    loop_lengths = match_dirs(get_loops(stal_indexing.intervals[idx]), loop_lengths)\n",
    "    sse_lengths = match_dirs(get_sse_len(stal_indexing.intervals[idx], stal_indexing.total_keys), sse_lengths, exclude=[0])\n",
    "    center_res = match_dirs(get_pos_res(stal_indexing.center_dirs[idx], valid_collection.collection[idx]), center_residues)\n",
    "\n",
    "    present_sse = stal_indexing.intervals[idx].keys()\n",
    "    for i, kk in enumerate(newkeys):\n",
    "        for j in range(i,len(newkeys)):\n",
    "            if kk in present_sse and newkeys[j] in present_sse:\n",
    "                sse_matrix[j,i] += 1\n",
    "\n",
    "plt.imshow(sse_matrix, cmap='gist_yarg')\n",
    "plt.xticks(ticks= range(len(newkeys)), labels=newkeys, rotation=90)\n",
    "plt.yticks(ticks= range(len(newkeys)), labels=newkeys)\n",
    "plt.xlim(-0.5,19.5)\n",
    "plt.ylim(19.5,-0.5)\n",
    "cbar = plt.colorbar(shrink=0.5)\n",
    "plt.savefig(\"r4_stal_occ_map_unique.svg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the _all\\_plddt.tsv_ file containing info about the AlphaFold2 confidence values, read them in and construct a data matrix for evaluating the element quality and occupancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plddt_dir = get_plddt_dir('all_plddt.tsv')\n",
    "all_starts = [ gain.start for gain in valid_collection.collection ]\n",
    "#print(list(plddt_dir.keys())[:10])\n",
    "plddt_values, occ_values, label_seq = construct_id_occupancy(stal_indexing.intervals, \n",
    "                                                             stal_indexing.center_dirs, \n",
    "                                                             stal_indexing.length, \n",
    "                                                             plddt_dir, \n",
    "                                                             stal_indexing.names, \n",
    "                                                             stal_indexing.sequences,\n",
    "                                                             all_starts,\n",
    "                                                             debug=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the statistics for each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sse in newkeys:\n",
    "    # Transform the values first\n",
    "    pp = plddt_values[sse]\n",
    "    #print(occ_values[sse])\n",
    "    av_pp = {k:np.average(np.array(v))/100 for k,v in pp.items()}\n",
    "    #print(av_pp)\n",
    "    norm_occ = {k:v/14435 for k,v in occ_values[sse].items()}\n",
    "    xax = sorted(av_pp.keys())\n",
    "    y_pp = [av_pp[x] for x in xax]\n",
    "    y_occ = [norm_occ[x] for x in xax]\n",
    "    norm_pp = np.array(y_pp)*np.array(y_occ)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[5,2])\n",
    "    fig.set_facecolor('w')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1)) #AutoMinorLocator())\n",
    "    ax.xaxis.set_major_locator(FixedLocator([a for a in range(2,100,3)]))#MultipleLocator(3)))\n",
    "    ax.tick_params(which='both', width=2)\n",
    "    ax.tick_params(which='major', length=8)\n",
    "    ax.tick_params(which='minor', length=6)\n",
    "    plt.bar(xax,y_pp, color='silver', alpha=0.7)\n",
    "    plt.plot(xax, y_occ, color='dodgerblue')\n",
    "    plt.bar(xax, norm_pp, color='xkcd:lightish red', alpha=0.1)\n",
    "    plt.title(f'Element Composition ({sse})')\n",
    "    plt.yticks(ticks = [0, 0.2, 0.4, 0.6, 0.8, 1], labels = ['0%', '20%', '40%', '60%', '80%', '100%'])\n",
    "    #plt.ylabel('')\n",
    "    ax.set_xticklabels([f'{sse}.{str(int(v))}' for v in ax.get_xticks()], rotation=90)\n",
    "    plt.savefig(f'../fig/r4stal/stal_{sse}_stats.svg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a DataFrame for AA-logoplots and plot them element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE A FULL DATAFRAME FOR THE LABELED POSITIONS AND THEIR RESPECTIVE AA FREQUENCIES FOR LOGOPLOTS\n",
    "sse_aa_freqs = {}\n",
    "aastr = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "cols = {aa:i for i,aa in enumerate(aastr)}\n",
    "for sse in newkeys:\n",
    "    sse_dict = label_seq[sse]\n",
    "    aafreqs = np.zeros(shape=(len(sse_dict.keys()), 21))\n",
    "    for p_index, pos in enumerate(sorted(sse_dict.keys())):\n",
    "        aas, freq = np.unique(np.array(sse_dict[pos]), return_counts=True)\n",
    "        for i, aa in enumerate(aas):\n",
    "            aafreqs[p_index, cols[aa]] = freq[i]/14435\n",
    "    sse_aa_freqs[sse] = aafreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGOPLOTS FOR THE ELEMENTS\n",
    "\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "for sse in newkeys:\n",
    "\n",
    "    lframe = pd.DataFrame(data=sse_aa_freqs[sse], columns=[c for c in aastr], index = sorted(plddt_values[sse].keys()))\n",
    "\n",
    "    # Note down the first and last row where the occupation threshold is met.\n",
    "    firstval = None\n",
    "    for i, r in lframe.iterrows():\n",
    "        if np.sum(r) > 0.05: \n",
    "            if firstval is None:\n",
    "                firstval = i\n",
    "            lastval = i\n",
    "    print(firstval, lastval)\n",
    "    subframe = lframe.truncate(before=firstval, after=lastval)\n",
    "    #x_offset = sorted(plddt_values[sse].keys())[0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[5,2])\n",
    "    cons_logo = logomaker.Logo(subframe,\n",
    "                                ax=ax,\n",
    "                                color_scheme='chemistry',\n",
    "                                show_spines=False,\n",
    "                                font_name='DejaVu Mono')\n",
    "\n",
    "    fig.set_facecolor('w')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1)) #AutoMinorLocator())\n",
    "    ax.xaxis.set_major_locator(FixedLocator([a for a in range(2,100,3)]))#MultipleLocator(3))\n",
    "    ax.tick_params(which='both', width=2)\n",
    "    ax.tick_params(which='major', length=8)\n",
    "    ax.tick_params(which='minor', length=6)\n",
    "    ax.set_xticklabels([f'{sse}.{str(int(v))}' for v in ax.get_xticks()], rotation=90)\n",
    "    cons_logo.draw()\n",
    "    fig.tight_layout()\n",
    "    fig.set_facecolor('w')\n",
    "    plt.savefig(f\"../fig/r4stal/stal_conslogo_{sse}.svg\", bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle the issue of a very broad assignment of some elements, there is a two-fold modification of STRIDE files in place:\n",
    "\n",
    "1. Every residue outside of 2 Sigmas of the mean (keep in mind, this is circular statistics) gets assigned a lower-case letter as the SSE descriptor, enabling resolving element ambiguities\n",
    "2. The multiple of sigmas for outliers is written into columns 66-70 of the stride file. If this exceeds a defines threshold (usually 5.0), the element is truncated here always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTLIER MODE 2\n",
    "# modify all stride files in place, including the sigma multiplier of the outlier\n",
    "def modify_stride(stride_file, outfolder, phi_lim, psi_lim, n_sigma=2.0):\n",
    "    outliers = []\n",
    "    # also add the max float mult of sigma into the \"~~~~\" (line[75:79])\n",
    "    # \"{:.2f}\".format(maxsigma)\n",
    "    with open(stride_file) as stride:\n",
    "        d = stride.readlines()\n",
    "    newdata = []\n",
    "    for l in d:\n",
    "        if not l.startswith(\"ASG\") or l[24] != \"E\":\n",
    "            newdata.append(l)\n",
    "            continue\n",
    "\n",
    "        i = l.split()\n",
    "        angles = [float(i[7]), float(i[8])]\n",
    "        adj_angles = [a+360 if a<0 else a for a in angles]\n",
    "        if abs(sse_func.angle_diff(adj_angles[0], phi_lim[0])) > n_sigma*phi_lim[1] or abs(sse_func.angle_diff( adj_angles[1], psi_lim[0])) > n_sigma*psi_lim[1]:\n",
    "            # print(\"outlier found.\", l, sep=\"\\n\")\n",
    "            maxsigma = max([ abs(sse_func.angle_diff(adj_angles[0], phi_lim[0]) / phi_lim[1]) , \n",
    "                             abs(sse_func.angle_diff(adj_angles[1],psi_lim[0]) / psi_lim[1])    \n",
    "                           ])\n",
    "            k = l[:24]+\"e\"+l[25:75]+\"{:.2f}\".format(maxsigma)+\"\\n\"\n",
    "            #print(\"DEBUG:\", k)\n",
    "            newdata.append(k)\n",
    "            outliers.append(round(maxsigma, 2))\n",
    "            continue\n",
    "        \n",
    "        newdata.append(l)\n",
    "    \n",
    "    open(f\"{outfolder}/{stride_file.split('/')[-1]}\", 'w').write(\"\".join(newdata))\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sse_func, glob\n",
    "# GET THE BB ANGLE DISTRIBUTION. THIS TAKES A WHILE, THIS IS WHY THE RESULTING VALUES ARE STATED HARDCODED BELOW.\n",
    "\n",
    "#stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2/*\")\n",
    "#hphi, hpsi, sphi, spsi = sse_func.get_bb_distribution(stride_files)\n",
    "#print(hphi, hpsi, sphi, spsi)\n",
    "\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2/*\")\n",
    "phi_lim = [-113.01754866504291, 29.968104201971208]#[245.248, 30.129] # This is mean and SD of the Angle PHI\n",
    "psi_lim = [132.75257372738366, 31.172184167730734]#[136.615, 33.950] #                                   PSI\n",
    "outfolder = \"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2_floats\"\n",
    "outliers = []\n",
    "for stride_file in stride_files:\n",
    "    outliers += modify_stride(stride_file, outfolder, phi_lim, psi_lim)\n",
    "print(max(outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we construct the indexing for the human set with the modified STRIDE files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_collection = pd.read_pickle(\"../human_collection.q.pkl\")\n",
    "\n",
    "human_accessions = [gain.name.split(\"-\")[0].split(\"_\")[0] for gain in human_collection.collection]\n",
    "human_sequences = [\"\".join(gain.sequence) for gain in human_collection.collection]\n",
    "\n",
    "human_fasta_offsets = find_offsets(\"/home/hildilab/projects/GPS_massif/uniprot_query/agpcr_celsr.fasta\", \n",
    "                                 human_accessions, \n",
    "                                 human_sequences)\n",
    "\n",
    "for i, gain in enumerate(human_collection.collection):\n",
    "    element_intervals, element_centers, residue_labels, unindexed_elements, params = tf.assign_indexing(gain, \n",
    "                                                                                                file_prefix=f\"../test_stal_indexing/human_{i}_{gain.name.split('-')[0]}\", \n",
    "                                                                                                gain_pdb=find_pdb(gain.name, '../all_pdbs'), \n",
    "                                                                                                template_dir='../r4_template_pdbs/',\n",
    "                                                                                                gesamt_bin=gesamt_bin,\n",
    "                                                                                                debug=True, \n",
    "                                                                                                create_pdb=True,\n",
    "                                                                                                template_json='template_data.json',\n",
    "                                                                                                hard_cut={\"S2\":7,\"S6\":3,\"H5\":3},\n",
    "                                                                                                patch_gps=True\n",
    "                                                                                                )\n",
    "stal_human_indexing = stal_indexing = StAlIndexing(human_collection.collection, \n",
    "                             prefix=\"../test_stal_indexing/test\", \n",
    "                             pdb_dir='../all_pdbs/',  \n",
    "                             template_dir='../r4_template_pdbs/', \n",
    "                             template_json = 'template_data.json',\n",
    "                             outlier_cutoff=5.0,\n",
    "                             fasta_offsets=human_fasta_offsets,\n",
    "                             gesamt_bin=gesamt_bin,\n",
    "                             n_threads=1,\n",
    "                             debug=True)\n",
    "\n",
    "header, matrix = stal_human_indexing.construct_data_matrix(overwrite_gps=True, unique_sse=False)\n",
    "stal_human_indexing.data2csv(header, matrix, \"human_indexing.r4.csv\")\n",
    "\n",
    "# Also include unique Helices in a separate file.\n",
    "header, matrix = stal_human_indexing.construct_data_matrix(overwrite_gps=True, unique_sse=True)\n",
    "stal_human_indexing.data2csv(header, matrix, \"human_indexing.r4u.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Indexing for the whole set is best constructed via multithreaded exection by _stal\\_indexing.py_ and saved in a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gain in enumerate(human_collection.collection): print(i, gain.name, human_fasta_offsets[i]-gain.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stal_indexing = pd.read_pickle(\"../stal_indexing.r4.pkl\")\n",
    "print(dir(stal_indexing))\n",
    "print(stal_indexing.accessions[0])\n",
    "print(stal_indexing.indexing_dirs[0])\n",
    "print(stal_indexing.receptor_types[0])\n",
    "e4_species = []\n",
    "for i, gain in enumerate(valid_collection.collection):\n",
    "    if stal_indexing.receptor_types[i] == 'E4':\n",
    "        e4_species.append(gain.name.split(\"-\")[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(e4_species), np.unique(e4_species), len(np.unique(e4_species)))\n",
    "print(\"\\n\".join(np.unique(e4_species)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gain_classes import GainDomainNoAln\n",
    "\n",
    "pkd_gain = GainDomainNoAln(start=None,\n",
    "                           subdomain_boundary=None, \n",
    "                           end=None, \n",
    "                           fasta_file=None, \n",
    "                           name='hPKD1', \n",
    "                           sequence=None, \n",
    "                           explicit_stride_file=\"../hpkd1/PKD1_1.stride\")\n",
    "\n",
    "element_intervals, element_centers, residue_labels, unindexed_elements, params = tf.assign_indexing(pkd_gain, \n",
    "                                                                                                file_prefix=f\"../test_stal_indexing/hpkd1\", \n",
    "                                                                                                gain_pdb=\"../hpkd1/PKD1_HUMAN_unrelaxed_rank_1_model_3.pdb\", \n",
    "                                                                                                template_dir='../r4_template_pdbs/',\n",
    "                                                                                                gesamt_bin=gesamt_bin,\n",
    "                                                                                                debug=True, \n",
    "                                                                                                create_pdb=True,\n",
    "                                                                                                template_json='template_data.json',\n",
    "                                                                                                hard_cut={\"S2\":7,\"S6\":3,\"H5\":3},\n",
    "                                                                                                patch_gps=True\n",
    "                                                                                                )\n",
    "offset = 2271 # resid 1 in PDB = 2272 in Sequence\n",
    "offset_labels = {k:v+offset for k,v in residue_labels.items()}\n",
    "print(offset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../hpkd1/pkd1_grn.csv\", \"w\") as c:\n",
    "    c.write(\"GRN,residue\\n\")\n",
    "    for k,v in offset_labels.items():\n",
    "        c.write(f\"{k},{v}\")\n",
    "        c.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
