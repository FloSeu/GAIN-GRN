{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a database from a dataset for nomenclating\n",
    "# INPUT: a collection of GAIN domain PDBs, their sequences as one large \".fa\" file\n",
    "from gain_classes import GainDomain, GainCollection, Anchors, GPS\n",
    "import sse_func\n",
    "import execute\n",
    "import numpy as np\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "from subprocess import Popen, PIPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Goal is to create a GainCollection from all GAIN domains here\n",
    "# The set of proteins needs to be filtered and analyzed statistically before\n",
    "\n",
    "# Example PDB name:\n",
    "#  R0LVI9-R0LVI9_ANAPL-PutativeGR125-Anas_platyrhynchos_unrelaxed_rank_1_model_3.pdb\n",
    "# corresponding STRIDE name:\n",
    "#  R0LVI9-R0LVI9_ANAPL-PutativeGR125-Anas_platyrhynchos.stride\n",
    "#pdbs = glob.glob(\"/home/hildilab/projects/agpcr_nom/*/batch*/*_rank_1_*.pdb\")\n",
    "pdbs = glob.glob(\"/home/hildilab/projects/agpcr_nom/*output/**/*_rank_1_*.pdb\")\n",
    "print(f\"Found {len(pdbs)} best ranked models in target directories.\")\n",
    "#print(len(celsr_pdbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride_folder = f\"/home/hildilab/projects/agpcr_nom/all_gps_stride\"\n",
    "stride_bin = \"/home/hildilab/lib/stride/stride\"\n",
    "           \n",
    "def compile_stride_mp_list(pdbs, stride_folder,stride_bin):\n",
    "    stride_mp_list = []\n",
    "    \n",
    "    for pdb in pdbs:\n",
    "        pdb_name = pdb.split(\"/\")[-1]\n",
    "        name = pdb_name.split(\"_unrelaxed_\")[0]\n",
    "        out_file = f\"{stride_folder}/{name}.stride\"\n",
    "        arg = [pdb, out_file, stride_bin]\n",
    "        \n",
    "        stride_mp_list.append(arg)\n",
    "        \n",
    "    return stride_mp_list\n",
    "\n",
    "def run_stride(arg):\n",
    "    pdb_file, out_file, stride_bin = arg\n",
    "    stride_command = f\"{stride_bin} {pdb_file} -f{out_file}\"\n",
    "    execute.run_command(stride_command)\n",
    "\n",
    "def execute_stride_mp(stride_mp_list, n_threads=10):\n",
    "        stride_pool = mp.Pool(n_threads)\n",
    "        stride_pool.map(run_stride, stride_mp_list)\n",
    "        print(\"Completed mutithreaded creation of STRIDE files!\")\n",
    "        \n",
    "        #execute.run_stride(pdb, out_file, stride_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stride_mp_list = compile_stride_mp_list(celsr_pdbs, stride_folder, stride_bin)\n",
    "#stride_mp_list = compile_stride_mp_list(singlet_pdbs, stride_folder, stride_bin)\n",
    "#print(len(stride_mp_list))\n",
    "#[print(arg) for arg in stride_mp_list[:10]]\n",
    "# MP execution of STRIDE\n",
    "#execute_stride_mp(stride_mp_list, n_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate double entries (both in the original run and the added small runs)\n",
    "# Form the \"pdbs\" list\n",
    "\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/all_gps_stride/*\")\n",
    "print(len(stride_files))\n",
    "accessions = [f.split(\".strid\")[0].split(\"/\")[-1].split(\"-\")[0] for f in stride_files]\n",
    "pdb_accessions = np.array([p.split(\"_unrelaxed_\")[0].split(\"/\")[-1].split(\"-\")[0] for p in pdbs])\n",
    "\n",
    "# Find duplicate in the original pdbs list and indicate them via > is_duplicate = True <\n",
    "is_duplicate=np.zeros([len(pdbs)], dtype=bool)\n",
    "sort_pdb_ac = np.sort(pdb_accessions)\n",
    "duplicate_list = []\n",
    "for i, pdb in enumerate(sort_pdb_ac):\n",
    "    if i+1 == len(sort_pdb_ac):\n",
    "        break\n",
    "    if pdb == sort_pdb_ac[i+1]:\n",
    "        duplicate_list.append(pdb)\n",
    "        multi_indices = np.where(pdb == pdb_accessions)[0]\n",
    "        is_duplicate[multi_indices[0]] = True\n",
    "\n",
    "np_pdbs = np.array(pdbs)\n",
    "singlet_pdbs = np_pdbs[is_duplicate == False] # This is the reduced list with ONLY UNIQUE PDBs\n",
    "print(f\"Reduced the initial set of {len(pdbs)} PDB files down to {len(singlet_pdbs)} files.\")\n",
    "\n",
    "# This is a check routine if there are PDBs in the reduced list which have NOT a STRIDE file\n",
    "singlet_pdb_accessions = np.array([p.split(\"_unrelaxed_\")[0].split(\"/\")[-1].split(\"-\")[0] for p in singlet_pdbs])\n",
    "\n",
    "counter = 0\n",
    "for ac in singlet_pdb_accessions:\n",
    "    if ac not in accessions:\n",
    "        print(ac)\n",
    "    else:\n",
    "        counter += 1\n",
    "print(f\"Found {counter}/{len(singlet_pdb_accessions)} accessions in the accession list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"stride_folder = f\"{pdb_folder}/stride_files\"\n",
    "fasta_file = \"/home/hildilab/projects/agpcr_nom/batch_files/batch_60.fa\"\n",
    "# CHECK FILES!\n",
    "import os\n",
    "if not os.path.isfile(fasta_file) or os.path.islink(fasta_file):\n",
    "    print(\"ERROR: Specify FASTA FILE CONTAINING ALL SEQUENCES OF THE PROTEINS!\")\n",
    "    \n",
    "    \n",
    "alignment_file = \"/home/hildilab/projects/agpcr_nom/batch_out_test/mafft.fa\"\n",
    "#alignment_file = None\n",
    "\n",
    "# If there is not a specified base alignment, create one (that might take a while tho.)\n",
    "if not alignment_file:\n",
    "    mafft_bin = \"mafft\"\n",
    "    mafft_command = f\"{mafft_bin} --auto --thread 4 {fasta_file}\"\n",
    "    out_dir = \"/\".join(fasta_file.split(\"/\")[:-2])\n",
    "    out_file = f\"{out_dir}/mafft.fa\"\n",
    "    execute.run_command(mafft_command, out_file = out_file)\n",
    "    alignment_file = out_file\n",
    "gps_minus_one_column = 1209\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GainCollection class needs to be modified to also parse a list of sequences \n",
    "# instead of a folder containing one fasta per seq\n",
    "class FilterCollection:\n",
    "    ''' \n",
    "    A collection of GainDomain objects used for filtering a set of AF2 models\n",
    "    This is used to condense the dataset towards one containing only GAIN domains\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    collection : list\n",
    "        List of GainDomain instances\n",
    "   \n",
    "    valid_gps : np.array(bool)\n",
    "        For each protein, specify if the GPS detection is valid or not\n",
    "    \n",
    "    valid_subdomain : np.array(bool)\n",
    "        For each protein, specify if it has detected subdomains or not\n",
    "    \n",
    "    Methods\n",
    "    ----------\n",
    "    print_gps(self):\n",
    "        Prints info about all GainDomain GPS subinstances\n",
    "\n",
    "    write_all_seq(self, savename):\n",
    "        Writes all sequences in the Collection to one file\n",
    "\n",
    "    transform_alignment(self, input_alignment, output_alignment, aln_cutoff):\n",
    "        Transforms a given alignment with SSE data to a SSE-assigned version of the alignment\n",
    "        \n",
    "    write_filtered(self, savename, bool_mask):\n",
    "        Writes all sequences to File where a boolean mask (i.e. subdomain_criterion, gps_criterion)\n",
    "        is True at respective position\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                alignment_file, \n",
    "                aln_cutoff,\n",
    "                quality,\n",
    "                gps_index,\n",
    "                stride_files,\n",
    "                sequence_files=None, # modified to object containing all seqs\n",
    "                sequences=None, # replaces sequence_files\n",
    "                subdomain_bracket_size=20,\n",
    "                domain_threshold=20,\n",
    "                coil_weight=0.00,\n",
    "                alignment_dict=None): \n",
    "        '''\n",
    "        Constructs the GainCollection objects by initializing one GainDomain instance per given sequence\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alignment_file:     str, required\n",
    "            The base dataset alignment file\n",
    "\n",
    "        aln_cutoff:         int, required\n",
    "            Integer value of last alignment column\n",
    "\n",
    "        quality:            list, required\n",
    "            List of quality valus for each alignment column. Has to have $aln_cutoff items\n",
    "            By default, would take in the annotated Blosum62 values from the alignment exported from Jalview\n",
    "\n",
    "        gps_index:          int, required\n",
    "            Alignment column index of the GPS-1 residue (consensus: Leu)\n",
    "        \n",
    "        stride_files:       list, required\n",
    "            A list of stride files corresponding to the sequences contained within.\n",
    "        \n",
    "        sequence_files:     list, optional\n",
    "            A list of sequence files to be read as the collection - specify either this\n",
    "            or sequences as an object instead of files for sequences\n",
    "        \n",
    "        sequences:          object, optional\n",
    "            A list of (sequence_name, sequence) tuples containing all sequences. Can be specified\n",
    "            instead of sequence_files\n",
    "\n",
    "        subdomain_bracket_size: int, optional\n",
    "            Smoothing window size for the signal convolve function. Default = 20.\n",
    "\n",
    "        domain_threshold:   int, optional\n",
    "            Minimum size of a helical segment to be considered candidate for Subdomain A of GAIN domain. Default = 20.\n",
    "\n",
    "        coil_weight:        float, optional\n",
    "            Weight assigned to unordered residues during Subdomain detection. Enables decay of helical signal\n",
    "            default = 0. Recommended values < +0.2 for decay\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        None\n",
    "        '''\n",
    "        # Initialize collection (containing all GainDomain instances) and the anchor-histogram\n",
    "        if sequence_files:\n",
    "            # Compile all sequence files to a sequences object\n",
    "            sequences = np.empty([len(sequence_files)])\n",
    "            for i, seq_file in enumerate(sequence_files):\n",
    "                name, seq = sse_func.read_seq(seq_file, return_name=True)\n",
    "                sequences[i] = (name, seq)\n",
    "        elif (sequences is not None):\n",
    "            print(f\"Found sequences object.\")\n",
    "        else: \n",
    "            print(f\"ERROR: no sequence_files or sequences parameter found. Aborting compilation.\")\n",
    "            return None\n",
    "        self.collection = np.empty([len(sequences)], dtype=object)\n",
    "        \n",
    "        valid_gps = np.zeros([len(sequences)], dtype=bool)\n",
    "        valid_subdomain = np.zeros([len(sequences)], dtype=bool)\n",
    "        #anchor_hist = np.zeros([aln_cutoff])#\n",
    "        # Create a GainDomain instance for each sequence file contained in the list\n",
    "        for i,seq_tup in enumerate(sequences):\n",
    "            # updated GainDomain class\n",
    "            name, sequence = seq_tup\n",
    "            explicit_stride = [stride for stride in stride_files if name.split(\"-\")[0] in stride]\n",
    "            if len(explicit_stride) == 0:\n",
    "                print(f\"Stride file not found for {name}\")\n",
    "                continue\n",
    "            newGain = GainDomain(alignment_file, \n",
    "                                  aln_cutoff,\n",
    "                                  quality,\n",
    "                                  name = name,\n",
    "                                  sequence = sequence,\n",
    "                                  gps_index = gps_index, \n",
    "                                  subdomain_bracket_size = subdomain_bracket_size,\n",
    "                                  domain_threshold = domain_threshold,\n",
    "                                  coil_weight = coil_weight,\n",
    "                                  explicit_stride_file = explicit_stride[0],\n",
    "                                  without_anchors = True,\n",
    "                                  skip_naming = True,\n",
    "                                  alignment_dict = alignment_dict)\n",
    "\n",
    "            # Check if the object staisfies minimum criteria\n",
    "            if newGain.isValid: \n",
    "                  \n",
    "                self.collection[i] = newGain\n",
    "                  \n",
    "                if newGain.hasSubdomain:\n",
    "                    valid_subdomain[i] = True\n",
    "                if newGain.GPS.isConsensus:\n",
    "                    valid_gps[i] = True\n",
    "        \n",
    "        self.valid_subdomain = valid_subdomain\n",
    "        self.valid_gps = valid_gps\n",
    "        print(f\"Completed Collection initialitazion of {len(sequences)} sequences.\\n\"\n",
    "             f\"{np.count_nonzero(self.collection)} valid proteins were found.\\n\"\n",
    "             f\"{np.count_nonzero(self.valid_subdomain)} of which have detected Subdomains.\\n\"\n",
    "             f\"{np.count_nonzero(self.valid_gps)} of which have detected consensus GPS motifs.\\n\")\n",
    "\n",
    "    def print_gps(self):\n",
    "        '''\n",
    "        Prints information about the GPS of each GAIN domain.\n",
    "\n",
    "        Parameters:\n",
    "            None\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        for i, gain in enumerate(self.collection):\n",
    "            try:\n",
    "                gain.GPS.info()\n",
    "            except:\n",
    "                print(f\"No GPS data for {gain.name}. Likely not a GAIN Domain!\")\n",
    "\n",
    "    def write_all_seq(self, savename):\n",
    "        '''\n",
    "        Write all GAIN sequences of the GainCollection into one fasta file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        savename: str, required\n",
    "            Output name of the fasta file.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        None\n",
    "        '''\n",
    "        with open(savename, 'w') as f:\n",
    "            for gain in self.collection:\n",
    "                f.write(f\">{gain.name[:-3]}\\n{''.join(gain.sequence)}\\n\")\n",
    "    \n",
    "    def write_filtered(self, savename, bool_mask=None, write_mode='w'):\n",
    "                  \n",
    "        '''\n",
    "        Internal function for writing filtered sequences to file.\n",
    "        Takes the Gain.sequence np.array type to write the truncated versions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        savename: str, required\n",
    "            Output name of the fasta file.\n",
    "        bool_mask: list/array, required\n",
    "            A mask of len(self.collection) where a boolean defines whether to write the\n",
    "            sequence to file or not\n",
    "        Returns\n",
    "        ----------\n",
    "        None\n",
    "        '''\n",
    "        with open(savename, write_mode) as f:\n",
    "            print(f\"writing filtered to {savename}\")\n",
    "            for i, gain in enumerate(self.collection):\n",
    "                if gain is not None and bool_mask[i] == True:\n",
    "                    f.write(f\">{gain.name.replace('.fa','')}\\n{''.join(gain.sequence)}\\n\")\n",
    "\n",
    "    def transform_alignment(self, input_alignment, output_alignment, aln_cutoff):\n",
    "        ''' \n",
    "        Transform any input alignment containing all sequences in the GainCollection \n",
    "        into one where each residue is replaced with the respective \n",
    "        Secondary Structure from the STRIDE files\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_alignment: str, required\n",
    "            Input alignment file\n",
    "        output_alignment: str, required\n",
    "            Output alignment file\n",
    "        aln_cutoff: int, required\n",
    "            Last alignment column to be read from the Input Alignment\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        None\n",
    "        '''\n",
    "        initial_dict = sse_func.read_alignment(input_alignment, aln_cutoff)\n",
    "        out_dict = {}\n",
    "        for gain in self.collection:\n",
    "            sse_alignment_row = np.full([aln_cutoff], fill_value='-', dtype='<U1')\n",
    "            mapper = sse_func.get_indices(gain.name, gain.sequence, input_alignment, aln_cutoff)\n",
    "            for index, resid in enumerate(gain.sse_sequence):\n",
    "                sse_alignment_row[mapper[index]] = resid\n",
    "            out_dict[gain.name[:-3]] = sse_alignment_row\n",
    "\n",
    "        # Write to file\n",
    "        with open(output_alignment, \"w\") as f:\n",
    "            for key in out_dict.keys():\n",
    "                f.write(f\">{key}\\n{''.join(out_dict[key])}\\n\")\n",
    "        print(f\"Done transforming alignment {input_alignment} to {output_alignment} with SSE data.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alignment_file = \"/home/hildilab/projects/agpcr_nom/batch_out_test/big_mafft.fa\"\n",
    "#quality_file = \"/home/hildilab/projects/agpcr_nom/batch_out_test/big_mafft.jal\"\n",
    "alignment_file = \"/home/hildilab/projects/GPS_massif/uniprot_query/trunc_celsr.mafft.fa\"\n",
    "quality_file = \"/home/hildilab/projects/GPS_massif/uniprot_query/trunc_celsr.mafft.jal\"\n",
    "fasta_file = \"/home/hildilab/projects/GPS_massif/uniprot_query/all_celsr_trunc.fa\"\n",
    "#stride_folder = f\"{pdb_folder}/stride_files\"\n",
    "quality = sse_func.read_quality(quality_file)\n",
    "gps_minus_one = 4966 #19258\n",
    "aln_cutoff = 4990 #19822\n",
    "sequences = sse_func.read_multi_seq(fasta_file)\n",
    "print(len(sequences))\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/all_gps_stride/*\")\n",
    "print(len(stride_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"filterCollection = FilterCollection(alignment_file,\n",
    "                                   aln_cutoff = 19822,\n",
    "                                   quality = quality,\n",
    "                                   gps_index = gps_minus_one,\n",
    "                                   stride_files = stride_files,\n",
    "                                   sequences = sequences)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filterCollection.write_filtered(savename=\"test.fa\", bool_mask = filterCollection.valid_gps, write_mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_filter_seqs(arg_item):\n",
    "                   # [sequences,      # A number of sequences as tuple instances\n",
    "                   #  stride_folder,  # A folder containing ALL stride files\n",
    "                   #   output_prefix,  # A prefix for individual file identification\n",
    "                   #   alignment_file, # The big (initial) alignment file\n",
    "                   #   quality,        # The corr. parsed quality for BLOSUM62 score\n",
    "                   #   aln_cutoff,     # the left-most column (19822 for big_mafft.fa)\n",
    "                   #   gps_minus_one,  # The column index of GPS-1 (zero-indexed! 19258 big_mafft)\n",
    "                   #   ]\n",
    "    sequences, stride_folder, output_prefix, alignment_file, quality, aln_cutoff, gps_minus_one, alignment_dict = arg_item\n",
    "    # Parallelizable version of filtering sequences and models via FilterCollection\n",
    "    # This should create separate files for each valid, fragment and no-SD group\n",
    "    # These files should then be grouped together\n",
    "    # The batch size is arbitrary and is considered the number of sequences passed\n",
    "    \n",
    "    # Output: Profiles; 4 Text files (valid, fragment, invalidGPS, invalid)\n",
    "    filteredBatch = FilterCollection(alignment_file,\n",
    "                                   aln_cutoff = aln_cutoff,\n",
    "                                   quality = quality,\n",
    "                                   gps_index = gps_minus_one,\n",
    "                                   stride_files = stride_files,\n",
    "                                   sequences = sequences,\n",
    "                                   alignment_dict = alignment_dict)\n",
    "    outpath = \"/home/hildilab/projects/agpcr_nom/all_gps_profiles_001\"\n",
    "    hel_path = \"/home/hildilab/projects/agpcr_nom/all_gps_hels_001\"\n",
    "    \n",
    "    for Gain in filteredBatch.collection:\n",
    "        if Gain:\n",
    "            Gain.plot_profile(outdir=outpath, noshow=True)\n",
    "            if Gain.hasSubdomain:\n",
    "                Gain.plot_helicality(coil_weight=0.01, savename=f'{outpath}/{Gain.name}.hel.png', debug=False, noshow=True)\n",
    "        \n",
    "    suffixes = [\"gain\", \n",
    "                \"fragments\", \n",
    "                \"noncons_gps\", \n",
    "                \"invalid\"]\n",
    "    masks = [np.logical_and(filteredBatch.valid_gps, filteredBatch.valid_subdomain),\n",
    "              np.logical_not(filteredBatch.valid_subdomain),\n",
    "              np.logical_not(filteredBatch.valid_gps),\n",
    "              np.logical_not(np.logical_and(filteredBatch.valid_gps, filteredBatch.valid_subdomain))]\n",
    "    # write four separate files, matching each criterion\n",
    "    for k in range(4):\n",
    "        filteredBatch.write_filtered(savename=f\"{outpath}/{output_prefix}_{suffixes[k]}.fa\", \n",
    "                                     bool_mask = masks[k],\n",
    "                                     write_mode = 'w')\n",
    "    del filteredBatch\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mp_collection(arg_list, n_threads=10):\n",
    "    pool = mp.Pool(n_threads)\n",
    "    pool.map(batch_filter_seqs, arg_list)\n",
    "    print(\"Completed mutithreaded filtering.\")\n",
    "\n",
    "def construct_arg_list(batch_sequence_files, \n",
    "                       output_folder,\n",
    "                       stride_folder, \n",
    "                       quality, \n",
    "                       alignment_file, \n",
    "                       aln_cutoff, \n",
    "                       gps_minus_one,\n",
    "                       alignment_dict = None):\n",
    "    \"\"\" each item looks like this:\n",
    "        sequences, \\ \n",
    "        stride_folder, \\\n",
    "        output_prefix, \\\n",
    "        alignment_file, \\\n",
    "        quality, \\\n",
    "        aln_cutoff, \\\n",
    "        gps_minus_one = arg_item\"\"\"\n",
    "    # static : stride_folder, quality, alignment_file, aln_cutoff, gps_minus_one\n",
    "    # flexible : sequences, output_prefix\n",
    "    arg_list = []\n",
    "    #\n",
    "    for idx, sequence_file in enumerate(batch_sequence_files):\n",
    "        \n",
    "        index_string = str(idx)\n",
    "        sequences = sse_func.read_multi_seq(sequence_file)\n",
    "        output_prefix = f\"{output_folder}_{index_string.zfill(3)}\"\n",
    "        item = [sequences, \n",
    "                stride_folder, \n",
    "                output_prefix, \n",
    "                alignment_file, \n",
    "                quality, \n",
    "                aln_cutoff, \n",
    "                gps_minus_one,\n",
    "                alignment_dict]\n",
    "        \n",
    "        arg_list.append(item)\n",
    "    \n",
    "    print(f\"[NOTE] : Compiled list of arguments for multithreaded filtering\"\n",
    "          f\" containing {len(arg_list)} items.\")\n",
    "    return arg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "batch_sequence_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/*output/batch_*.fa\")\n",
    "print(len(batch_sequence_files))\n",
    "output_folder = \"app_gain_domains_001\"\n",
    "\n",
    "alignment_file = \"/home/hildilab/projects/agpcr_nom/appended_big_mafft.fa\" # This is a combined alignment of ALL sequences in ALL queries!\n",
    "quality_file = \"/home/hildilab/projects/agpcr_nom/appended_big_mafft.jal\"  # ^ corresponding quality file\n",
    "stride_folder = \"/home/hildilab/projects/agpcr_nom/all_gps_stride\" \n",
    "quality = sse_func.read_quality(quality_file)\n",
    "gps_minus_one = 21160  # 19258\n",
    "aln_cutoff = 21813 # 19822\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/all_gps_stride/*\")\n",
    "alignment_dict = sse_func.read_alignment(alignment_file, aln_cutoff)\n",
    "print(len(stride_files))\n",
    "print(len(batch_sequence_files))\n",
    "#print(quality)\n",
    "#sequences = sse_func.read_multi_seq(fasta_file)\n",
    "\n",
    "arg_list = construct_arg_list(batch_sequence_files, \n",
    "                       output_folder,\n",
    "                       stride_folder, \n",
    "                       quality, \n",
    "                       alignment_file, \n",
    "                       aln_cutoff, \n",
    "                       gps_minus_one,\n",
    "                       alignment_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k in range(0, 10):#len(arg_list)+1):\n",
    "#    batch_filter_seqs(arg_list[k])\n",
    "run_mp_collection(arg_list[400:], n_threads=16)\n",
    "#for arg in arg_list[-5:]: print(arg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compile_fastas(prefix, out_prefix):\n",
    "    # Compiles the fasta files together to construct one large file containing the sequences\n",
    "    # satisfying each criterion in the 2x2 matrix\n",
    "    # we want to have the GAIN sequence only that is output by the write_filtered() func.\n",
    "    \n",
    "    # Gather all files:\n",
    "    suffixes = [\"gain\", \n",
    "                \"fragments\", \n",
    "                \"noncons\", \n",
    "                \"invalid\"]\n",
    "    all_files = np.asarray(glob.glob(f\"{prefix}*fa\"))\n",
    "    print(len(all_files))\n",
    "    for suffix in suffixes:\n",
    "        sub_list = sorted([f for f in all_files if suffix in f.split(\"_\")[-1]])\n",
    "        print(f\"Sublist constructed for {suffix = } containing {len(sub_list)} files.\")\n",
    "        with open(f\"{out_prefix}_{suffix}.fa\", \"w\") as all_file:\n",
    "            all_seqs = []\n",
    "            for file in sub_list:\n",
    "                seqs = sse_func.read_multi_seq(file)\n",
    "                for j in seqs:\n",
    "                    if j in all_seqs:\n",
    "                        print(j[0], \"doublet\")\n",
    "                        continue\n",
    "                    all_seqs.append(j)\n",
    "                    all_file.write(f\">{j[0]}\\n{j[1]}\\n\")\n",
    "\n",
    "compile_fastas(\"/home/hildilab/projects/agpcr_nom/all_gps_profiles_001/app_gain_domains\",\n",
    "              out_prefix = \"/home/hildilab/projects/agpcr_nom/app_gain_001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full profile for D1 : Q6QNK2.\n",
    "# Compare the 0.01 sequences to the 0.00 sequences:\n",
    "f1 = \"../app_gain_001_gain.fa\"\n",
    "f0 = \"../app_gain_gain.fa\"\n",
    "dict1 = sse_func.read_alignment(f1, -1)\n",
    "dict0 = sse_func.read_alignment(f0, -1)\n",
    "for k in dict0.keys():\n",
    "    if k not in dict1.keys():\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad450da4e6f5b8cdd3942a14692e4e36ff09ff1e9c8df5cfaea1622af0db4001"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
