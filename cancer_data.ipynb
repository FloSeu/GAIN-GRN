{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancer Mutations, based on the 3D strucutal alignment data via pairwise GESAMT\n",
    "\n",
    "Cancer Integration with Data from NCGA < https://portal.gdc.cancer.gov/\n",
    "\n",
    "There is an API, however since we are only dealing with 31 GAIN domains / 32 receptors, I grabbed all data in JSON. https://docs.gdc.cancer.gov/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json, glob, re, math\n",
    "import template_finder as tf\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, FixedLocator)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sse_func\n",
    "from gain_classes import *\n",
    "jsons = glob.glob(\"../repo_data/gain_json/*.json\") # specific path for rk4\n",
    "#jsons = glob.glob(\"../gain_json/*.json\") # specific path for hildilap1a\n",
    "csvs = glob.glob(\"/home/hildilab/projects/agpcr_nom/snp_mane/*csv\") # specific path for rk4\n",
    "#csvs = glob.glob(\"../snp_mane/*csv\") # specific path for hildilap1a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded JSON will contain a list of dictionaries, where every dictionary corresponds to a single mutation of the respective receptor. With the pre-defined window, we have a list of mutations, where the keys correspond to properties of respective mutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION BLOCK FOR VARIANT FILE PARSING AND LOOP CONSTRUCTION.\n",
    "def find_offsets(fasta_file, accessions, sequences):\n",
    "    # searches through the accessions in the big sequence file,\n",
    "    # finds the start for the provided sequence\n",
    "    with open(fasta_file,\"r\") as fa:\n",
    "        fa_data = fa.read()\n",
    "        fasta_entries = fa_data.split(\">\")\n",
    "    seqs = []\n",
    "    headers = []\n",
    "    offsets = []\n",
    "    for seq in fasta_entries:\n",
    "        # Fallback for too short sequences\n",
    "        if len(seq) < 10: \n",
    "            continue\n",
    "        data = seq.strip().split(\"\\n\")\n",
    "        headers.append(data[0].split(\"|\")[1]) # This is only the UniProtKB Accession Number and will be matched EXACTLY\n",
    "        seqs.append(\"\".join(data[1:]))\n",
    "    \n",
    "    heads = np.array(headers)\n",
    "    for idx, accession in enumerate(accessions):\n",
    "        seq_idx = np.where(heads == accession)[0][0]\n",
    "        offset = sse_func.find_the_start(seqs[seq_idx], sequences[idx])\n",
    "        #print(offset)\n",
    "        offsets.append(offset)\n",
    "    \n",
    "    return offsets\n",
    "\n",
    "def retrieve_json_vars(name,jsons):\n",
    "    # A function retrieving variants from a JSON File as exported by TCGA/NCGA, returning them as a dictionary\n",
    "    # The list of {jsons} is filtered to find {name}\n",
    "    # Returns a loaded dict from the json.\n",
    "    try: \n",
    "        identifier = name.split(\"AGR\")[1][:2] # When its a normal aGPCR\n",
    "    except: \n",
    "        identifier = \"C\"+name.split(\"CELR\")[1][0]\n",
    "\n",
    "    json_identifier = identifier.lower()\n",
    "    json_file = [j for j in jsons if json_identifier in j.lower()]\n",
    "    #print(f\"DEBUG : retrieve_mutation_json: {json_file = }\" )\n",
    "    if len(json_file) > 1 : print(\"WARNING: Multiple matching JSON Files detected:\", json_file)\n",
    "    with open(json_file[0]) as j_open:\n",
    "        j_data = j_open.read()\n",
    "    mutation_data = json.loads(j_data) # Will contain a list of dicts\n",
    "    return mutation_data\n",
    "\n",
    "def retrieve_csv_vars(name, csv_files, filter_str=None, with_resid=True):\n",
    "    # A function retrieving variants from a CSV File as exported by gnomAD, returning them as a dictionary\n",
    "    # The list of {csv_files} is filtered to find {name}, filter_str is used to find specific mutations, i.e. missense\n",
    "    # With_resid also adds the residue and the standard AA at this position to the mutation entries.\n",
    "    # Returns a list of dict, where each dict corresponds to one mutation\n",
    "    try: \n",
    "        identifier = name.split(\"AGR\")[1][:2] # When its a normal aGPCR\n",
    "    except: \n",
    "        identifier = \"C\"+name.split(\"CELR\")[1][0]\n",
    "\n",
    "    csv_identifier = identifier.lower()\n",
    "    csv_file = [c for c in csv_files if csv_identifier in c.lower()]\n",
    "    print(f\"DEBUG : parse_snp_csv: {csv_file = }\" )\n",
    "    \n",
    "    # the target CSV files have 53 items per row. RETURNS columns: dict where key points to axis 1 index of np.array; val_arr: value array \n",
    "    with open(csv_file[0]) as cc:\n",
    "        data = cc.readlines()\n",
    "\n",
    "    # Pre-filter the data if specified.\n",
    "    if filter_str is not None:\n",
    "        newdata = [l for l in data[1:] if filter_str in l]\n",
    "        data = [data[0]]\n",
    "        for l in newdata: \n",
    "            data.append(l)\n",
    "\n",
    "    oneletter = {'Ala':'A','Arg':'R','Asn':'N','Asp':'D','Cys':'C','Gln':'Q','Glu':'E','Gly':'G','His':'H',\n",
    "                 'Ile':'I','Leu':'L','Lys':'K','Met':'M','Phe':'F','Pro':'P','Pyl':'O','Ser':'S','Thr':'T',\n",
    "                'Trp':'W','Tyr':'Y','Val':'V'}\n",
    "\n",
    "    variant_list = []\n",
    "    variant_keys = data[0].strip().split(\",\")\n",
    "\n",
    "    for i,row in enumerate(data[1:]):\n",
    "        row = re.sub(r'\\\".*\\\"', 'REMOVED', row)   # Removes combined irrelevant entries that contain a \",\" within string denotation. This confuses the split routine.\n",
    "        var_dict = {variant_keys[i]:val for i,val in enumerate(row.strip().split(\",\"))}\n",
    "        #print(\"DEBUG\", var_dict)\n",
    "        if with_resid:\n",
    "            var_string = var_dict['Protein Consequence']\n",
    "            #print(f\"DEBUG: {var_string = }\")\n",
    "            if len(var_string) < 2:\n",
    "                var_dict['resid'] = None\n",
    "                var_dict['resname'] = None\n",
    "            else:\n",
    "                var_dict['resname'] = oneletter[re.search(r'[A-z]{3}', var_string).group(0)]\n",
    "                var_dict['resid'] = int(re.search(r'[\\d]+', var_string).group(0))\n",
    "        variant_list.append(var_dict)\n",
    "\n",
    "    return variant_list\n",
    "\n",
    "def extract_variants(gain, files, resid_key):\n",
    "    # Wrapper function for getting variants / mutations in the correct format.\n",
    "    # Draws the list of vars/muts from the respective list of jsons/csvs abd gets the individual positions either at entry \n",
    "    # resid_key : 'x' (json from NCGA) / 'resid' (csv from gnomAD)\n",
    "\n",
    "    if files[0][-3:].lower() == 'csv':\n",
    "        print(\"List of CSV files detected.\")\n",
    "        vars = retrieve_csv_vars(gain.name, csvs, filter_str='missense_variant',with_resid=True)\n",
    "    else:\n",
    "        print(\"This should be a list of JSON Files.\")\n",
    "        v_dict = retrieve_json_vars(gain.name, files,)\n",
    "        vars = [mut for mut in v_dict['mutations']]\n",
    "        # returns a dictionary with a list of dicts, each dict corresponding to a var.\n",
    "    variant_dict = {}\n",
    "    positions = []\n",
    "    for var in vars:\n",
    "        # The \"x\" key is the identifying position!\n",
    "        resid = int(var[resid_key]) \n",
    "        #\n",
    "        # Here, one can define criteria for skipping said var. For now, use all.\n",
    "        #\n",
    "        # Add the receptor name to the var dictionary\n",
    "        var[\"receptor\"] = gain.name\n",
    "\n",
    "        if resid in variant_dict.keys():\n",
    "            variant_dict[resid].append(var)\n",
    "        else:\n",
    "            variant_dict[resid] = [var]\n",
    "        positions.append(resid)\n",
    "    return positions, variant_dict\n",
    "\n",
    "def get_loop_stats(indexing_dir, sequence):\n",
    "    # Returns a named dict with loop lengths, i.e. {\"H1-H2\":13, \"H8-S1\":12}\n",
    "    inverted_dir = {sse[0] : (sse[1],ki) for ki, sse in indexing_dir.items() if \"GPS\" not in ki} # The begin of each sse is here {0:(13, \"H2\")}\n",
    "    loop_loc = {}\n",
    "    loop_dir = {}\n",
    "    ordered_starts = sorted(inverted_dir.keys())\n",
    "    for i, sse_start in enumerate(ordered_starts):\n",
    "        if i == 0: \n",
    "            continue # Skip the first and go from the second SSE onwards, looking in N-terminal direction.\n",
    "        c_label = inverted_dir[sse_start][1]\n",
    "        n_end, n_label = inverted_dir[ordered_starts[i-1]]\n",
    "        loop_loc[f\"{n_label}-{c_label}\"] = (n_end, sse_start-1)\n",
    "        loop_dir[f\"{n_label}-{c_label}\"] = sequence[n_end+1:sse_start] # The one-letter-coded seqeuence. Will be a list of lists\n",
    "    return loop_loc, loop_dir\n",
    "\n",
    "def compose_vars(aGainCollection, files, resid_key, aa_key, fasta_offsets):\n",
    "    # Take the Gain Collection and retrieve variant/mutation info from the files list via extract_variants()\n",
    "    # Collect these for each GAIN in the aGainCollection.collection and compose them together to enable addressing each individual labeled position.\n",
    "    # Returns the generalized (aligned) variants/mutations and their respective counts\n",
    "    mismatch_flag = False\n",
    "    generalized_vars = {}\n",
    "    valid = 0\n",
    "    invalid = 0\n",
    "\n",
    "    for gain_ndx, gain in enumerate(aGainCollection.collection):\n",
    "        gain_valid = 0\n",
    "        gain_invalid = 0\n",
    "        fasta_offset = fasta_offsets[gain_ndx]\n",
    "        # Retrieve vars/mutations for respective receptor\n",
    "        positions, gain_pos_dict = extract_variants(gain, files, resid_key)\n",
    "        # Find all vars/mutations whose RESID (fasta) matches the corrected GAIN domain INTERVAL(for fasta resids)\n",
    "        within = [pos for pos in positions if pos in range(fasta_offset, fasta_offset + 1 + gain.end - gain.start)]\n",
    "        #print(\"DEBUG: vars/mutations within GAIN space\\n\", sorted(within))\n",
    "        # Evaluate the vars/mutations. If the residue in question (corrected index) is named (i.e. \"H6.50\"), get its resepective label.\n",
    "        for p_resid in gain_pos_dict.keys():\n",
    "            # Since fasta_offset maps to the first gain residue (i.e. 2027 with gain.start being 459), an offset needs to be set by the difference\n",
    "            corrected_resid = p_resid - fasta_offset + gain.start -1\n",
    "            #print(f\"DEBUG {corrected_resid = }, {p_resid = }\n",
    "            #   TRUE GAIN INTERVAL {fasta_offset+gain.start} - {fasta_offset+gain.end}\\nCORRECTED GAIN INTERVAL {gain.start}-{gain.end}\")\n",
    "            # map it to its corresponding label. If there is none, continue\n",
    "            if corrected_resid < gain.start or corrected_resid >= gain.end:\n",
    "                gain_invalid += 1\n",
    "                continue\n",
    "            try:\n",
    "                p_label = gain.rev_idx_dir[corrected_resid]\n",
    "                gain_valid +=1\n",
    "            except KeyError:\n",
    "                gain_invalid += 1\n",
    "                continue\n",
    "            # SANITY CHECK:\n",
    "            if gain_pos_dict[p_resid][0][aa_key][0] != gain.sequence[corrected_resid-gain.start]:\n",
    "                print(f\"{gain_pos_dict[p_resid][0][aa_key][0] = }\", {gain.sequence[corrected_resid-gain.start]})\n",
    "                print(f\"MISMATCH! {p_resid} -> {corrected_resid}\")\n",
    "                print(f\"{gain_pos_dict[p_resid][0][aa_key][0]} : {gain.sequence[corrected_resid-gain.start]} in GAIN\",\n",
    "                    f\"{gain.sequence[corrected_resid-gain.start-2:corrected_resid-gain.start+3]}\")\n",
    "                mismatch_flag = True\n",
    "            #else:\n",
    "                #print(f\"MATCH {p_resid} -> {corrected_resid} | {gain_mutation_dict[p_resid][0][aa_key][0]} : {gain.sequence[corrected_resid-gain.start]}\")\n",
    "            \n",
    "            # with the Label, map into generalized dictionary\n",
    "            if p_label not in generalized_vars.keys():\n",
    "                generalized_vars[p_label] = gain_pos_dict[p_resid]\n",
    "            else:\n",
    "                [generalized_vars[p_label].append(pos) for pos in gain_pos_dict[p_resid]]\n",
    "        valid += gain_valid\n",
    "        invalid += gain_invalid\n",
    "        print(\"VALID MAPPED POSITIONS: \", gain_valid, \"\\nINVALID POSITIONS     : \", gain_invalid, \"\\nTOTAL POSITIONS       : \", len(gain_pos_dict.keys()))\n",
    "    if mismatch_flag: \n",
    "        print(\"[WARNING]: MISMATCHES HAVE BEEN FOUND. PLEASE CHECK THE OUTPUT.\")\n",
    "    else: \n",
    "        print(\"[NOTE]: NO MISMATCHES HAVE BEEN FOUND.\")\n",
    "    print(f\"TOTAL VARS WITHIN GAIN:\", valid, \"\\nTOTAL VARS OUTSIDE GAIN:\", invalid, \"\\nTOTAL VARS:\", valid+invalid)\n",
    "    \n",
    "    generalized_counts = {k:len(v) for k,v in generalized_vars.items()}\n",
    "\n",
    "    return generalized_vars, generalized_counts\n",
    "\n",
    "def loop2fasta(outfile, itemlist):\n",
    "    # Write the collected loop sequences to a FASTA file for later alignment.\n",
    "    with open(outfile, 'w') as out:\n",
    "        for subdict in itemlist:\n",
    "            out.write(f\">{subdict['name']}\\n{subdict['sequence']}\\n\")\n",
    "    print(\"Done with\", outfile)\n",
    "\n",
    "def compose_loop_vars(aGainCollection, files, resid_key, aa_key, fasta_offsets):\n",
    "    # Take the Gain Collection and retrieve variant/mutation info from the files list via extract_variants()\n",
    "    # Collect these for each GAIN in the aGainCollection.collection and compose them together to enable addressing each individual labeled position.\n",
    "    # Returns the generalized (aligned) variants/mutations and their respective counts\n",
    "    mismatch_flag = False\n",
    "    generalized_vars = {}\n",
    "    valid = 0\n",
    "    invalid = 0\n",
    "\n",
    "    loop_valid = 0\n",
    "    loop_invalid = 0\n",
    "    for idx, gain in enumerate(aGainCollection.collection):\n",
    "    # Retrieve vars/mutations for respective receptor\n",
    "        positions, gain_pos_dict = extract_variants(gain, files, resid_key)\n",
    "        fasta_offset = fasta_offsets[idx]\n",
    "        # Find all vars/mutations whose RESID (fasta) matches the corrected GAIN domain INTERVAL(for fasta resids)\n",
    "        within = [pos for pos in positions if pos in range(fasta_offset, fasta_offset + 1 + gain.end - gain.start)]\n",
    "        #print(\"DEBUG: vars/mutations within GAIN space\\n\", sorted(within))\n",
    "        #idx_dir, _, _, _ = sse_func.create_indexing(gain, anchors, anchor_occupation, anchor_dict, outdir=None, offset=fasta_offsets[idx], silent=True, split_mode='double')\n",
    "        idx_dir, _, _, _, _, = tf.assign_indexing(gain_obj=gain,\n",
    "                                            file_prefix=f'../indexing_tmp/x{idx}',\n",
    "                                            gain_pdb=sse_func.find_pdb(gain.name, '../all_pdbs/'),\n",
    "                                            template_dir='../r2_template_pdbs',\n",
    "                                            gesamt_bin=\"/home/hildilab/lib/xtal/ccp4-8.0/ccp4-8.0/bin/gesamt\",\n",
    "                                            hard_cut={\"S2\":7,\"S6\":3,\"H5\":3},\n",
    "                                            debug=False,\n",
    "                                            patch_gps=True)\n",
    "        # i.e. 'H2-H3': (13, 20), 'H3-H4': (36, 42), 'H4-H5': (61, 75), 'H5-H6': (87, 89), ...., 'S11-S12': (316, 319), 'S12-S13': (327, 329)}\n",
    "        i_loc, _ = get_loop_stats(idx_dir, gain.sequence)\n",
    "        # Modify this to a position --> loop dict\n",
    "        loop_locations = {}\n",
    "        for loop, interval in i_loc.items():\n",
    "            for i in range(interval[0], interval[1]+1):\n",
    "                loop_locations[i+gain.start] = loop\n",
    "        # Evaluate the vars/mutations. If the residue in question (corrected index) is named (i.e. \"H6.50\"), get its resepective label.\n",
    "        print(\"DEBUG\")\n",
    "        #[print(i,k) for i,k in loop_locations.items()]\n",
    "\n",
    "        for p_resid in gain_pos_dict.keys():\n",
    "            # Since fasta_offset maps to the first gain residue (i.e. 2027 with gain.start being 459), an offset needs to be set by the difference\n",
    "            corrected_resid = p_resid - fasta_offset + gain.start -1\n",
    "            # Check in which loop this is located and collect it in a dictionary.\n",
    "            try:\n",
    "                loop_label = loop_locations[corrected_resid]\n",
    "                loop_valid +=1\n",
    "            except KeyError:\n",
    "                loop_invalid += 1\n",
    "                continue\n",
    "            # SANITY CHECK:\n",
    "            if gain_pos_dict[p_resid][0][aa_key][0] != gain.sequence[corrected_resid-gain.start]:\n",
    "                print(f\"{gain_pos_dict[p_resid][0][aa_key][0] = }\", {gain.sequence[corrected_resid-gain.start]})\n",
    "                print(f\"MISMATCH! {p_resid} -> {corrected_resid}\")\n",
    "                print(f\"{gain_pos_dict[p_resid][0][aa_key][0]} : {gain.sequence[corrected_resid-gain.start]} in GAIN\",\n",
    "                    f\"{gain.sequence[corrected_resid-gain.start-2:corrected_resid-gain.start+3]}\")\n",
    "                mismatch_flag = True\n",
    "            # with the Label, map into generalized dictionary\n",
    "            if loop_label not in generalized_vars.keys():\n",
    "                generalized_vars[loop_label] = gain_pos_dict[p_resid]\n",
    "            else:\n",
    "                [generalized_vars[loop_label].append(pos) for pos in gain_pos_dict[p_resid]]\n",
    "        valid += loop_valid\n",
    "        invalid += loop_invalid\n",
    "        print(\"VALID LOOP POSITIONS: \", loop_valid, \"\\nINVALID POSITIONS     : \", loop_invalid, \"\\nTOTAL POSITIONS       : \", len(gain_pos_dict.keys()))\n",
    "    if mismatch_flag: \n",
    "        print(\"[WARNING]: MISMATCHES HAVE BEEN FOUND. PLEASE CHECK THE OUTPUT.\")\n",
    "    else: \n",
    "        print(\"[NOTE]: NO MISMATCHES HAVE BEEN FOUND.\")\n",
    "    print(f\"TOTAL VARS WITHIN GAIN:\", valid, \"\\nTOTAL VARS OUTSIDE GAIN:\", invalid, \"\\nTOTAL VARS:\", valid+invalid)\n",
    "    \n",
    "    generalized_counts = {k:len(v) for k,v in generalized_vars.items()}\n",
    "\n",
    "    return generalized_vars, generalized_counts\n",
    "\n",
    "def json2text(j:str, mutation_data:dict):\n",
    "    print(j.split(\"/\")[-1].split(\".\")[0].upper(), \"CANCER GENOME ATLAS MUTATIONS\\n\",\"_\"*70)\n",
    "    positions = []\n",
    "    for mutation in mutation_data['mutations']:\n",
    "        for key in mutation.keys():\n",
    "            print(key.ljust(30), mutation[key])\n",
    "        print(\"_\"*70)\n",
    "        resid = int(mutation[\"x\"]) # X is the residue ID\n",
    "        positions.append(resid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to construct a new dictionary where the indexing itself locates to. This can be a new attribute of the class\n",
    "Let's construct the human collection first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the human GAIN collection\n",
    "human_collection = pd.read_pickle(\"../human_collection.p.pkl\") # Well thats short now aint it :>\n",
    "\n",
    "human_accessions = [gain.name.split(\"-\")[0].split(\"_\")[0] for gain in human_collection.collection]\n",
    "human_sequences = [\"\".join(gain.sequence) for gain in human_collection.collection]\n",
    "#seq_file = \"/home/hildilab/projects/GPS_massif/uniprot_query/agpcr_celsr.fasta\"\n",
    "seq_file = '/home/hildilab/projects/GPS_massif/uniprot_query/agpcr_celsr.fasta'\n",
    "human_fasta_offsets = find_offsets(seq_file,\n",
    "                                 human_accessions, \n",
    "                                 human_sequences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each GAIN domain within this collection, map the mutations by residue on the corresponding element. Then, merge all that to construct a global mutation map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# VALID DATA FROM THE MSA AND THE ANALYSIS PIPELINE\n",
    "sdb = 3425\n",
    "# ADJUSTED ANCHORS!\n",
    "anchors = [ 662, 1194, 1912, 2490, 2848, 3011, 3073, 3260, 3455, 3607, 3998, 4279, 4850, 5339,\n",
    " 5413, 5813, 6337, 6659, 6696, 6765, 6808] # removed S7 @  5341\n",
    "anchor_occupation = [ 4594.,  6539., 11392., 13658.,  8862., 5092.,  3228., 14189.,  9413., 12760.,\n",
    "  9420., 11201., 12283.,  3676.,   13992., 12575., 13999., 14051., 14353., 9760., 14215.] # removed S7 @ 4562.,\n",
    "\n",
    "anchor_dict = sse_func.make_anchor_dict(anchors, 3425)\n",
    "\"\"\"\n",
    "gain_adresses = {}\n",
    "named_dirs = []\n",
    "for gain_idx, gain in enumerate(human_collection.collection):\n",
    "    #_,_, named_dir, _ = sse_func.create_indexing(gain, anchors, anchor_occupation, anchor_dict)\n",
    "\n",
    "    #gain_obj:object, file_prefix: str, gain_pdb: str, template_dir: str, gesamt_bin:str, \n",
    "    #                template_json=\"tdata.json\", outlier_cutoff=10.0,\n",
    "    #                hard_cut=None, debug=False, create_pdb=False, patch_gps=False#\n",
    "    _,_, named_dir, _,_ =tf.assign_indexing(gain_obj=gain,\n",
    "                                            file_prefix=f'../indexing_tmp/{gain_idx}',\n",
    "                                            gain_pdb=sse_func.find_pdb(gain.name, '../all_pdbs/'),\n",
    "                                            template_dir='../r3_template_pdbs',\n",
    "                                            gesamt_bin=\"/home/hildilab/lib/xtal/ccp4-8.0/bin/gesamt\",\n",
    "                                            template_json='template_data.json',\n",
    "                                            hard_cut={\"S2\":7,\"S6\":3,\"H5\":3},\n",
    "                                            debug=True,\n",
    "                                            create_pdb=False,\n",
    "                                            patch_gps=True)\n",
    "    named_dirs.append(named_dir)\n",
    "    gain.idx_dir = named_dir\n",
    "    gain.rev_idx_dir = {v: k for k, v in named_dir.items()}\n",
    "    gain_adresses[gain_idx] = gain.name\n",
    "\n",
    "rev_named_dirs = []\n",
    "for named_dir in named_dirs:\n",
    "    rev_dir = {v: k for k, v in named_dir.items()}\n",
    "    rev_named_dirs.append(rev_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a Dictionary with info to all mutations at respective RESID. Also just make a list with the resids to summarize all entry occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalized_counts = {}     # For every nomenclature label, the number of MUTATION entries is denoted\n",
    "generalized_mutations = {}  #   ^ The info behind the entries is here.\n",
    "gen_var_counts = {}         # For every nomenclature label, the number of VARIANT entries is denoted\n",
    "gen_vars = {}               #   ^ The info behind the entries is here.\n",
    "invalid = 0\n",
    "valid = 0\n",
    "\n",
    "mismatch_flag = False\n",
    "\n",
    "total_within = 0\n",
    "for gain_ndx, gain in enumerate(human_collection.collection):\n",
    "    print(f\"[DEBUG]: {gain.name}\\n\\t{gain.start = }\\n\\t{len(gain.sequence) = }\\n\\t{gain.end = }\")\n",
    "    gain_valid = 0\n",
    "    gain_invalid = 0\n",
    "    fasta_offset = human_fasta_offsets[gain_ndx]\n",
    "    # Retrieve mutations for respective receptor\n",
    "    positions, gain_mutation_dict = extract_variants(gain, jsons, 'x')\n",
    "    # Retrieve variants for respective receptor\n",
    "    var_positions, var_dict = extract_variants(gain, csvs, 'resid')\n",
    "    # Find all mutations whose RESID (fasta) matches the corrected INTERVAL(for fasta resids)\n",
    "    within = [pos for pos in positions if pos in range(fasta_offset, fasta_offset + 1 + gain.end - gain.start)]\n",
    "    #print(\"DEBUG: Mutations within GAIN space\\n\", sorted(within))\n",
    "    # Go through the gain_mutation_dict and add into the two dicts\n",
    "    total_within += len(within)\n",
    "    \"\"\"print(\"EXTENT OF MUTATIONS IN TARGET RESIDUE INTERVAL:\\nINTERVAL\\t\\t:\",fasta_offset, fasta_offset + 1 + gain.end - gain.start,\n",
    "        \"\\n FIRST OCC\\t\\t:\",within[0], \n",
    "        \"\\n LAST OCC\\t\\t:\",within[-1],\n",
    "        \"\\n TOTAL MUT\\t\\t:\",len(positions),\n",
    "        \"\\n WITHIN INTERVAL\\t:\", len(within),\n",
    "        \"\\n DEBUG: RESID WITHIN\", within\n",
    "        )\"\"\"\n",
    "    # Evaluate the mutations. If the residue in question (corrected index) is named (i.e. \"H6.50\"), get its resepective label.\n",
    "    #print(f\"DEBUG {gain.rev_idx_dir.keys() = }\")\n",
    "    for mutated_resid in gain_mutation_dict.keys():\n",
    "        # Since fasta_offset maps to the first gain residue (i.e. 2027 with gain.start being 459), an offset needs to be set by the difference\n",
    "        corrected_resid = mutated_resid - fasta_offset + gain.start -1\n",
    "        # map it to its corresponding label. If there is none, continue\n",
    "        if corrected_resid < gain.start or corrected_resid >= gain.end:\n",
    "            gain_invalid += 1\n",
    "            continue\n",
    "        try:\n",
    "            mutated_label = gain.rev_idx_dir[corrected_resid]\n",
    "            #print(mutated_label)\n",
    "            gain_valid +=1\n",
    "        except KeyError:\n",
    "            gain_invalid += 1\n",
    "            continue\n",
    "        # SANITY CHECK:\n",
    "        if gain_mutation_dict[mutated_resid][0]['aa_change'][0] != gain.sequence[corrected_resid-gain.start]:\n",
    "            print(f\"MISMATCH! {mutated_resid} -> {corrected_resid}\",\n",
    "                 f\"{gain_mutation_dict[mutated_resid][0]['aa_change'][0]} : {gain.sequence[corrected_resid-gain.start]} in GAIN\",\n",
    "                 f\"{gain.sequence[corrected_resid-gain.start-2:corrected_resid-gain.start+3]}\")\n",
    "            mismatch_flag = True\n",
    "        #else:\n",
    "            #print(f\"MATCH {mutated_resid} -> {corrected_resid} | {gain_mutation_dict[mutated_resid][0]['aa_change'][0]} : {gain.sequence[corrected_resid-gain.start]}\")\n",
    "        \n",
    "        # with the Label, map into generalized dictionary\n",
    "        if mutated_label not in generalized_mutations.keys():\n",
    "            generalized_mutations[mutated_label] = gain_mutation_dict[mutated_resid]\n",
    "        else:\n",
    "            [generalized_mutations[mutated_label].append(mutation) for mutation in gain_mutation_dict[mutated_resid]]\n",
    "    valid += gain_valid\n",
    "    invalid += gain_invalid\n",
    "    print(\"VALID MAPPED POSITIONS: \", gain_valid, \"\\nINVALID POSITIONS     : \", gain_invalid, \"\\nTOTAL POSITIONS       : \", len(gain_mutation_dict.keys()))\n",
    "if mismatch_flag: \n",
    "    print(\"[WARNING]: MISMATCHES HAVE BEEN FOUND. PLEASE CHECK THE OUTPUT.\")\n",
    "else: \n",
    "    print(\"[NOTE]: NO MISMATCHES HAVE BEEN FOUND.\")\n",
    "print(f\"TOTAL MUTATIONS WITHIN GAIN:\", valid, \"\\nTOTAL MUTATIONS OUTSIDE GAIN:\", invalid, \"\\nTOTAL MUTATIONS:\", valid+invalid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In L3, there is a mismatch between canonical (UniProtKB) and the GDC form (equiv. Isoform 4) - see MSA mapping\n",
    "D2 (2/2)   | GDC ENST00000334810 = D2-201 (971 aa) - Uniprot Q7Z7M1 (963 aa) - Files OK - remap the four mutations in question : 496, 556, 460, 386, 507 -->  514, 529, 478, 404, X --> MSA mapping manual curation of these four mutations\n",
    "F4 (23/26) | GDC ENST00000283303 (695 aa) - Uniprot Q8IZF3 (695 aa) - Re-Download resolved all errors\n",
    "E2 (6/20)  | GDC ENST00000315576 (823 aa) - Uniprot Q9UHX3 (823 aa) - Re-Download resolved all errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dictionaries for the SNPs and the Cancer mutations.\n",
    "\"\"\"fasta_offsets = [2027, 2106, 2175, 479, 632, 588, 568, 528, 517, 579, 723, 2449, \n",
    "                 5619, 546, 0, 300, 22, 140, 112, 356, 154, 96, 365, 415, 265, \n",
    "                 170, 235, 120, 418, 269, 324]\"\"\"\n",
    "mutations, mut_counts = compose_vars(human_collection, jsons, 'x', 'aa_change', human_fasta_offsets)\n",
    "snps, snp_counts = compose_vars(human_collection, csvs, 'resid', 'resname', human_fasta_offsets)\n",
    "\n",
    "#print(valid, invalid, valid+invalid, total_within)\n",
    "# For each GAIN domain in the collection, count the occurence of a specific label\n",
    "occupancy_dict = {}\n",
    "for gain in human_collection.collection:\n",
    "    #rev_dir = {v: k for k, v in gain.named_dir.items()}\n",
    "    for key in gain.idx_dir.keys():\n",
    "        if key not in occupancy_dict.keys():\n",
    "            occupancy_dict[key] = 1\n",
    "        else:\n",
    "            occupancy_dict[key] += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the Cancer-enrichement scores of Positions. For each SSE, count the mutations/SNPs at each position and generate a score according to \n",
    "Wright et al. 2019 https://doi.org/10.1038/s41467-019-08630-2 - cancer-enriched positions will have a positive score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_y(x_range, sse, count_dict):\n",
    "    y_vals = np.zeros(shape=len(x_range))\n",
    "    for i, val in enumerate(x_range):\n",
    "        k = f\"{sse}{val}\"\n",
    "        try:\n",
    "            y_vals[i] = count_dict[k]\n",
    "        except:\n",
    "            pass\n",
    "    return y_vals\n",
    "\n",
    "def score(m,v):\n",
    "    mmax = max(m)\n",
    "    vmax = max(v)\n",
    "    return [math.log10( ((m[i]/mmax)+1) / ((v[i]/vmax)+1) ) for i in range(len(m))]            \n",
    "\n",
    "order = [f\"H{str(num)}.\" for num in range(1,7)]\n",
    "[order.append(f\"S{str(num)}.\") for num in range(1,15)]\n",
    "order.append(\"GPS\")\n",
    "\n",
    "for sse in order:\n",
    "# mutations, mut_counts\n",
    "# snps, snp_counts\n",
    "    varkeys = [k for k in mut_counts.keys() if sse in k]+[k for k in snp_counts.keys() if sse in k]\n",
    "    #print(subkeys)\n",
    "    if sse == \"GPS\":\n",
    "        x_range = (1,2,3)\n",
    "        mut_y = [mut_counts[\"GPS-1\"], mut_counts[\"GPS+1\"], 0]\n",
    "        var_y = [mut_counts[\"GPS-1\"], mut_counts[\"GPS+1\"], 0]\n",
    "    if sse != 'GPS':\n",
    "        x_positions = [int(x.split('.')[-1]) for x in varkeys]\n",
    "        x_range = range(min(x_positions), max(x_positions)+1)\n",
    "        mut_y = compose_y(x_range, sse, mut_counts)\n",
    "        var_y = compose_y(x_range, sse, snp_counts)\n",
    "\n",
    "    fig, ax = plt.subplots(1,3, figsize=(8,2))\n",
    "    fig.tight_layout()\n",
    "    fig.set_facecolor('w')\n",
    "    score_y = score(mut_y,var_y)\n",
    "    y_max = max(var_y)\n",
    "    xlab = [f\"{sse}{p}\" for p in x_range][::3]\n",
    "    for a in ax:    #ax = plt.subplot(1,3,2)\n",
    "        a.spines['top'].set_visible(False)\n",
    "        a.spines['right'].set_visible(False)\n",
    "        a.spines['bottom'].set_visible(False)\n",
    "        a.spines['left'].set_visible(False)\n",
    "        a.xaxis.set_minor_locator(MultipleLocator(1)) #AutoMinorLocator())\n",
    "        a.xaxis.set_major_locator(FixedLocator([a for a in range(2,100,3)]))#MultipleLocator(3))\n",
    "        a.tick_params(which='both', width=2)\n",
    "        a.tick_params(which='major', length=8)\n",
    "        a.tick_params(which='minor', length=6)\n",
    "\n",
    "    ax[0].set_ylim(-0.2, y_max)\n",
    "    ax[0].set_title(f'Cancer Variation ({sse[:-1]})')\n",
    "    ax[0].bar(x_range, mut_y, width=1.1, color='silver')  \n",
    "\n",
    "    ax[1].set_ylim(-0.2, y_max)\n",
    "    ax[1].set_title(f'Natural Variation ({sse[:-1]})')\n",
    "    ax[1].bar(x_range, var_y, width=1.1, color='silver')\n",
    "\n",
    "    ax[2].set_title(f'Combined Variation ({sse[:-1]})')\n",
    "    ax[2].hlines(0, min(x_range), max(x_range), color='dimgray', linewidth=0.7)\n",
    "    for i in range(len(x_range)):\n",
    "        if score_y[i] < 0:\n",
    "            cx = 'xkcd:blurple'\n",
    "        else:\n",
    "            cx = 'xkcd:tomato red'\n",
    "            if score_y[i] > 0.05:\n",
    "                label = f'{sse}{x_range[i]}'\n",
    "                print(f'{label},{round(score_y[i],2)}')\n",
    "        ax[2].bar(x_range[i], score_y[i], color=cx)\n",
    "\n",
    "    for a in ax:\n",
    "        a.set_xticklabels([f'{sse}{str(int(v))}' for v in a.get_xticks()], rotation=90)\n",
    "        \n",
    "    fig.savefig(f\"../fig/r3stal/{sse}.variants.svg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    #plt.show(fig)\n",
    "    #plt.close()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generalized_mutations.keys()\n",
    "\n",
    "names = []\n",
    "nums = []\n",
    "occus = []\n",
    "# Construct a sequence of secondary structural elements.\n",
    "order = [f\"H{str(num)}.\" for num in range(1,9)]\n",
    "[order.append(f\"S{str(num)}.\") for num in range(1,13)]\n",
    "order.append(\"GPS-2\")\n",
    "order.append(\"GPS-1\")\n",
    "order.append(\"GPS+1\")\n",
    "order.append(\"S13.\")\n",
    "\n",
    "for k, v in snps.items():\n",
    "    print(k, len(v))\n",
    "\n",
    "#print(order)\n",
    "# Enumerate SSE Sequence and find the corresponding keys, smash together into $names, $nums, $occus\n",
    "for sse in order:\n",
    "    #print(sse)\n",
    "    keys = sorted([ki for ki in generalized_mutations.keys() if sse in ki])\n",
    "    for ki in keys:\n",
    "        #print(ki,  len(generalized_mutations[ki]))#, \"\\tOCC\",occupancy_dict[ki])\n",
    "        pass\n",
    "        #mut_names.append(ki)                                # The position\n",
    "        #mut_nums.append(len(generalized_mutations[ki]))     # Number of mutations at this position\n",
    "        #mut_occus.append(occupancy_dict[ki])                # The mutations (data) in question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#print(names)\n",
    "fig = plt.figure(figsize=[9,4])\n",
    "fig.set_facecolor(\"w\")\n",
    "ymax = 15\n",
    "col_dict = { \"H\" : \"dodgerblue\",\n",
    "             \"S\" : \"darkorange\",\n",
    "             \"G\" : \"forestgreen\"}\n",
    "label_x = [0]\n",
    "label_txt = [\"GPS\"]\n",
    "stored = \"H\"\n",
    "stored_x = 0\n",
    "for lab_idx, label in enumerate(names):\n",
    "    #print(label.split(\".\")[0])\n",
    "    if label.split(\".\")[0] != stored:\n",
    "        if label not in [\"GPS-1\",\"GPS+1\"]:\n",
    "            box_col = col_dict[stored[0]]\n",
    "            col = col_dict[label[0]]\n",
    "            \n",
    "            plt.vlines(lab_idx, 0,ymax, color=col, alpha=0.4)\n",
    "            plt.fill_between([stored_x, lab_idx], 0, ymax, color=box_col, alpha=0.2)\n",
    "            \n",
    "            stored_x = lab_idx\n",
    "            stored = label.split(\".\")[0]\n",
    "            \n",
    "            label_x.append(lab_idx)\n",
    "            label_txt.append(label.split(\".\")[0])\n",
    "\n",
    "        plt.xticks(ticks = label_x, labels = label_txt, rotation=45)\n",
    "        #plt.yticks(ticks = [0,1,2], labels = [\"0\", \"100%\", \"200%\"])\n",
    "        #plt.ylabel(\"Mutation Number over Position Occurence\")\n",
    "        plt.xlabel(\"Element name\")\n",
    "#plt.fill_between([stored_x, lab_idx], 0, ymax, color=\"darkorange\", alpha=0.2)\n",
    "#for x in range(len(nums)):\n",
    "#    alfa = 0.2+(0.8*occus[x]/31)\n",
    "#    plt.scatter(x, nums[x]/occus[x], color='red', alpha=alfa, s=11)\n",
    "#plt.plot(range(len(nums)),nums, linewidth=1.5, color='black')\n",
    "#plt.bar(range(len(nums)),nums, color='black')\n",
    "plt.scatter(range(len(nums)),np.divide(nums, occus), color='red', s=6)\n",
    "#plt.savefig(\"corr.human_indexed_.png\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b-factor : line[60:66]\n",
    "d1_gain = human_collection.collection[26]\n",
    "print(d1_gain.name, d1_gain.start, d1_gain.end)\n",
    "ref_pdb = \"d1_gain.pdb\"\n",
    "with open(ref_pdb) as inpdb:\n",
    "    pdb_data = inpdb.readlines()\n",
    "#names, nums, occus\n",
    "print(d1_gain.idx_dir)\n",
    "new_data = []\n",
    "for line in pdb_data:\n",
    "    if not line.startswith(\"ATOM\"):\n",
    "        new_data.append(line)\n",
    "        continue\n",
    "    resid = int(line[22:26])\n",
    "    try:\n",
    "        position_label = d1_gain.rev_idx_dir[resid-1] # i.e. \"H2.54\", account for zero vs. one-indexed!\n",
    "        try: \n",
    "            b = float(len(generalized_mutations[position_label]))\n",
    "            #print(position_label, len(generalized_mutations[position_label]))\n",
    "        except KeyError:\n",
    "            b = 0.0\n",
    "    except KeyError: \n",
    "        b = 0.0\n",
    "    new_data.append(line[:61]+\"%6.2f\"%(b)+line[68:])\n",
    "\n",
    "with open(\"d1_gain_mut.pdb\",\"w\") as outpdb:\n",
    "    for line in new_data:\n",
    "        outpdb.write(line)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b-factor : line[60:66]\n",
    "for i, gain in enumerate(human_collection.collection):\n",
    "    if \"AGRL1\" in gain.name: \n",
    "        print(i)\n",
    "        break\n",
    "l1_gain = human_collection.collection[i]\n",
    "print(l1_gain.name, l1_gain.start, l1_gain.end)\n",
    "ref_pdb = \"../repo_data/l1_gain.pdb\"\n",
    "with open(ref_pdb) as inpdb:\n",
    "    pdb_data = inpdb.readlines()\n",
    "#names, nums, occus\n",
    "print(l1_gain.idx_dir)\n",
    "new_data = []\n",
    "for line in pdb_data:\n",
    "    if not line.startswith(\"ATOM\"):\n",
    "        new_data.append(line)\n",
    "        continue\n",
    "    resid = int(line[22:26])\n",
    "    try:\n",
    "        position_label = l1_gain.rev_idx_dir[resid-1] # i.e. \"H2.54\" account for 0-indexed residues in the dictionary!\n",
    "        try: \n",
    "            b = float(len(generalized_mutations[position_label]))\n",
    "            #print(position_label, len(generalized_mutations[position_label]))\n",
    "        except KeyError:\n",
    "            b = 0.0\n",
    "    except KeyError: \n",
    "        b = 0.0\n",
    "    new_data.append(line[:61]+\"%6.2f\"%(b)+line[68:])\n",
    "\n",
    "with open(\"l1_gain_mut.pdb\",\"w\") as outpdb:\n",
    "    for line in new_data:\n",
    "        outpdb.write(line)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(generalized_mutations[label])\n",
    "def position_vars(gen_vars, label, aa_key=None, text_out=None, return_aa=False):\n",
    "    # Prints or writes to File the mutations at a specific label position.\n",
    "    def tee(file_obj=None, text_string=\"\"):\n",
    "        # Selector function similar to UNIX \"tee\" -> where it prints and - if existing - writes to File.\n",
    "        if file_obj is not None:\n",
    "            file_obj.write(text_string)\n",
    "        print(text_string, end='')\n",
    "\n",
    "    if text_out is not None: # Initialize TEXT output file if specified. Otherwise construct None\n",
    "        text = open(text_out, 'w')\n",
    "    else: text = None\n",
    "\n",
    "    #tee(text, f\"CANCER GENOME ATLAS MUTATIONS AT POSITION {label.upper()} \\nTOTAL: {len(gen_vars[label])} MUTATIONS\\n\\n\")\n",
    "\n",
    "    rstring=''\n",
    "    for var in gen_vars[label]:\n",
    "        for key in var.keys():\n",
    "            if return_aa and key == 'aa_change':\n",
    "                    rlet = var[key][0]\n",
    "                    rstring = rstring+rlet\n",
    "            #tee(text, f\"{key.ljust(30)} {var[key]}\\n\")\n",
    "        if aa_key is not None:\n",
    "            resid = int(var[aa_key]) # X is the residue ID\n",
    "        #tee(text, \"\\n\"+\"_\"*30+\"\\n\")\n",
    "    print(label, \",\", rstring)\n",
    "\n",
    "enriched_positions = ['H1.50','H1.54','H1.57','H1.61','H2.56','H2.57','H2.60','H2.61','H3.36','H3.43',\n",
    "'H3.44','H3.51','H3.53','H3.56','H4.38','H4.41','H4.51','H5.37','H5.38','H5.42','H5.44',\n",
    "'H5.48','H5.50','H5.59','H6.42','H6.54','H6.56','H7.40','H7.51','H8.46','H8.58','H8.60',\n",
    "'S1.48','S2.47','S2.51','S2.53','S2.58','S3.53','S3.55','S4.56','S5.48','S5.52','S5.55',\n",
    "'S6.50','S7.45','S7.52','S7.57','S9.53','S10.50','S11.54','S11.55','S12.47','S12.50','S12.52','S13.48','S13.50']\n",
    "\n",
    "for label in enriched_positions:\n",
    "    position_vars(generalized_mutations, label, return_aa=True)#text_out=f\"../mut/{label}.out\", return_aa=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_criteria(*pairs, sift=None, poly=None, mutation_dict):\n",
    "    # Defines a function where key - value pairs will be used to specifically filter criteria to be applied for subselection of mutations.\n",
    "    #       i.e. (\"consequence\", \"missense\"), (\"sift_impact\",\"deleterious\")\n",
    "    # Scores \"sift\" and \"polyphen\" define minimum and maximum values with an operator to determine lower than or higher than $val cutoffs\n",
    "    #       i.e. (\"sift_score\", \">\", 0.22) as tuples\n",
    "    print(\"DEBUG:\", pairs)\n",
    "    def eval_entry(pairs, entry):\n",
    "        if pairs is None:\n",
    "            return True\n",
    "\n",
    "        for tup in pairs:\n",
    "            if entry[tup[0]] != tup[1]:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Set a custom_func flag indicating if there are custom dependent lambda functions for scores specified\n",
    "    custom_func = True\n",
    "    if sift is None and poly is None:\n",
    "        custom_func = False\n",
    "    if sift is None:\n",
    "        sift = lambda n: True\n",
    "    if poly is None:\n",
    "        poly = lambda n: True\n",
    "\n",
    "        \n",
    "    print(\"DEBUG\", custom_func)\n",
    "\n",
    "    valid_ct = 0\n",
    "    out_dict = {}\n",
    "    # We want to filter the generalized dictionary by key\n",
    "    for ki in mutation_dict.keys():\n",
    "        mutation_list = mutation_dict[ki]\n",
    "        filtered_mutation_list = []\n",
    "        for mutation in mutation_list:\n",
    "            # Check the scores first. If there are no scores, but custom functions specified, thats ok. Otherwise, do not consider this entry\n",
    "            try: \n",
    "                s = float(mutation[\"sift_score\"])\n",
    "                p = float(mutation[\"polyphen_score\"])\n",
    "            except:\n",
    "                #print(\"Scores not in Entry...\", end ='')\n",
    "                if custom_func == True:\n",
    "                    continue\n",
    "                print(\"But no score functions specified\")\n",
    "            if custom_func and not sift(float(mutation[\"sift_score\"])):\n",
    "                continue\n",
    "            if custom_func and not poly(float(mutation[\"polyphen_score\"])):\n",
    "                continue\n",
    "            # Here, check the *pairs argument if that matches the specifications.\n",
    "            if not eval_entry(pairs, mutation):\n",
    "                continue\n",
    "            \n",
    "            valid_ct += 1\n",
    "            filtered_mutation_list.append(mutation)\n",
    "        out_dict[ki] = filtered_mutation_list\n",
    "    print(f\"Found {valid_ct} entries matching criteria.\")\n",
    "    return out_dict\n",
    "            \n",
    "# Example Lambda Functions for score evaluation\n",
    "poly = lambda n : n > 0.446 # > 0.446 for possibly damaging, > 0.908 for Probably Damaging\n",
    "sift = lambda n : n < 0.05 # For deleterious impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missense_dict = query_criteria((\"consequence\", \"missense\"), poly=lambda n : n > 0.446, mutation_dict=generalized_mutations)\n",
    "missense_dict2 = query_criteria(mutation_dict=generalized_mutations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gain in human_collection.collection:\n",
    "    if \"AGRL3\" in gain.name:\n",
    "        print(\"DEBUG:\", gain.name, gain.idx_dir, \"\".join(gain.sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"parse_dict, data = retrieve_csv_vars(csvs[0], \n",
    "                                    return_keys=['Protein Consequence', 'Transcript', 'Transcript Consequence', 'VEP Annotation'], \n",
    "                                    filter_str = 'missense_variant'\n",
    "                                    )\"\"\"\n",
    "variants = retrieve_csv_vars(human_collection.collection[0].name, csvs, filter_str='missense_variant', with_resid=True)\n",
    "\n",
    "# Filter for VEP annotation to only get the missense mutation\n",
    "oneletter = {'Ala':'A','Arg':'R','Asn':'N','Asp':'D','Cys':'C','Gln':'Q','Glu':'E','Gly':'G','His':'H','Ile':'I','Leu':'L','Lys':'K','Met':'M','Phe':'F','Pro':'P','Pyl':'O','Ser':'S','Thr':'T','Trp':'W','Tyr':'Y','Val':'V'}\n",
    "\n",
    "for var in variants:\n",
    "#for mutation in data[idx,:]:\n",
    "    if 'Protein Consequence' not in var.keys():\n",
    "        continue\n",
    "    #print(var['Protein Consequence'])\n",
    "    #print(re.findall('[A-z]{3}', var['Protein Consequence']))\n",
    "    res = oneletter[re.findall('[A-z]{3}', var['Protein Consequence'])[0]] # A, D, ...\n",
    "    resnum = int(re.findall('[\\d]+', var['Protein Consequence'])[0]) # 1364\n",
    "    print(res, resnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets map the vars/mutations to the loops as wäll\n",
    "\n",
    "loop_lengths = {}\n",
    "loop_seqs = {}\n",
    "loop_seq = {}\n",
    "\n",
    "loop_info = {}\n",
    "#[loop_info[loop] = {} for loop in loop_seqs.keys()] # into each of these keys, any entry is composed of \"name\":$name, \"sequence\":$seq\n",
    "\n",
    "for idx, gain in enumerate(human_collection.collection):\n",
    "    curr_name = gain.name\n",
    "    #idx_dir, cent, named_dir, _ = sse_func.create_indexing(gain, anchors, anchor_occupation, anchor_dict, outdir=None, offset=human_fasta_offsets[idx], silent=True, split_mode='double')\n",
    "    idx_dir, cent, named_dir, _,_ = tf.assign_indexing(gain_obj=gain,\n",
    "                                            file_prefix=f'../indexing_tmp/x{gain_idx}',\n",
    "                                            gain_pdb=sse_func.find_pdb(gain.name, '../all_pdbs/'),\n",
    "                                            template_dir='../r2_template_pdbs',\n",
    "                                            gesamt_bin=\"/home/hildilab/lib/xtal/ccp4-8.0/ccp4-8.0/bin/gesamt\",\n",
    "                                            hard_cut={\"S2\":7,\"S6\":3,\"H5\":3},\n",
    "                                            debug=False,\n",
    "                                            patch_gps=True)\n",
    "    #print(f'{named_dir = }')\n",
    "    i_loc, i_dir = get_loop_stats(idx_dir, gain.sequence)\n",
    "    for k, seq in i_dir.items():\n",
    "        if k not in loop_info.keys():\n",
    "            loop_info[k] = []\n",
    "        loop_info[k].append({'name':f'{gain.name}_{i_loc[k][0]+gain.start}-{i_loc[k][1]+gain.start}', 'sequence':''.join(seq)})\n",
    "\n",
    "#for loop in loop_info.keys():\n",
    "#    loop2fasta(f\"../loops_human/{loop}.fa\", loop_info[loop])\n",
    "\n",
    "# mafft\n",
    "mafft_aln = glob.glob('../loops_human/aln/*fa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a labeled dict with each connector containing the corresponding mutations for this - independent of the receptor\n",
    "# ie \"H8-S1\": [{mut1}, {mut2}]\n",
    "# we rather not use the Alignment to sort mutations, since the loops themselves have too much variability as shown in the ALN. Subfamily-level.\n",
    "\n",
    "loop_muts, loop_counts = compose_loop_vars(human_collection, jsons, resid_key='x', aa_key='aa_change', fasta_offsets=human_fasta_offsets)\n",
    "\n",
    "for loop in loop_counts.keys():\n",
    "    print(loop, loop_counts[loop])\n",
    "    #for k, seq in i_dir.items():\n",
    "    #    if k not in loop_info.keys():\n",
    "    #        loop_info[k] = []\n",
    "    #    loop_info[k].append({'name':f'{gain.name}_{i_loc[k][0]+gain.start}-{i_loc[k][1]+gain.start}', 'sequence':''.join(seq)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#position_vars(loop_muts,'S9-S10', aa_key='x')\n",
    "[position_vars(generalized_mutations, x) for x in generalized_mutations.keys() if 'S9' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed671274c087d00ab9ecb8e0fee95cd6aaf5c323b152e6f5d51e6ecc9c97d119"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
