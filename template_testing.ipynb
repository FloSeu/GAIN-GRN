{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "import glob, math, json, glob, re\n",
    "#from shutil import copyfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import logomaker\n",
    "# LOCAL IMPORTS\n",
    "#from indexing_classes import GPCRDBIndexing\n",
    "from gain_classes import GainDomain, GainCollection, Anchors, GPS\n",
    "import sse_func\n",
    "import matplotlib.pyplot as plt\n",
    "import template_finder as tf\n",
    "\n",
    "def calc_identity(aln_matrix, return_best_aa=False):\n",
    "    # This takes an alignment matrix with shape=(n_columns, n_sequences) and generates counts based on the identity matrix.\n",
    "    # Returns the highest non \"-\" residue count as the most conserved residue and its occupancy based on count(\"-\") - n_struc\n",
    "    n_struc = aln_matrix.shape[0]\n",
    "    quality = []\n",
    "    occ = []\n",
    "    aa = []\n",
    "    for col in range(aln_matrix.shape[1]):\n",
    "        chars, count = np.unique(aln_matrix[:,col], return_counts=True)\n",
    "        dtype = [('aa', 'S1'), ('counts', int)]\n",
    "        values = np.array(list(zip(chars,count)), dtype=dtype)\n",
    "        s_values = np.sort(values, order='counts')\n",
    "\n",
    "        if s_values[-1][0] == b'-':\n",
    "            q = s_values[-2][1]\n",
    "            aa.append(s_values[-2][0])\n",
    "        else:\n",
    "            q = s_values[-1][1]\n",
    "            aa.append(s_values[-1][0])\n",
    "        x = np.where(chars == '-')[0][0]\n",
    "        occ.append(n_struc - count[x])\n",
    "        quality.append(q)\n",
    "    if not return_best_aa:\n",
    "        return quality, occ\n",
    "    if return_best_aa:\n",
    "        return quality, occ, aa\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize pre-calculated metrix for the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'precalc_anchors = [ 662, 1194, 1912, 2490, 2848, 3011, 3073, 3260, #H1-H8\\n            3455, 3607, 3998, 4279, 4850, 5339, #5341 S1-S6, S7 REMOVED!\\n            5413, 5813, 6337, 6659, 6696, 6765, 6808] #S8-13\\nprecalc_anchor_occupation = [ 4594.,  6539., 11392., 13658.,  8862., 5092.,  3228., 14189., #H1-H8\\n                      9413., 12760.,  9420., 11201., 12283., 3676.,#  4562. S1-S6, S7 REMOVED!\\n                     13992., 12575., 13999., 14051., 14353., 9760., 14215.] #S8-13\\nprecalc_anchor_dict = sse_func.make_anchor_dict(precalc_anchors, 3425)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''valid_seqs = sse_func.read_multi_seq(\"/home/hildilab/projects/agpcr_nom/app_gain_gain.fa\")\n",
    "quality_file = \"/home/hildilab/projects/agpcr_nom/app_gain_gain.mafft.jal\"\n",
    "alignment_file = \"/home/hildilab/projects/agpcr_nom/app_gain_gain.mafft.fa\"\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2/*\")\n",
    "# This only contains the sigma files for truncated (?) PDBs.\n",
    "#quality = sse_func.read_quality(quality_file)\n",
    "gps_minus_one = 6781 \n",
    "aln_cutoff = 6826 \n",
    "alignment_dict = sse_func.read_alignment(alignment_file, aln_cutoff)\n",
    "aln_matrix = np.array([list(seq) for seq in alignment_dict.values()])\n",
    "#print(aln_matrix.shape)\n",
    "quality, occ = calc_identity(aln_matrix)'''\n",
    "\n",
    "\"\"\"precalc_anchors = [ 662, 1194, 1912, 2490, 2848, 3011, 3073, 3260, #H1-H8\n",
    "            3455, 3607, 3998, 4279, 4850, 5339, #5341 S1-S6, S7 REMOVED!\n",
    "            5413, 5813, 6337, 6659, 6696, 6765, 6808] #S8-13\n",
    "precalc_anchor_occupation = [ 4594.,  6539., 11392., 13658.,  8862., 5092.,  3228., 14189., #H1-H8\n",
    "                      9413., 12760.,  9420., 11201., 12283., 3676.,#  4562. S1-S6, S7 REMOVED!\n",
    "                     13992., 12575., 13999., 14051., 14353., 9760., 14215.] #S8-13\n",
    "precalc_anchor_dict = sse_func.make_anchor_dict(precalc_anchors, 3425)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to the collection, we need the corresponding PDB files.\n",
    "valid_collection = pd.read_pickle(\"../valid_collection.pkl\")\n",
    "allpdbs = glob.glob('../all_pdbs/*.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0A6G1Q0B9-A0A6G1Q0B9_9TELE-AGRA2-Channa_argus 430 541 749 \n",
      "\n",
      "HELIX  #1: L444 @ SSE residue 9 | q = 6152.0 with res_idx 13 | [4123.0, 3101.0, 1122.0, 1154.0, 1448.0, 1826.0, 507.0, 4311.0, 1196.0, 6152.0, 4769.0] | 434-444\n",
      "HELIX  #2: V459 @ SSE residue 6 | q = 9625.0 with res_idx 28 | [1526.0, 2061.0, 3425.0, 2580.0, 909.0, 4501.0, 9625.0, 961.0, 657.0, 1207.0, 1361.0, 3982.0, 4258.0, 2598.0, 7156.0, 2344.0, 1344.0] | 452-468\n",
      "HELIX  #3: D474 @ SSE residue 1 | q = 9594.0 with res_idx 43 | [4475.0, 9594.0, 1731.0, 4584.0, 720.0, 2789.0, 4062.0, 3264.0, 3938.0, 6238.0, 1975.0, 2470.0, 2539.0, 1539.0, 507.0, 1092.0, 2542.0, 3321.0, 3576.0, 1905.0, 3941.0, 2501.0, 7326.0, 1936.0, 3861.0, 3166.0, 5774.0, 1096.0, 2500.0, 5492.0, 7088.0, 5871.0, 7573.0, 7316.0, 9472.0, 1120.0] | 472-507\n",
      "HELIX  #4: A515 @ SSE residue 3 | q = 10269.0 with res_idx 84 | [3636.0, 758.0, 3434.0, 10269.0, 2700.0, 4348.0, 5015.0, 916.0, 6692.0, 1699.0] | 511-520\n",
      "HELIX  #5: E532 @ SSE residue 8 | q = 10433.0 with res_idx 101 | [5172.0, 5562.0, 3071.0, 10250.0, 8766.0, 3786.0, 5111.0, 5204.0, 10433.0, 3899.0] | 523-532\n",
      "Element length below threshold. Skipping. [106 108]\n",
      "Element length below threshold. Skipping. [132 134]\n",
      "__________\n"
     ]
    }
   ],
   "source": [
    "# Get the Data for the initially selected templates\n",
    "def get_gain(identifier, a_gain_collection):\n",
    "    for gain in a_gain_collection:\n",
    "        if identifier in gain.name:\n",
    "            return gain\n",
    "        \n",
    "def get_struc_aln_anchors(gain, aln_dict, subdomain='a', threshold=3):\n",
    "    aln_matrix = np.array([list(seq) for seq in aln_dict.values()])\n",
    "    # Get the identity scores from the alignment\n",
    "    quality, occ, aa = calc_identity(aln_matrix, return_best_aa=True)\n",
    "    # The columns here exactly correspond to the template sequence order\n",
    "    if subdomain.lower() == 'a':\n",
    "        sse = gain.sda_helices\n",
    "        d_string = \"HELIX \"\n",
    "        sse_type = \"H\"\n",
    "    elif subdomain.lower() == 'b':\n",
    "        sse = gain.sdb_sheets\n",
    "        d_string = \"STRAND\"\n",
    "        sse_type = \"S\"\n",
    "    else:\n",
    "        print(\"NO SUBDOMAIN specified. EXITING.\")\n",
    "    \n",
    "    anchor_quality = {}\n",
    "    anchors = {}\n",
    "    counter = 1\n",
    "\n",
    "    for i,element in enumerate(sse):\n",
    "        if element[1]-element[0] <= threshold:\n",
    "            print(\"Element length below threshold. Skipping.\", element)\n",
    "            continue\n",
    "        if subdomain =='a' and gain.start+element[0] > gain.subdomain_boundary:\n",
    "            print(\"Skipping Subdomain A Helix\", element)\n",
    "            continue\n",
    "\n",
    "        q = quality[element[0]:element[1]+1]\n",
    "        label = f'{sse_type}{counter}'\n",
    "        max_id = element[0]+np.argmax(q)\n",
    "        max_res = gain.sequence[max_id]\n",
    "\n",
    "        res_id = gain.start+max_id+1\n",
    "\n",
    "        print(f\"{d_string} #{i+1}: {max_res}{res_id} @ SSE residue {max_id-element[0]} | q = {np.max(q)} with res_idx {max_id} | MOST CONSERVED: {aa[max_id]} | PDB-res {gain.start+element[0]+1}-{gain.start+element[1]+1}\")\n",
    "        anchor_quality[label] = np.max(q)\n",
    "        anchors[label] = max_id\n",
    "        counter += 1\n",
    "        pdb_anchors = {v:k+gain.start+1 for v,k in anchors.items()}\n",
    "    print(\"__________\")\n",
    "    return anchors, anchor_quality, pdb_anchors\n",
    "\n",
    "def get_template_information(identifier, gain_collection, subdomain='a', threshold=3, no_input=True):\n",
    "    for gain in gain_collection.collection:\n",
    "        if identifier in gain.name:\n",
    "            print(gain.name, gain.start, gain.subdomain_boundary, gain.end, \"\\n\")\n",
    "\n",
    "            if subdomain.lower() == 'a':\n",
    "                sse = gain.sda_helices\n",
    "                d_string = \"HELIX \"\n",
    "                sse_type = \"H\"\n",
    "            elif subdomain.lower() == 'b':\n",
    "                sse = gain.sdb_sheets\n",
    "                d_string = \"STRAND\"\n",
    "                sse_type = \"S\"\n",
    "            else:\n",
    "                print(\"NO SUBDOMAIN specified. EXITING.\")\n",
    "        \n",
    "            #print(sse)\n",
    "            anchor_quality = {}\n",
    "            anchors = {}\n",
    "            counter = 1\n",
    "            aln_indices = []\n",
    "            for i,element in enumerate(sse):\n",
    "                if element[1]-element[0] <= threshold:\n",
    "                    print(\"Element length below threshold. Skipping.\", element)\n",
    "                    continue\n",
    "                if subdomain =='a' and gain.start+element[0] > gain.subdomain_boundary:\n",
    "                    print(\"Skipping Subdomain A Helix\", element)\n",
    "                    continue\n",
    "                label = f'{sse_type}{counter}'\n",
    "                q = [ gain.residue_quality[res] for res in range(element[0], element[1]+1)]\n",
    "                max_id = element[0]+np.argmax(q)\n",
    "                max_res = gain.sequence[max_id]\n",
    "                #aln_idx = gain.alignment_indices[max_id]\n",
    "                res_id = gain.start+max_id+1\n",
    "                print(f\"{d_string} #{i+1}: {max_res}{res_id} @ SSE residue {max_id-element[0]} | q = {np.max(q)} with res_idx {max_id} | {q} | {gain.start+element[0]}-{gain.start+element[1]}\")\n",
    "                if not no_input:\n",
    "                    confirm = input(f\"{d_string} #{i+1}: {max_res}{res_id} @ SSE re {max_id-element[0]} | q={np.max(q)} w res_idx {max_id} | {gain.start+element[0]}-{gain.start+element[1]}. Keep?\")\n",
    "                    if confirm.lower() != \"y\":\n",
    "                        print(\"Skipping this anchor.\");continue\n",
    "                anchor_quality[label] = np.max(q)\n",
    "                anchors[label] = max_id\n",
    "                aln_indices.append(gain.alignment_indices[max_id])\n",
    "                counter += 1\n",
    "            pdb_anchors = {v:k+gain.start+1 for v,k in anchors.items()}\n",
    "            print(\"__________\")\n",
    "            return anchors, anchor_quality, aln_indices, pdb_anchors\n",
    "_,_,_,_ = get_template_information('A0A6G1Q0B9', valid_collection, 'a')\n",
    "#get_template_information('A0A3P8S994', valid_collection, 'b')\n",
    "        ##_, cd, _, _ = gain.create_indexing(precalc_anchors, precalc_anchor_occupation, precalc_anchor_dict)\n",
    "        #_, cd, _, _ = gain.create_indexing(anchors, anchor_occupation, anchor_dict)\n",
    "        #ac =  {k[:-3]:gain.start+v for k,v in cd.items()}\n",
    "        #print(ac)\n",
    "        #print(gain.Anchors.alignment_indices )\n",
    "        #print(gain.Anchors.gain_residues)\n",
    "        #print(gain.start)\n",
    "        #qq =[ quality[v] for v in gain.alignment_indices[324-gain.start:331-gain.start]]\n",
    "        #print(qq)\n",
    "        #print(gain.alignment_indices[324-gain.start:331-gain.start])\n",
    "# pre:  'H1': 309, 'H2': 324, 'H3': 358, 'H4': 385, 'H5': 410, 'H6': 421, 'H8': 438\n",
    "# cons:            'H1': 317, 'H2': 358, 'H3': 377, 'H4': 415, 'H6': 425, 'H7': 443,\n",
    "\n",
    "# pre:  'S1': 621, 'S2': 628, 'S3': 643, 'S4': 669,                         'S7': 717,  'S8': 734,  'S9': 750, 'S10': 764, 'S11': 770, 'S12': 782, 'S13': 793\n",
    "# cons: 'S1': 621, 'S3': 629, 'S4': 644, 'S5': 671, 'S7': 697, 'S10': 712, 'S11': 717, 'S13': 736, 'S14': 753, 'S15': 764, 'S16': 770, 'S18': 784, 'S19': 793"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    tf.run_gesamt_execution(valid_collection.collection, \\n                            outfolder=gesamt_outfolder, \\n                            pdb_folder='../all_pdbs', \\n                            domain='sda', \\n                            n_threads=5, \\n                            max_struc=len(valid_collection.collection), \\n                            no_run=True,\\n                            template=current_template)\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a GESAMT bashfile for pairwise aln to each subdomain. Output the resulting PDB into respective folder\n",
    "#SDB TEMPLATE\n",
    "import shutil\n",
    "sdb_template = 'A0A3P8S994-A0A3P8S994_AMPPE-AGRE5b,duplicate2-Amphiprion_percula'\n",
    "# SDA TEMPLATEs\n",
    "sda_templates = {'A': 'A0A2Y9F628-A0A2Y9F628_PHYMC-AGRA3isoformX1-Physeter_macrocephalus', \n",
    "                    'B': 'A0A4W6DVA0-A0A4W6DVA0_LATCA-AGRB1b-Lates_calcarifer', \n",
    "                    'C': 'A0A7K6E127-A0A7K6E127_9PASS-CELR3protein-Grantiella_picta.', \n",
    "                    'D': 'A0A1A7WJQ6-A0A1A7WJQ6_9TELE-GR144-Iconisemion_striatum.', \n",
    "                    'E': 'A0A3P8S994-A0A3P8S994_AMPPE-AGRE5b,duplicate2-Amphiprion_percula', \n",
    "                    'F': 'A0A452IH20-A0A452IH20_9SAUR-AGRF5-Gopherus_agassizii', \n",
    "                    'G': 'A0A1W4WJB1-A0A1W4WJB1_AGRPL-AGRG6-likeisoformX1-Agrilus_planipennis', #'A0A7K5TKG3-A0A7K5TKG3_9FRIN-AGRG6protein-Urocynchramus_pylzowi.', missing H1\n",
    "                    'L': 'A0A452HCU9-A0A452HCU9_9SAUR-AGRL3-Gopherus_agassizii', \n",
    "                    'V': 'A0A6Q2XYK2-A0A6Q2XYK2_ESOLU-AGRV1-Esox_lucius',\n",
    "                    'X': \"A0A6F9A857-A0A6F9A857_9TELE-Uncharacterizedprotein-Coregonus_sp._'balchen'.\"}\n",
    "def find_pdb(name, pdb_folder):\n",
    "    identifier = name.split(\"-\")[0]\n",
    "    target_pdb = glob.glob(f\"{pdb_folder}/*{identifier}*.pdb\")[0]\n",
    "    return target_pdb\n",
    "\n",
    "sdb_template_pdb = find_pdb(sdb_template, '../all_pdbs')\n",
    "\"\"\"tf.run_gesamt_execution(valid_collection.collection, \n",
    "                            outfolder=\"../A0A3P8S994_sdb\",\n",
    "                            pdb_folder='../all_pdbs', \n",
    "                            domain='sdb', \n",
    "                            n_threads=5, \n",
    "                            max_struc=len(valid_collection.collection), \n",
    "                            no_run=False,\n",
    "                            template=sdb_template_pdb)\"\"\"\n",
    "#shutil.copyfile(sdb_template_pdb, f'../calc_templates/sdb_A0A3P8S994.pdb')\n",
    "\n",
    "for fam, prot in sda_templates.items():\n",
    "    identifier = prot.split(\"-\")[0]\n",
    "    current_template = find_pdb(prot, '../all_pdbs')\n",
    "\n",
    "    shutil.copyfile(current_template, f'../calc_templates/{fam}_{identifier}.pdb')\n",
    "    gesamt_outfolder = f'../{identifier}_{fam}_sda'\n",
    "\n",
    "\"\"\"    tf.run_gesamt_execution(valid_collection.collection, \n",
    "                            outfolder=gesamt_outfolder, \n",
    "                            pdb_folder='../all_pdbs', \n",
    "                            domain='sda', \n",
    "                            n_threads=5, \n",
    "                            max_struc=len(valid_collection.collection), \n",
    "                            no_run=True,\n",
    "                            template=current_template)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRAND #1: T328 @ SSE residue 4 | q = 5675 with res_idx 102 | MOST CONSERVED: b'T' | PDB-res 324-328\n",
      "STRAND #2: V333 @ SSE residue 1 | q = 8382 with res_idx 107 | MOST CONSERVED: b'L' | PDB-res 332-340\n",
      "STRAND #3: L350 @ SSE residue 2 | q = 7127 with res_idx 124 | MOST CONSERVED: b'F' | PDB-res 348-353\n",
      "STRAND #4: T359 @ SSE residue 3 | q = 8653 with res_idx 133 | MOST CONSERVED: b'L' | PDB-res 356-361\n",
      "STRAND #5: Y381 @ SSE residue 7 | q = 8465 with res_idx 155 | MOST CONSERVED: b'Y' | PDB-res 374-382\n",
      "STRAND #6: V409 @ SSE residue 2 | q = 8166 with res_idx 183 | MOST CONSERVED: b'V' | PDB-res 407-410\n",
      "STRAND #7: V414 @ SSE residue 0 | q = 7676 with res_idx 188 | MOST CONSERVED: b'I' | PDB-res 414-419\n",
      "STRAND #8: H436 @ SSE residue 5 | q = 9338 with res_idx 210 | MOST CONSERVED: b'H' | PDB-res 431-436\n",
      "STRAND #9: W453 @ SSE residue 6 | q = 13341 with res_idx 227 | MOST CONSERVED: b'W' | PDB-res 447-455\n",
      "Element length below threshold. Skipping. [233 235]\n",
      "STRAND #11: N470 @ SSE residue 5 | q = 6618 with res_idx 244 | MOST CONSERVED: b'N' | PDB-res 465-470\n",
      "STRAND #12: C478 @ SSE residue 4 | q = 14069 with res_idx 252 | MOST CONSERVED: b'C' | PDB-res 474-479\n",
      "STRAND #13: L487 @ SSE residue 3 | q = 13774 with res_idx 261 | MOST CONSERVED: b'L' | PDB-res 484-491\n",
      "__________\n",
      "HELIX  #1: T416 @ SSE residue 1 | q = 4775 with res_idx 5 | MOST CONSERVED: b'S' | PDB-res 415-424\n",
      "HELIX  #2: L439 @ SSE residue 6 | q = 9299 with res_idx 28 | MOST CONSERVED: b'L' | PDB-res 433-449\n",
      "HELIX  #3: D454 @ SSE residue 1 | q = 8616 with res_idx 43 | MOST CONSERVED: b'D' | PDB-res 453-468\n",
      "HELIX  #4: M489 @ SSE residue 13 | q = 9151 with res_idx 78 | MOST CONSERVED: b'L' | PDB-res 476-490\n",
      "HELIX  #5: L496 @ SSE residue 2 | q = 7365 with res_idx 85 | MOST CONSERVED: b'W' | PDB-res 494-503\n",
      "HELIX  #6: Q514 @ SSE residue 8 | q = 9618 with res_idx 103 | MOST CONSERVED: b'E' | PDB-res 506-523\n",
      "__________\n",
      "HELIX  #1: M531 @ SSE residue 5 | q = 4766 with res_idx 6 | MOST CONSERVED: b'Q' | PDB-res 526-542\n",
      "HELIX  #2: L556 @ SSE residue 6 | q = 9845 with res_idx 31 | MOST CONSERVED: b'L' | PDB-res 550-563\n",
      "HELIX  #3: D569 @ SSE residue 1 | q = 8707 with res_idx 44 | MOST CONSERVED: b'D' | PDB-res 568-586\n",
      "HELIX  #4: L607 @ SSE residue 14 | q = 9308 with res_idx 82 | MOST CONSERVED: b'L' | PDB-res 593-607\n",
      "HELIX  #5: W615 @ SSE residue 5 | q = 8581 with res_idx 90 | MOST CONSERVED: b'W' | PDB-res 610-621\n",
      "HELIX  #6: F628 @ SSE residue 2 | q = 8671 with res_idx 103 | MOST CONSERVED: b'L' | PDB-res 626-642\n",
      "__________\n",
      "HELIX  #1: E463 @ SSE residue 9 | q = 3808 with res_idx 15 | MOST CONSERVED: b'Q' | PDB-res 454-467\n",
      "HELIX  #2: L484 @ SSE residue 9 | q = 9606 with res_idx 36 | MOST CONSERVED: b'L' | PDB-res 475-490\n",
      "HELIX  #3: D498 @ SSE residue 1 | q = 8508 with res_idx 50 | MOST CONSERVED: b'D' | PDB-res 497-515\n",
      "HELIX  #4: T524 @ SSE residue 4 | q = 2108 with res_idx 76 | MOST CONSERVED: b'S' | PDB-res 520-525\n",
      "HELIX  #5: L541 @ SSE residue 11 | q = 9198 with res_idx 93 | MOST CONSERVED: b'L' | PDB-res 530-541\n",
      "HELIX  #6: W549 @ SSE residue 5 | q = 5233 with res_idx 101 | MOST CONSERVED: b'W' | PDB-res 544-551\n",
      "HELIX  #7: R567 @ SSE residue 7 | q = 9236 with res_idx 119 | MOST CONSERVED: b'E' | PDB-res 560-580\n",
      "Element length below threshold. Skipping. [168 170]\n",
      "Element length below threshold. Skipping. [186 188]\n",
      "Element length below threshold. Skipping. [240 242]\n",
      "__________\n",
      "HELIX  #1: S379 @ SSE residue 13 | q = 4989 with res_idx 15 | MOST CONSERVED: b'S' | PDB-res 366-381\n",
      "HELIX  #2: I390 @ SSE residue 5 | q = 3491 with res_idx 26 | MOST CONSERVED: b'V' | PDB-res 385-393\n",
      "HELIX  #3: S420 @ SSE residue 9 | q = 9044 with res_idx 56 | MOST CONSERVED: b'L' | PDB-res 411-427\n",
      "HELIX  #4: D435 @ SSE residue 1 | q = 8894 with res_idx 71 | MOST CONSERVED: b'D' | PDB-res 434-448\n",
      "HELIX  #5: I480 @ SSE residue 19 | q = 9197 with res_idx 116 | MOST CONSERVED: b'L' | PDB-res 461-480\n",
      "HELIX  #6: W488 @ SSE residue 5 | q = 8039 with res_idx 124 | MOST CONSERVED: b'W' | PDB-res 483-491\n",
      "HELIX  #7: D506 @ SSE residue 7 | q = 9373 with res_idx 142 | MOST CONSERVED: b'E' | PDB-res 499-517\n",
      "Skipping Subdomain A Helix [203 211]\n",
      "Element length below threshold. Skipping. [227 230]\n",
      "Element length below threshold. Skipping. [304 306]\n",
      "__________\n",
      "Element length below threshold. Skipping. [ 7 10]\n",
      "HELIX  #2: I251 @ SSE residue 3 | q = 7291 with res_idx 25 | MOST CONSERVED: b'L' | PDB-res 248-263\n",
      "HELIX  #3: T286 @ SSE residue 10 | q = 7823 with res_idx 60 | MOST CONSERVED: b'L' | PDB-res 276-290\n",
      "HELIX  #4: G300 @ SSE residue 1 | q = 7045 with res_idx 74 | MOST CONSERVED: b'D' | PDB-res 299-319\n",
      "Element length below threshold. Skipping. [136 140]\n",
      "Element length below threshold. Skipping. [159 162]\n",
      "__________\n",
      "HELIX  #1: L526 @ SSE residue 4 | q = 5251 with res_idx 5 | MOST CONSERVED: b'L' | PDB-res 522-535\n",
      "HELIX  #2: L550 @ SSE residue 10 | q = 9245 with res_idx 29 | MOST CONSERVED: b'L' | PDB-res 540-563\n",
      "HELIX  #3: N567 @ SSE residue 1 | q = 8716 with res_idx 46 | MOST CONSERVED: b'D' | PDB-res 566-580\n",
      "HELIX  #4: V601 @ SSE residue 13 | q = 8977 with res_idx 80 | MOST CONSERVED: b'L' | PDB-res 588-601\n",
      "HELIX  #5: W609 @ SSE residue 5 | q = 8089 with res_idx 88 | MOST CONSERVED: b'W' | PDB-res 604-615\n",
      "HELIX  #6: E628 @ SSE residue 8 | q = 9532 with res_idx 107 | MOST CONSERVED: b'E' | PDB-res 620-634\n",
      "Element length below threshold. Skipping. [164 166]\n",
      "Element length below threshold. Skipping. [184 186]\n",
      "__________\n",
      "HELIX  #1: H524 @ SSE residue 9 | q = 4242 with res_idx 14 | MOST CONSERVED: b'Q' | PDB-res 515-525\n",
      "HELIX  #2: D534 @ SSE residue 0 | q = 6285 with res_idx 24 | MOST CONSERVED: b'A' | PDB-res 534-543\n",
      "HELIX  #3: E551 @ SSE residue 1 | q = 8262 with res_idx 41 | MOST CONSERVED: b'D' | PDB-res 550-565\n",
      "HELIX  #4: L582 @ SSE residue 10 | q = 5037 with res_idx 72 | MOST CONSERVED: b'L' | PDB-res 572-583\n",
      "HELIX  #5: M589 @ SSE residue 2 | q = 7613 with res_idx 79 | MOST CONSERVED: b'W' | PDB-res 587-596\n",
      "HELIX  #6: D607 @ SSE residue 8 | q = 8584 with res_idx 97 | MOST CONSERVED: b'E' | PDB-res 599-612\n",
      "Element length below threshold. Skipping. [105 107]\n",
      "Element length below threshold. Skipping. [140 142]\n",
      "Element length below threshold. Skipping. [154 158]\n",
      "Skipping Subdomain A Helix [170 179]\n",
      "__________\n",
      "HELIX  #1: Q494 @ SSE residue 6 | q = 4573 with res_idx 13 | MOST CONSERVED: b'Q' | PDB-res 488-499\n",
      "HELIX  #2: L509 @ SSE residue 6 | q = 9056 with res_idx 28 | MOST CONSERVED: b'L' | PDB-res 503-515\n",
      "HELIX  #3: D521 @ SSE residue 1 | q = 8314 with res_idx 40 | MOST CONSERVED: b'D' | PDB-res 520-543\n",
      "HELIX  #4: L580 @ SSE residue 28 | q = 9130 with res_idx 99 | MOST CONSERVED: b'L' | PDB-res 552-580\n",
      "HELIX  #5: W588 @ SSE residue 5 | q = 8377 with res_idx 107 | MOST CONSERVED: b'W' | PDB-res 583-591\n",
      "HELIX  #6: E608 @ SSE residue 14 | q = 9439 with res_idx 127 | MOST CONSERVED: b'E' | PDB-res 594-617\n",
      "Skipping Subdomain A Helix [183 188]\n",
      "Element length below threshold. Skipping. [206 208]\n",
      "__________\n",
      "HELIX  #1: D528 @ SSE residue 9 | q = 4709 with res_idx 12 | MOST CONSERVED: b'Q' | PDB-res 519-531\n",
      "HELIX  #2: M546 @ SSE residue 9 | q = 8851 with res_idx 30 | MOST CONSERVED: b'L' | PDB-res 537-550\n",
      "HELIX  #3: Q558 @ SSE residue 1 | q = 8634 with res_idx 42 | MOST CONSERVED: b'D' | PDB-res 557-565\n",
      "HELIX  #4: A589 @ SSE residue 13 | q = 7841 with res_idx 73 | MOST CONSERVED: b'L' | PDB-res 576-589\n",
      "HELIX  #5: E607 @ SSE residue 6 | q = 6841 with res_idx 91 | MOST CONSERVED: b'E' | PDB-res 601-614\n",
      "Element length below threshold. Skipping. [131 134]\n",
      "Element length below threshold. Skipping. [166 168]\n",
      "Element length below threshold. Skipping. [238 240]\n",
      "__________\n",
      "HELIX  #1: N388 @ SSE residue 13 | q = 4655 with res_idx 15 | MOST CONSERVED: b'S' | PDB-res 375-390\n",
      "HELIX  #2: Q400 @ SSE residue 6 | q = 4696 with res_idx 27 | MOST CONSERVED: b'L' | PDB-res 394-402\n",
      "HELIX  #3: S423 @ SSE residue 9 | q = 9234 with res_idx 50 | MOST CONSERVED: b'L' | PDB-res 414-430\n",
      "HELIX  #4: D438 @ SSE residue 1 | q = 8747 with res_idx 65 | MOST CONSERVED: b'D' | PDB-res 437-451\n",
      "HELIX  #5: I484 @ SSE residue 13 | q = 9405 with res_idx 111 | MOST CONSERVED: b'L' | PDB-res 471-484\n",
      "HELIX  #6: W492 @ SSE residue 5 | q = 8400 with res_idx 119 | MOST CONSERVED: b'W' | PDB-res 487-495\n",
      "HELIX  #7: V505 @ SSE residue 2 | q = 9474 with res_idx 132 | MOST CONSERVED: b'L' | PDB-res 503-524\n",
      "Skipping Subdomain A Helix [198 206]\n",
      "Element length below threshold. Skipping. [223 225]\n",
      "Element length below threshold. Skipping. [285 287]\n",
      "__________\n",
      "{'sdb': {'S1': 328, 'S2': 333, 'S3': 350, 'S4': 359, 'S5': 381, 'S6': 409, 'S7': 414, 'S8': 436, 'S9': 453, 'S10': 470, 'S11': 478, 'S12': 487}, 'A': {'H1': 416, 'H2': 439, 'H3': 454, 'H4': 489, 'H5': 496, 'H6': 514}, 'B': {'H1': 531, 'H2': 556, 'H3': 569, 'H4': 607, 'H5': 615, 'H6': 628}, 'C': {'H1': 463, 'H2': 484, 'H3': 498, 'H4': 524, 'H5': 541, 'H6': 549, 'H7': 567}, 'D': {'H1': 379, 'H2': 390, 'H3': 420, 'H4': 435, 'H5': 480, 'H6': 488, 'H7': 506}, 'E': {'H1': 251, 'H2': 286, 'H3': 300}, 'F': {'H1': 526, 'H2': 550, 'H3': 567, 'H4': 601, 'H5': 609, 'H6': 628}, 'G': {'H1': 524, 'H2': 534, 'H3': 551, 'H4': 582, 'H5': 589, 'H6': 607}, 'L': {'H1': 494, 'H2': 509, 'H3': 521, 'H4': 580, 'H5': 588, 'H6': 608}, 'V': {'H1': 528, 'H2': 546, 'H3': 558, 'H4': 589, 'H5': 607}, 'X': {'H1': 388, 'H2': 400, 'H3': 423, 'H4': 438, 'H5': 484, 'H6': 492, 'H7': 505}}\n",
      "{'sdb': {'S1': 102, 'S2': 107, 'S3': 124, 'S4': 133, 'S5': 155, 'S6': 183, 'S7': 188, 'S8': 210, 'S9': 227, 'S10': 244, 'S11': 252, 'S12': 261}, 'A': {'H1': 5, 'H2': 28, 'H3': 43, 'H4': 78, 'H5': 85, 'H6': 103}, 'B': {'H1': 6, 'H2': 31, 'H3': 44, 'H4': 82, 'H5': 90, 'H6': 103}, 'C': {'H1': 15, 'H2': 36, 'H3': 50, 'H4': 76, 'H5': 93, 'H6': 101, 'H7': 119}, 'D': {'H1': 15, 'H2': 26, 'H3': 56, 'H4': 71, 'H5': 116, 'H6': 124, 'H7': 142}, 'E': {'H1': 25, 'H2': 60, 'H3': 74}, 'F': {'H1': 5, 'H2': 29, 'H3': 46, 'H4': 80, 'H5': 88, 'H6': 107}, 'G': {'H1': 14, 'H2': 24, 'H3': 41, 'H4': 72, 'H5': 79, 'H6': 97}, 'L': {'H1': 13, 'H2': 28, 'H3': 40, 'H4': 99, 'H5': 107, 'H6': 127}, 'V': {'H1': 12, 'H2': 30, 'H3': 42, 'H4': 73, 'H5': 91}, 'X': {'H1': 15, 'H2': 27, 'H3': 50, 'H4': 65, 'H5': 111, 'H6': 119, 'H7': 132}}\n",
      "{'H1': 0, 'H2': 1, 'H3': 2, 'H4': 3, 'H5': 4, 'H6': 5, 'H7': 6} 7\n",
      "(7, 10)\n",
      "{'H1': 4775, 'H2': 9299, 'H3': 8616, 'H4': 9151, 'H5': 7365, 'H6': 9618}\n",
      "H1\n",
      "H2\n",
      "H3\n",
      "H4\n",
      "H5\n",
      "H6\n",
      "{'H1': 4766, 'H2': 9845, 'H3': 8707, 'H4': 9308, 'H5': 8581, 'H6': 8671}\n",
      "H1\n",
      "H2\n",
      "H3\n",
      "H4\n",
      "H5\n",
      "H6\n",
      "{'H1': 3808, 'H2': 9606, 'H3': 8508, 'H4': 2108, 'H5': 9198, 'H6': 5233, 'H7': 9236}\n",
      "H1\n",
      "H2\n",
      "H3\n",
      "H4\n",
      "H5\n",
      "H6\n",
      "H7\n",
      "{'H1': 4989, 'H2': 3491, 'H3': 9044, 'H4': 8894, 'H5': 9197, 'H6': 8039, 'H7': 9373}\n",
      "H1\n",
      "H2\n",
      "H3\n",
      "H4\n",
      "H5\n",
      "H6\n",
      "H7\n",
      "{'H1': 7291, 'H2': 7823, 'H3': 7045}\n",
      "H1\n",
      "H2\n",
      "H3\n",
      "{'H1': 5251, 'H2': 9245, 'H3': 8716, 'H4': 8977, 'H5': 8089, 'H6': 9532}\n",
      "H1\n",
      "H2\n",
      "H3\n",
      "H4\n",
      "H5\n",
      "H6\n",
      "{'H1': 4242, 'H2': 6285, 'H3': 8262, 'H4': 5037, 'H5': 7613, 'H6': 8584}\n",
      "H1\n",
      "H2\n",
      "H3\n",
      "H4\n",
      "H5\n",
      "H6\n",
      "{'H1': 4573, 'H2': 9056, 'H3': 8314, 'H4': 9130, 'H5': 8377, 'H6': 9439}\n",
      "H1\n",
      "H2\n",
      "H3\n",
      "H4\n",
      "H5\n",
      "H6\n",
      "{'H1': 4709, 'H2': 8851, 'H3': 8634, 'H4': 7841, 'H5': 6841}\n",
      "H1\n",
      "H2\n",
      "H3\n",
      "H4\n",
      "H5\n",
      "{'H1': 4655, 'H2': 4696, 'H3': 9234, 'H4': 8747, 'H5': 9405, 'H6': 8400, 'H7': 9474}\n",
      "H1\n",
      "H2\n",
      "H3\n",
      "H4\n",
      "H5\n",
      "H6\n",
      "H7\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'schwobbel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hildilab/projects/agpcr_nom/repo/template_testing.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hildilab/projects/agpcr_nom/repo/template_testing.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         \u001b[39mprint\u001b[39m(v)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hildilab/projects/agpcr_nom/repo/template_testing.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         hasAnchor[anchor_col[v], fam_count] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hildilab/projects/agpcr_nom/repo/template_testing.ipynb#X10sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m schwobbel\n",
      "\u001b[0;31mNameError\u001b[0m: name 'schwobbel' is not defined"
     ]
    }
   ],
   "source": [
    "templates = {'sdb':['A0A3P8S994-A0A3P8S994_AMPPE-AGRE5b,duplicate2-Amphiprion_percula', 'b', '../A0A3P8S994_sdb'],\n",
    "                    'A': ['A0A2Y9F628-A0A2Y9F628_PHYMC-AGRA3isoformX1-Physeter_macrocephalus', 'a', '../A0A2Y9F628_A_sda'], \n",
    "                    'B': ['A0A4W6DVA0-A0A4W6DVA0_LATCA-AGRB1b-Lates_calcarifer', 'a', '../A0A4W6DVA0_B_sda'], \n",
    "                    'C': ['A0A7K6E127-A0A7K6E127_9PASS-CELR3protein-Grantiella_picta.', 'a', '../A0A7K6E127_C_sda'], \n",
    "                    'D': ['A0A1A7WJQ6-A0A1A7WJQ6_9TELE-GR144-Iconisemion_striatum.', 'a', '../A0A1A7WJQ6_D_sda'], \n",
    "                    'E': ['A0A3P8S994-A0A3P8S994_AMPPE-AGRE5b,duplicate2-Amphiprion_percula', 'a', '../A0A3P8S994_E_sda'], \n",
    "                    'F': ['A0A452IH20-A0A452IH20_9SAUR-AGRF5-Gopherus_agassizii', 'a', '../A0A452IH20_F_sda'], \n",
    "                    'G': ['A0A1W4WJB1-A0A1W4WJB1_AGRPL-AGRG6-likeisoformX1-Agrilus_planipennis', 'a', '../A0A1W4WJB1_G_sda'], \n",
    "                    'L': ['A0A452HCU9-A0A452HCU9_9SAUR-AGRL3-Gopherus_agassizii', 'a', '../A0A452HCU9_L_sda'], \n",
    "                    'V': ['A0A6Q2XYK2-A0A6Q2XYK2_ESOLU-AGRV1-Esox_lucius', 'a', '../A0A6Q2XYK2_V_sda'],\n",
    "                    'X': [\"A0A6F9A857-A0A6F9A857_9TELE-Uncharacterizedprotein-Coregonus_sp._'balchen'.\", 'a', '../A0A6F9A857_X_sda']}\n",
    "template_anchors = {}\n",
    "template_quality = {}\n",
    "template_indices = []\n",
    "\n",
    "all_indices = []\n",
    "for k, v in templates.items():\n",
    "    if k == 'sdb':\n",
    "        threshold = 2\n",
    "    else:\n",
    "        threshold = 4\n",
    "    \n",
    "    template_gain = get_gain(v[0].split(\"-\")[0], valid_collection.collection)\n",
    "    #raw_anchors, a_qual, indices, anchors = get_template_information(v[0].split(\"-\")[0], valid_collection, v[1], threshold=threshold)\n",
    "    structural_alignment = tf.construct_structural_alignment(template_gain_domain=template_gain,\n",
    "                                                             list_of_gain_obj=valid_collection.collection,\n",
    "                                                             gain_indices=range(len(valid_collection.collection)),\n",
    "                                                             gesamt_folder=v[2],\n",
    "                                                             infile=f'../{v[0].split(\"-\")[0]}_{k}.struc_aln.fa'\n",
    "                                                             )\n",
    "    #print(structural_alignment)\n",
    "    a_qual, indices, anchors = get_struc_aln_anchors(gain=template_gain,\n",
    "                                                                  aln_dict=structural_alignment,\n",
    "                                                                  subdomain=v[1],\n",
    "                                                                  threshold=threshold)\n",
    "    template_anchors[k] = anchors\n",
    "    template_quality[k] = a_qual\n",
    "    if k == 'sdb':\n",
    "        continue\n",
    "    for i in indices:\n",
    "        template_indices.append(i)\n",
    "    all_indices.append(indices)\n",
    "\n",
    "print(template_anchors)\n",
    "print(template_quality)\n",
    "a,b  = np.unique(template_indices, return_counts=True)\n",
    "a_counts = dict(zip(a,b)) \n",
    "anchor_col = {x:i for i,x in enumerate(a)}\n",
    "print(anchor_col, len(a))\n",
    "hasAnchor = np.zeros(shape=(len(a), len(all_indices)))\n",
    "print(hasAnchor.shape)\n",
    "for fam_count, indices in enumerate(all_indices):\n",
    "    print(indices)\n",
    "    for v in indices:\n",
    "        print(v)\n",
    "        hasAnchor[anchor_col[v], fam_count] = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the pairwise GESAMT, we can use the resulting OUT and PDB files for analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=[4,4], facecolor='w')\n",
    "im = plt.imshow(hasAnchor.T, cmap='gray')\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(range(len(a)))\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_xticklabels(a, rotation=90)\n",
    "ax.set_yticklabels(list(\"ABCDEFGLVX\"))\n",
    "ax.set_xticks(np.arange(-.5, 11, 1), minor=True)\n",
    "ax.set_yticks(np.arange(-.5, 10, 1), minor=True)\n",
    "plt.xlabel(\"Helix Anchor column\")\n",
    "plt.ylabel(\"Subfamily SDA Template\")\n",
    "ax.grid(which='minor', linewidth=2)\n",
    "plt.savefig(\"../sda_template_anchors.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"UNUSED CODE FROM BEFORE TEMPLATE_FINDER.py\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def gain_set_to_template(list_of_gains, index_list, anchors, anchor_occupation, anchor_dict, mode='template', penalty=None, n_anch=23, print_matches=False):\n",
    "    distances = np.full(shape=(len(list_of_gains), n_anch), fill_value=penalty)\n",
    "    all_sda_anch = []\n",
    "    all_sdb_anch = []\n",
    "\n",
    "    for gain_idx, gain in enumerate(list_of_gains):\n",
    "        gain_distances, template_sda_anchors, template_sdb_anchors = match_to_template(index_list[gain_idx], \n",
    "                                                                                       gain, \n",
    "                                                                                       anchors=anchors, \n",
    "                                                                                       anchor_occupation=anchor_occupation, \n",
    "                                                                                       anchor_dict=anchor_dict,\n",
    "                                                                                       n_anch=n_anch,\n",
    "                                                                                       penalty=penalty, mode=mode)\n",
    "        distances[gain_idx,:] = gain_distances\n",
    "        all_sda_anch.append(template_sda_anchors)\n",
    "        all_sdb_anch.append(template_sdb_anchors)\n",
    "        if print_matches : print(template_sda_anchors, template_sdb_anchors)\n",
    "    return distances, all_sda_anch, all_sdb_anch\n",
    "\n",
    "def match_to_template(gain_idx, gain, anchors, anchor_occupation, anchor_dict, n_anch, penalty=None, mode='anchor-anchor'):\n",
    "    '''\n",
    "    Match a GAIN domain to a template PDB. This function has two modes. \n",
    "    In the 'template' mode, we match the closest residue to the template as the new anchor and output the residues and their distances.\n",
    "    In the 'anchor-anchor' mode, we also calculate anchor-anchor distances and output them.\n",
    "\n",
    "    gain_idx indicates the index of the domain used for locating GESAMT alignment files.\n",
    "    template PDBs are global, but not needed since we are handling only the aligned files / PDBs.\n",
    "\n",
    "    -GLOBALS\n",
    "       sda_anchors, sdb_anchors, sda_a_coords, sdb_a_coords'''\n",
    "    if gain.name.endswith('.'): # Adjust the name for finding the GESAMT aligned PDBs.\n",
    "\n",
    "        name = gain.name[:-1]\n",
    "    else:\n",
    "        name = gain.name\n",
    "    name = name.replace('(','-').replace(')','-').replace('\\'','')\n",
    "    \n",
    "    a_gesamt_file = f'../sda_template_aligned_files/sda_{gain_idx}.out'\n",
    "    b_gesamt_file = f'../sdb_template_aligned_files/sdb_{gain_idx}.out'\n",
    "\n",
    "    #a_matches = find_anchor_matches(a_gesamt_file, sda_gain_anchors, isTarget=True)\n",
    "    #b_matches = find_anchor_matches(b_gesamt_file, sdb_gain_anchors, isTarget=True)\n",
    "    sda_actual_anchors = find_anchor_matches(a_gesamt_file, sda_anchors)\n",
    "    sdb_actual_anchors = find_anchor_matches(b_gesamt_file, sdb_anchors)\n",
    "    #print('SDA MATCHES (mobile --> template_res):', matches, '\\n\\nACTUAL ANCHORS (template --> mobile_res)', actual_anchors)\n",
    "    #print('SDB MATCHES (mobile --> template_res):', matches, '\\n\\nACTUAL ANCHORS (template --> mobile_res)', actual_anchors)\n",
    "\n",
    "    if mode == 'anchor-anchor':\n",
    "        # Get the precalculated anchors from the GainDomain object, split into helices and strand dictionaries.\n",
    "        _, gain_anchor_dict, _, _ = gain.create_indexing(anchors, anchor_occupation, anchor_dict,split_mode='double', silent=True)\n",
    "        gain_anchors =  {k[:-3]:v+gain.start for k,v in gain_anchor_dict.items()} #modify X#.50 --> X#\n",
    "        sda_gain_anchors = {k:v for k,v in gain_anchors.items() if \"H\" in k and v < gain.subdomain_boundary}\n",
    "        sdb_gain_anchors = {k:v for k,v in gain_anchors.items() if \"S\" in k and v > gain.subdomain_boundary}\n",
    "        # Here, the pre-defined anchor-anchor distances will be calculated.\n",
    "        _, a_dist = calculate_anchor_distances(sda_a_coords, f'../sda_template_aligned_pdbs/{name}_sda.pdb', sda_gain_anchors)\n",
    "        _, b_dist = calculate_anchor_distances(sdb_a_coords, f'../sdb_template_aligned_pdbs/{name}_sdb.pdb', sdb_gain_anchors)\n",
    "        all_dist = {**a_dist, **b_dist}\n",
    "\n",
    "    elif mode == 'template': \n",
    "        all_dist = {k:v[1] for k,v in {**sda_actual_anchors, **sdb_actual_anchors}.items() } #{'H1': (653, 1.04) , ...}\n",
    "    # From the anchor-residue or anchor-anchor distances, construct a distance matrix where unmatched entried will be assigned a penalty value.\n",
    "    else: \n",
    "        print('Unknown mode. Just returning matched anchors.')\n",
    "        return None, sda_actual_anchors, sdb_actual_anchors\n",
    "    # Fill a matrix with the individual distances, assign unmatched anchors a pre-set penalty value\n",
    "    distances = np.full(shape=(n_anch), fill_value=penalty)\n",
    "    for i, sse in enumerate(anchor_dict.values()):\n",
    "        #print(i, sse)\n",
    "        if sse in all_dist.keys():\n",
    "            if all_dist[sse] is not None:\n",
    "                distances[i] = all_dist[sse]\n",
    "            #print(all_dist[sse])\n",
    "            for i,val in enumerate(distances):\n",
    "                if penalty is not None and val > penalty: \n",
    "                    distances[i] = penalty\n",
    "\n",
    "    return distances, sda_actual_anchors, sdb_actual_anchors\n",
    "\n",
    "def plot_pca(distance_matrix, cluster_intervals, n_components, name, plot3D=False, save=True):\n",
    "    colorlist = ['blue','red','green','yellow','orange','purple','forestgreen','limegreen','firebrick']\n",
    "    #X = anchor_distance_matrix\n",
    "    X = distance_matrix\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X)\n",
    "    print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "    #print(pca.singular_values_)\n",
    "    X_r = pca.fit(X).transform(X)\n",
    "    print(X_r.shape)\n",
    "\n",
    "    fig = plt.figure(figsize=[5,5])\n",
    "    fig.set_facecolor('w')\n",
    "    if plot3D:\n",
    "        ax = ax = fig.add_subplot(projection='3d')\n",
    "        for i, interval in enumerate(cluster_intervals):\n",
    "            ax.scatter(X_r[interval[0]:interval[1],0], X_r[interval[0]:interval[1],1], X_r[interval[0]:interval[1],2], marker='o', s=8, c=colorlist[i])\n",
    "    else:\n",
    "        ax = fig.add_subplot()\n",
    "        for i, interval in enumerate(cluster_intervals):\n",
    "            ax.scatter(X_r[interval[0]:interval[1],0], X_r[interval[0]:interval[1],1], marker='o', s=8, c=colorlist[i])\n",
    "    #cbar = plt.colorbar()\n",
    "    #plt.figure()\n",
    "    #plt.imshow(anchor_distance_matrix[::30])\n",
    "    #print(np.max(anchor_distance_matrix))\n",
    "    #plt.colorbar()\n",
    "    ax.set_title(f'PCA of MiniBatchKMeans - {name}')\n",
    "    ax.set_xlabel('PC 0')\n",
    "    ax.set_ylabel('PC 1')\n",
    "    if plot3D:\n",
    "        ax.set_zlabel('PC 2')\n",
    "    if save:\n",
    "        plt.savefig(f'{name}_pca.png', dpi=300)\n",
    "        \n",
    "def cluster_k_means(matrix, list_of_gain_obj, n_cluster=9):\n",
    "    struc_list = [gain.name for gain in list_of_gain_obj]\n",
    "    clust = MiniBatchKMeans(n_clusters=n_cluster,\n",
    "                            random_state=0,\n",
    "                            batch_size=6,\n",
    "                            max_iter=10,\n",
    "                            n_init=\"auto\").fit(matrix)\n",
    "    #clust = AgglomerativeClustering(n_clusters=n_cluster, metric='euclidean', \n",
    "    #                        memory=None, connectivity=None, compute_full_tree='auto', linkage='complete', distance_threshold=None, compute_distances=True).fit(anchor_distance_matrix)\n",
    "    clustering=clust.labels_\n",
    "    #print(np.unique(clustering, return_counts=True))\n",
    "    n_struc, n_distances = matrix.shape # 14432, 21\n",
    "    #print(clustering.labels_)\n",
    "    new_order = np.zeros(shape=(n_struc), dtype=int)\n",
    "    current_num = 0\n",
    "    for i in range(n_cluster):\n",
    "        for j, cluster_id in enumerate(clustering):\n",
    "            if cluster_id == i :\n",
    "                new_order[j] = current_num\n",
    "                current_num += 1\n",
    "    #print(new_order)\n",
    "    remap_dict = {old_idx:new_idx for old_idx, new_idx in enumerate(new_order)}\n",
    "    inv_remap_dict = {v:k for k,v in remap_dict.items()}\n",
    "    reordered_matrix = np.zeros(shape=(n_struc,n_distances))\n",
    "    for x in range(n_struc):\n",
    "        new_x = remap_dict[x]\n",
    "        reordered_matrix[new_x,:] = matrix[x,:]\n",
    "\n",
    "    #fig = plt.figure(figsize=[20,1])\n",
    "    #fig.set_facecolor('w')\n",
    "    #plt.imshow(ordered_distances.transpose(), cmap='Greys')\n",
    "    #cbar = plt.colorbar()\n",
    "    #cbar.set_label('RMSD [$\\AA$]')\n",
    "    #plt.savefig('../test_largedist.png',dpi=300)\n",
    "\n",
    "    reordered_names = [struc_list[inv_remap_dict[k]] for k in range(n_struc)]\n",
    "    reordered_clusters = [clustering[inv_remap_dict[k]] for k in range(n_struc)]\n",
    "\n",
    "    _, cluster_starts = np.unique(reordered_clusters, return_index = True)\n",
    "    cluster_intervals = [(cluster_starts[k], cluster_starts[k+1]) for k in range(n_cluster-1)]\n",
    "    cluster_intervals.append((cluster_starts[-1], n_struc))\n",
    "    #print(cluster_intervals)\n",
    "\n",
    "    return reordered_matrix, cluster_intervals, reordered_names\n",
    "\n",
    "def read_gesamt_pairs(gesamtfile):\n",
    "    with open(gesamtfile) as f:\n",
    "        pair_lines = [line for line in f.readlines() if line.startswith(\"|\")][2:]\n",
    "        #|H- A:LEU 720 | <**1.21**> |H- A:LEU 633 |\n",
    "        #| - A:ALA 721 | <..1.54..> | + A:GLN 634 |\n",
    "        #| + A:GLU 722 | <..1.75..> | + A:PRO 635 |\n",
    "        #| + A:GLU 723 | <==2.03==> | + A:GLN 636 |\n",
    "        #| + A:ASN 724 | <..1.91..> | - A:ALA 637 |\n",
    "        #|H+ A:ARG 725 | <..2.08..> |H- A:LEU 638 |\n",
    "    # construct a data structure with indices of both sides (fixed, mobile)\n",
    "    mobile_pairs = {}\n",
    "    template_pairs = {}\n",
    "    for pair in pair_lines:\n",
    "        template_str, distance_str, mobile_str = pair[9:13].strip(), pair[19:23].strip(), pair[36:40].strip()\n",
    "        #print(template_str, distance_str, mobile_str, \"\\n\", pair)\n",
    "        # If either residue is empty, let the pair point to None\n",
    "        if len(template_str) == 0:\n",
    "            mobile_pairs[int(mobile_str)] = (None, None)\n",
    "            continue\n",
    "        if len(mobile_str) == 0:\n",
    "            template_pairs[int(template_str)] = (None, None)\n",
    "            continue\n",
    "        if len(distance_str) == 0:\n",
    "            dist = None\n",
    "        else:\n",
    "            dist = float(distance_str)\n",
    "        template_pairs[int(template_str)] = (int(mobile_str), dist)\n",
    "        mobile_pairs[int(mobile_str)] = (int(template_str), dist)\n",
    "    \n",
    "    return template_pairs, mobile_pairs\n",
    "\n",
    "def find_anchor_matches(file, anchor_dict, isTarget=False):\n",
    "    # Takes a gesamt file and an anchor dictionary either of the target or the template and returns the matched other residue with the pairwise distance\n",
    "    # as a dictionary: {'H1': (653, 1.04) , ...}\n",
    "    template_pairs, mobile_pairs = read_gesamt_pairs(file)\n",
    "    # Find the closest residue to template anchors\n",
    "    matched_residues = {}\n",
    "    if not isTarget:\n",
    "        parsing_dict = template_pairs\n",
    "    else:\n",
    "        parsing_dict = mobile_pairs\n",
    "\n",
    "    start, end = min(parsing_dict.keys()), max(parsing_dict.keys())\n",
    "    for anchor_name, anchor_res in anchor_dict.items():\n",
    "        # If the anchor lies outside the aligned segments, pass empty match (None, None)\n",
    "        if anchor_res < start or anchor_res > end:\n",
    "            matched_residues[anchor_name] = (None, None)\n",
    "            continue\n",
    "        matched_residues[anchor_name] = parsing_dict[anchor_res]\n",
    "\n",
    "    return matched_residues\n",
    "\n",
    "def get_anchor_coords(pdbfile, anchor_dict, multistate=False):\n",
    "    # Find the CA coordinates of the anchor residue in the template PDB, return dictionary with the coords for each labeled anchor\n",
    "    with open(pdbfile) as p:\n",
    "        if multistate:\n",
    "            data = p.read().split(\"ENDMDL\")[1]\n",
    "        if not multistate:\n",
    "            data = p.read()\n",
    "        mdl2data = [l for l in data.split(\"\\n\") if l.startswith(\"ATOM\")]\n",
    "        ca_data = [atom for atom in mdl2data if \" CA \" in atom]\n",
    "        #print(anchor_dict, pdbfile)\n",
    "    # Find the PDB coords of the CA of each atom and get them as a dict with  {'myanchor':(x,y,z), ...}\n",
    "    res_ca_dict = {int(line[22:26]):(float(line[30:38]), float(line[38:46]), float(line[46:54])) for line in ca_data}\n",
    "    coord_dict={}\n",
    "    for anchor_name, anchor_res in anchor_dict.items():\n",
    "        if anchor_res in res_ca_dict.keys():\n",
    "            coord_dict[anchor_name]=res_ca_dict[anchor_res]\n",
    "        else:\n",
    "            \"FALLBACK. Eliminiating Residue Anchor.\"\n",
    "            coord_dict[anchor_name]=(1000.0, 1000.0, 1000.0)\n",
    "    #coord_dict = {anchor_name:res_ca_dict[anchor_res] for anchor_name, anchor_res in anchor_dict.items()}\n",
    "    \n",
    "    return coord_dict\n",
    "\n",
    "def space_distance(coords1, coords2):\n",
    "    # Pythagorean distance of two sets of coords\n",
    "    return round(math.sqrt(abs(coords1[0]-coords2[0])**2 + abs(coords1[1]-coords2[1])**2 + abs(coords1[2]-coords2[2])**2 ),3)\n",
    "\n",
    "def calculate_anchor_distances(template_anchor_coords, mobile_pdb, mobile_anchors, threshold=10):\n",
    "    # template_anchor_coords are a precalculated set of coords to save calc time.\n",
    "    # Matches always the closest Helix and sheet anchors, respectively, regardless of label.\n",
    "    mobile_anchor_coords = get_anchor_coords(mobile_pdb, mobile_anchors, multistate=True)\n",
    "    #distances = pd.DataFrame()\n",
    "    t_anchor_names = list(template_anchor_coords.keys())\n",
    "    t_coord = list(template_anchor_coords.values())\n",
    "    #print(t_coord)\n",
    "    anchor_occupied = np.zeros(shape=(len(template_anchor_coords.keys())), dtype=bool)\n",
    "    min_dists = []\n",
    "    matched_anchors = []\n",
    "    double_keys = []\n",
    "\n",
    "    mob_coords = list(mobile_anchor_coords.values())\n",
    "    n_template_keys = len(list(template_anchor_coords.keys()))\n",
    "\n",
    "    for m_coord in mob_coords:\n",
    "        distances = [space_distance(m_coord, coords) for coords in t_coord]\n",
    "        min_idx = np.argmin(distances)\n",
    "        if anchor_occupied[min_idx] == True:\n",
    "            double_keys.append(t_anchor_names[min_idx])\n",
    "        anchor_occupied[min_idx] = True\n",
    "        matched_anchors.append(t_anchor_names[min_idx])\n",
    "        min_dists.append(distances[min_idx])\n",
    "\n",
    "    if len(double_keys) == 0:\n",
    "        return dict(zip(mobile_anchors.keys(), matched_anchors)), dict(zip(mobile_anchors.keys(), min_dists))\n",
    "\n",
    "    # If anchors are present multiple times, delete the further distant entry and make None, None\n",
    "    for doublet in double_keys:\n",
    "        #print(f'{doublet = }, {matched_anchors = }, {type(matched_anchors) = }')\n",
    "        indices = [i for i,x in enumerate(matched_anchors) if doublet == x]\n",
    "        minimum = np.argmin(np.array(min_dists)[indices])\n",
    "        indices.remove(indices[minimum])\n",
    "        # Only the non-minimum values will remain in the indices list that need to be re-evaluated\n",
    "        newdists = np.empty(shape=(n_template_keys))\n",
    "        for idx in indices:\n",
    "\n",
    "            for i, coord in enumerate(t_coord):\n",
    "                if anchor_occupied[i] == True:\n",
    "                    newdists[i] = None\n",
    "                    continue\n",
    "                newdists[i] = space_distance(list(mobile_anchor_coords.values())[idx], coord)\n",
    "            newmindist = np.min(newdists)\n",
    "            if newmindist < threshold:\n",
    "                print('Found Alternative')\n",
    "                anchor_occupied[np.argmin(newdists)] = True\n",
    "                min_dists[idx] = newmindist\n",
    "                matched_anchors[idx] = list(mobile_anchor_coords.keys())[idx]\n",
    "            else: \n",
    "                #print('Removed double occurrence')\n",
    "                min_dists[idx] = None\n",
    "                matched_anchors[idx] = ''\n",
    "\n",
    "    return dict(zip(mobile_anchors.keys(), matched_anchors)), dict(zip(mobile_anchors.keys(), min_dists))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_family(name):\n",
    "    queries = [('AGR..', name, lambda x: x[-1][-2:]), #\n",
    "                ('ADGR..', name, lambda x: x[-1][-2:]), \n",
    "                ('cadher.*receptor.', name.lower(), lambda x: f\"C{x[-1][-1]}\"),\n",
    "                ('cels?r.', name.lower(), lambda x: f\"C{x[-1][-1]}\"), \n",
    "                ('latrophilin.*protein-?\\d', name.lower(), lambda x: f\"L{x[-1][-1]}\"),\n",
    "                ('latrophilin-?\\d', name.lower(), lambda x: f\"L{x[-1][-1]}\"),\n",
    "                ('GP?R133', name.upper(),lambda x: 'D1'),\n",
    "                ('GP?R126', name.upper(),lambda x: 'G6'),\n",
    "                ('GP?R?124', name.upper(),lambda x: 'A2'),\n",
    "                ('GP?R?125', name.upper(),lambda x: 'A3'),\n",
    "                ('GP?R112', name.upper(),lambda x: 'G4'),\n",
    "                ('GP?R116', name.upper(),lambda x: 'F5'),\n",
    "                ('GP?R144', name.upper(),lambda x: 'D2'),\n",
    "                ('ag-?.*-?coupled-?receptor-?.-?\\d', name.lower(),lambda x: x[-1].replace('-','')[-2:].upper()),\n",
    "                ('brain-?specific-?angiogenesis-?inhibitor-?\\d', name.lower(), lambda x: f\"B{x[-1][-1]}\"),\n",
    "                ('emr\\d', name.lower(), lambda x: f\"E{x[-1][-1]}\"),\n",
    "                ]\n",
    "    for pattern, searchstring, output in queries:\n",
    "        match = re.findall(pattern, searchstring)\n",
    "        if match != []:\n",
    "            #if output(match) == '': print(name)\n",
    "            return output(match)\n",
    "    return 'X'\n",
    "\n",
    "fam_list = [get_family(gain.name) for gain in valid_collection.collection]\n",
    "name_list = [gain.name for gain in valid_collection.collection]\n",
    "subfam_list = [x[0] for x in fam_list]\n",
    "receptors, counts  = np.unique(fam_list, return_counts=True)\n",
    "r_list = list(zip(receptors,counts))\n",
    "print(r_list)\n",
    "print(receptors)\n",
    "fam_counts = {}\n",
    "for prot in fam_list:\n",
    "    fam = prot[0]\n",
    "    if fam not in fam_counts.keys():\n",
    "        fam_counts[fam] = 0\n",
    "    fam_counts[fam] += 1\n",
    "\n",
    "print(fam_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match everything for each subfamily.\n",
    "#print(subfam_list)\n",
    "\n",
    "y = len(r_list)\n",
    "# famstring\n",
    "template_ids = list(templates.keys())\n",
    "\n",
    "t_occupancies = {}\n",
    "t_distances = {}\n",
    "unmatched = {}\n",
    "unmatched_counters = {}\n",
    "for t_id in template_ids[:1]:\n",
    "    #t_anchors = template_anchors[t_id]\n",
    "    #t_anchors = {'S1': 324, 'S2': 335, 'S3': 353, 'S4': 359, 'S5': 381, 'S6': 409, 'S7': 414, 'S8': 436, 'S9': 453, 'S10': 459, 'S11': 470, 'S12': 478, 'S13': 487}\n",
    "    t_anchors = {'S1': 324, 'S2': 335, 'S3': 349, 'S4': 359, 'S5': 381, 'S6': 409, 'S7': 414, 'S8': 436, 'S9': 453, 'S10': 459, 'S11': 466, 'S12': 478, 'S13': 487}\n",
    "    t_quality = template_quality[t_id]\n",
    "    t_folder = templates[t_id][-1]\n",
    "    n_anch = len(t_anchors.keys())\n",
    "    u_list = np.zeros(shape=(y), dtype=dict)\n",
    "    u_counters = np.zeros(shape=(y), dtype=int)\n",
    "    print(t_anchors)\n",
    "    anchor_index = {k:i for i, k in enumerate(t_anchors.keys())}\n",
    "    assigned_anchor_freq = np.zeros(shape=(len(receptors),n_anch))\n",
    "    all_anchor_averages = np.full(shape=(y,n_anch), fill_value=None)\n",
    "    all_anchor_occupancy = np.zeros(shape=(y,n_anch))\n",
    "\n",
    "    for fam_idx, r in enumerate(receptors):# in enumerate('ABCDEFGLVX'):\n",
    "        gain_subset = [ gain for i, gain in enumerate(valid_collection.collection) if fam_list[i] == r ]#subfam_list[i]==r ]\n",
    "        gain_idx_list = [ i for i,gain in enumerate(fam_list) if gain == r ]\n",
    "        #print(r, len(gain_subset))\n",
    "\n",
    "        element_occupation = {k:0 for k in t_anchors.keys()}\n",
    "\n",
    "        for key, val in element_occupation.items():\n",
    "            assigned_anchor_freq[fam_idx, anchor_index[key]] = float(val)/len(gain_subset)\n",
    "        #DEBUG:\n",
    "        #for gain in gain_subset: \n",
    "        #    print(gain.name, [hel for hel in gain.sda_helices if hel[0] < gain.subdomain_boundary-gain.start])\n",
    "        \n",
    "        fam_distances, fam_matched_anchors, unmatched_elements, unmatched_counter = tf.gain_set_to_template(gain_subset, \n",
    "                                                                                                            gain_idx_list, \n",
    "                                                                                                            t_anchors, \n",
    "                                                                                                            t_folder, \n",
    "                                                                                                            penalty=None,\n",
    "                                                                                                            subdomain='sdb',\n",
    "                                                                                                            return_unmatched_mode='all', \n",
    "                                                                                                            debug=False)\n",
    "        #print(\"DEBUG: ROOT\" , unmatched_elements)\n",
    "        mean_dist = np.empty(shape=(n_anch))\n",
    "        occ = np.zeros(shape=(n_anch))\n",
    "        \n",
    "        for j in range(n_anch):\n",
    "            occ_values = np.array([d for d in fam_distances[:,j] if d is not None])\n",
    "            if len(occ_values) != 0:\n",
    "                mean_dist[j] = round(np.mean(occ_values), 3)\n",
    "                occ[j] = round(np.count_nonzero(fam_distances[:,j])/len(gain_idx_list), 3)\n",
    "        all_anchor_averages[fam_idx,:] = mean_dist #np.mean(fam_distances, axis=0)\n",
    "        all_anchor_occupancy[fam_idx,:] = occ\n",
    "        u_counters[fam_idx] = unmatched_counter\n",
    "        u_list[fam_idx] = unmatched_elements\n",
    "        #print(all_anchor_averages)\n",
    "        #print(all_anchor_occupancy)\n",
    "    print(u_list.shape) # u_list is a list of dicts.\n",
    "    print(type(u_list[0]), type(u_list[1]))\n",
    "    print(f\"Done with Template {t_id}.\\n\", \"_\"*30)\n",
    "\n",
    "    t_distances[t_id] = all_anchor_averages\n",
    "    t_occupancies[t_id] = all_anchor_occupancy\n",
    "    unmatched[t_id] = u_list\n",
    "    unmatched_counters[t_id] = u_counters\n",
    "#print(unmatched_counters)\n",
    "#print(type(unmatched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fit.\n",
    "for t_id in template_ids:\n",
    "    #t_anchors = template_anchors[t_id]\n",
    "    #t_anchors = {'S1': 324, 'S2': 335, 'S3': 353, 'S4': 359, 'S5': 381, 'S6': 409, 'S7': 414, 'S8': 436, 'S9': 453, 'S10': 459, 'S11': 470, 'S12': 478, 'S13': 487}\n",
    "    t_anchors = {'S1': 324, 'S2': 335, 'S3': 349, 'S4': 359, 'S5': 381, 'S6': 409, 'S7': 414, 'S8': 436, 'S9': 453, 'S10': 459, 'S11': 466, 'S12': 478, 'S13': 487}\n",
    "    n_anch = len(t_anchors.keys())\n",
    "    t_anchor_freqs = t_occupancies[t_id]\n",
    "    u_counters = unmatched_counters[t_id]\n",
    "    fig = plt.figure(figsize=[6,10], facecolor='w')\n",
    "    #plt.yticks(ticks = range(ydim), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "    plt.title(f\"Template Match for : {t_id}\")\n",
    "    #plt.yticks(ticks = range(len(r_list)), labels= [f'{i[0]}:{i[1]} (u:{u_counters[x]})' for x,i in enumerate(r_list)])\n",
    "    plt.yticks(ticks = range(len(r_list)), labels= [f'{i[0]}:{i[1]}' for x,i in enumerate(r_list)])\n",
    "    plt.xticks(ticks = range(n_anch), labels=t_anchors.keys(), rotation=90)\n",
    "    #plt.imshow(t_anchor_freqs, cmap='summer')\n",
    "    distances = np.zeros(shape=(len(r_list), n_anch), dtype=float)\n",
    "    t_dists = t_distances[t_id]\n",
    "    for i,l in enumerate(t_dists):\n",
    "        distances[i,:] = l\n",
    "    plt.imshow(distances, cmap='spring')\n",
    "    cbar = plt.colorbar(shrink=0.5)\n",
    "    #cbar.set_label('Relative Occupancy')\n",
    "    cbar.set_label('AA-Distance')\n",
    "    ydim = len(r_list)\n",
    "    for y in range(ydim):\n",
    "        for x in range(n_anch):\n",
    "            if t_anchor_freqs[y,x] > 0.001:\n",
    "                pass\n",
    "            else:\n",
    "                plt.text(x,y,'x', horizontalalignment='center', verticalalignment='center', fontsize=18,color='k')\n",
    "    plt.savefig(f'{t_id}_dist.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"../unmatched_full.txt\", 'w')\n",
    "outfile.write(\"Temp  Grp   nGrp  alnIdx  nNoMat  avgLen  %unmat\\n\")\n",
    "adress_matrix = [] # (my_template, col_names[id], value)\n",
    "col_names = {} # A1-1094: 0\n",
    "\n",
    "\n",
    "#print(r_list, len(r_list))\n",
    "for t_index, t_id in enumerate(template_ids[1:]):\n",
    "    u_list = unmatched[t_id]\n",
    "\n",
    "    #print(u_dict)\n",
    "    for rud_idx, receptor_unmatched_dict in enumerate(u_list):\n",
    "        \n",
    "        e_length = []\n",
    "        e_res = []\n",
    "        res_len = {}\n",
    "        all_items = []\n",
    "        #print(receptor_unmatched_dict)\n",
    "        for lst in receptor_unmatched_dict.values():\n",
    "            lengths = [int(i[2]) for i in lst]\n",
    "            e_length = e_length+lengths\n",
    "            e_res += [i[0] for i in lst]\n",
    "            \n",
    "            for i in lst:\n",
    "                if int(i[0]) not in res_len.keys():\n",
    "                    res_len[int(i[0])] = [i[2]]\n",
    "                else:\n",
    "                    res_len[int(i[0])].append(i[2])\n",
    "                all_items.append(i)\n",
    "        res_av_len = {k:np.average(v) for k,v in res_len.items()}\n",
    "\n",
    "        #print(np.average(e_length))\n",
    "        resid, ct = np.unique(e_res, return_counts=True)\n",
    "        where_many = {resid[k]:c for k,c in enumerate(ct) if c > 5}\n",
    "        #print(where_many)\n",
    "        #plt.bar(resid, ct)\n",
    "        \n",
    "        sel_length = r_list[rud_idx][1]\n",
    "        receptor_name = r_list[rud_idx][0]\n",
    "        for idx, count in enumerate(ct):\n",
    "            if count > 0.1*sel_length and res_av_len[resid[idx]] > 3.5: # more than 10% of selection have this\n",
    "\n",
    "                unindexed_freq = count/sel_length\n",
    "                column_name = f\"{receptor_name}-{str(resid[idx]).ljust(4)}\"\n",
    "                if column_name not in col_names.keys(): \n",
    "                    name_idx = len(col_names.keys())\n",
    "                    col_names[column_name] = name_idx\n",
    "                    \n",
    "                else:\n",
    "                    name_idx = col_names[column_name]\n",
    "                adress_matrix.append( (t_index, name_idx, unindexed_freq) )\n",
    "\n",
    "                outfile.write(f\"{t_id}{receptor_name.rjust(7)}{str(sel_length).rjust(8)}\")\n",
    "                outfile.write(f\"{str(resid[idx]).rjust(8)}{str(count).rjust(8)}{str(round(res_av_len[resid[idx]],1)).rjust(8)}{str(round(count*100/sel_length)).rjust(7)}%   \")\n",
    "                for value in all_items[idx]:\n",
    "                    outfile.write(str(value).rjust(8))#plt.bar(resid[idx], count)\n",
    "                outfile.write(\"\\n\")\n",
    "                #plt.annotate(f\"{round(res_av_len[resid[idx]],1)}\", (resid[idx],count))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can construct a DataFrame from this shizzle\n",
    "unmatched_matrix = np.zeros(shape=(len(template_ids)-1, len(col_names.keys())), dtype=float)\n",
    "for item in adress_matrix:\n",
    "    unmatched_matrix[item[0],item[1]] = item[2]\n",
    "# Sort the matrix to match the receptor order in $receptors\n",
    "sorted_unmatched_matrix = np.zeros(shape=(len(template_ids)-1, len(col_names.keys())), dtype=float)\n",
    "new_order = sorted(range(len(col_names.keys())), key=lambda k: list(col_names.keys())[k])\n",
    "#print(new_order)\n",
    "for data_col in range(len(col_names.keys())):\n",
    "    sorted_unmatched_matrix[:,data_col] = unmatched_matrix[:,new_order[data_col]]\n",
    "\n",
    "fig = plt.figure(figsize=[12,4], facecolor='w')\n",
    "ax = plt.gca()\n",
    "ax.imshow(sorted_unmatched_matrix, cmap='binary')\n",
    "plt.xticks(ticks = range(len(col_names.keys())), labels = sorted(col_names.keys()) ,rotation=90, fontsize=4, verticalalignment='top')\n",
    "plt.yticks(ticks = range(len(template_ids)-1), labels=template_ids[1:], fontsize=4)\n",
    "stored_ki = 'A1'\n",
    "r_bounds = []\n",
    "for i, ki in enumerate(sorted(col_names.keys())):\n",
    "    r = ki.split(\"-\")[0]\n",
    "    if r != stored_ki:\n",
    "        stored_ki = r\n",
    "        r_bounds.append(i)\n",
    "for b in r_bounds:\n",
    "    plt.vlines(b-0.5, -0.5, len(template_ids)-1.5, color='r', linewidth=0.5)\n",
    "plt.savefig(\"../template_match1.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[4,12], facecolor='w')\n",
    "plt.imshow(sorted_unmatched_matrix.T)\n",
    "stored_ki = 'A1'\n",
    "r_bounds = []\n",
    "for i, ki in enumerate(sorted(col_names.keys())):\n",
    "    r = ki.split(\"-\")[0]\n",
    "    if r != stored_ki:\n",
    "        stored_ki = r\n",
    "        r_bounds.append(i)\n",
    "for b in r_bounds:\n",
    "    plt.vlines(b, 0, len(template_ids)-1, color='w', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "for fam_idx, r in enumerate(receptors[:1]):# in enumerate('ABCDEFGLVX'):\n",
    "    gain_subset = [ gain for i, gain in enumerate(valid_collection.collection) if fam_list[i] == r]#subfam_list[i]==r ]\n",
    "    for gain in gain_subset:\n",
    "        id_list.append(gain.name.split(\"-\")[0])\n",
    "    file_str = [(find_pdb(i, '../all_pdbs/')) for i in id_list]\n",
    "print(\"pymol\",\" \".join(file_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ydim = 40\n",
    "fig = plt.figure(figsize=[8,ydim/3], facecolor='w')\n",
    "plt.imshow(docc, cmap='spring')\n",
    "#plt.yticks(ticks = range(ydim), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "plt.yticks(ticks = range(ydim), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "plt.xticks(ticks = range(n_anch), labels= allsse, rotation=90)\n",
    "\n",
    "all_anchor_averages # 10,21\n",
    "for y in range(ydim):\n",
    "    for x in range(n_anch):\n",
    "        if all_anchor_averages[y,x] > 0.0001:\n",
    "            plt.text(x,y, round(all_anchor_averages[y,x], 2), horizontalalignment='center', verticalalignment='center', fontsize=7,color='k', rotation=45)\n",
    "cbar = plt.colorbar(shrink=float(8/ydim))\n",
    "cbar.set_label('Relative Occupancy')\n",
    "plt.vlines(6.5,-0.5,ydim-0.5, color='k', linewidth=1.5)\n",
    "plt.savefig('identity_receptor_anchor_occupancy.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=[8,ydim/3], facecolor='w')\n",
    "#plt.imshow(df, cmap='summer')\n",
    "im_data = np.zeros(shape=(ydim, n_anch))\n",
    "\n",
    "print(all_anchor_averages.shape)\n",
    "#plt.yticks(ticks = range(10), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "plt.yticks(ticks = range(ydim), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "plt.xticks(ticks = range(n_anch), labels= allsse, rotation=90)\n",
    "#all_anchor_averages # 10,21\n",
    "for y in range(ydim):\n",
    "    for x in range(n_anch):\n",
    "        if all_anchor_averages[y,x] > 0.001:\n",
    "            im_data[y,x] = all_anchor_averages[y,x]\n",
    "        else:\n",
    "            plt.text(x,y,'x', horizontalalignment='center', verticalalignment='center', fontsize=20,color='k')\n",
    "            #patches.Rectangle((x,y), 1, 1, linewidth=0.5, edgecolor='k', facecolor='w')\n",
    "plt.imshow(im_data, cmap='summer', vmax=3)\n",
    "            #plt.text(x,y, round(all_anchor_averages[y,x], 2), horizontalalignment='center', verticalalignment='center', fontsize=7,color='k', rotation=45)\n",
    "cbar = plt.colorbar(shrink=float(8)/ydim)\n",
    "cbar.set_label(r'Closest Anchor Residue Distance [$\\AA$]')\n",
    "plt.vlines(6.5,-0.5,ydim-0.5, color='k', linewidth=1.5)\n",
    "plt.savefig('identity_receptor_anchor_distance.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydim = 40\n",
    "fig = plt.figure(figsize=[8,ydim/3], facecolor='w')\n",
    "\n",
    "#plt.yticks(ticks = range(ydim), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "plt.yticks(ticks = range(ydim), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "plt.xticks(ticks = range(n_anch), labels= allsse, rotation=90)\n",
    "occ_values = df.to_numpy()\n",
    "is_off = np.zeros(shape=(40,26))\n",
    "print(docc.shape, all_anchor_averages.shape)\n",
    "#all_anchor_averages # 10,21\n",
    "for y in range(ydim):\n",
    "    for x in range(n_anch):\n",
    "        if all_anchor_averages[y,x] is not None and all_anchor_averages[y,x] > 1.5 and occ_values[y,x] > 0.1:\n",
    "            is_off[y,x] = 1\n",
    "            plt.text(x,y, round(all_anchor_averages[y,x], 2), horizontalalignment='center', verticalalignment='center', fontsize=7,color='k', rotation=45)\n",
    "plt.imshow(is_off, cmap='spring')\n",
    "cbar = plt.colorbar(shrink=float(8/ydim))\n",
    "cbar.set_label('Relative Occupancy')\n",
    "plt.vlines(6.5,-0.5,ydim-0.5, color='k', linewidth=1.5)\n",
    "#plt.savefig('identity_receptor_anchor_occupancy.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_stats = np.zeros(shape = (n_anch, 2))\n",
    "\n",
    "for fam_idx, r in enumerate(receptors):# in enumerate('ABCDEFGLVX'):\n",
    "    print(r)\n",
    "    gain_subset = [ gain for i, gain in enumerate(valid_collection.collection) if fam_list[i] == r]#subfam_list[i]==r ]\n",
    "    gain_idx_list = [ i for i,gain in enumerate(fam_list) if gain == r ]\n",
    "    n_sse = [[len(gain.sda_helices), len(gain.sdb_sheets)] for gain in gain_subset] # (n_struc, 2)\n",
    "    n_strucs = np.mean(np.array(n_sse), axis=0)\n",
    "    print(r, round(n_strucs[0], 2), round(n_strucs[1],2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new Anchor management.\n",
    "\n",
    "#\n",
    "# Use the \"max\" SDA / SDB template for generating the new anchors \n",
    "# get the center residue index for each template SSE\n",
    "for gain in valid_collection.collection:\n",
    "    if gain.name[:10] == 'A0A7K7IHI9': #SDA\n",
    "        hel_centers = []\n",
    "        for hel in gain.sda_helices: # each hel is a tuple\n",
    "            hel_centers.append( gain.start + int((hel[0]+hel[1])/2) )\n",
    "        hel_keys = [f'H{i+1}' for i in range(len(hel_centers))]\n",
    "        sda_centers = dict(zip(hel_keys, hel_centers))\n",
    "    if gain.name[:10] =='A0A3P9I6M5':\n",
    "        sheet_centers = []\n",
    "        for sheet in gain.sdb_sheets: # each hel is a tuple\n",
    "            #if sheet[1] - sheet[0] < 3:\n",
    "            #    print(gain.start+sheet[0], gain.start+sheet[1])\n",
    "            sheet_centers.append( gain.start + int((sheet[0]+sheet[1])/2) )\n",
    "        sheet_keys = [f'S{i+1}' for i in range(len(sheet_centers))]\n",
    "        sdb_centers = dict(zip(sheet_keys, sheet_centers))\n",
    "# Manually curated the centers to exclude two small strands in the CD between S6/S7 @ 707-708 and 711-713, respectively (low pLDDT here).\n",
    "sda_centers = {'H1': 313, 'H2': 328, 'H3': 359, 'H4': 383, 'H5': 409, 'H6': 424, 'H7': 444}\n",
    "sdb_centers = {'S1': 622, 'S2': 631, 'S3': 645, 'S4': 658, 'S5': 670, 'S6': 695, 'S7': 719, 'S8': 736, 'S9': 752, 'S10': 765, 'S11': 771, 'S12': 782, 'S13': 793}\n",
    "\n",
    "# Find closest residue to the center (GESAMT), note down the sequence, start, end of the matched SSE; write to FASTA\n",
    "    # A dict of dicts --> for each key, there is a dictionary inside sse_seqs['H1'][gain.name]:'seqlist'\n",
    "all_keys = list({**sda_centers, **sdb_centers}.keys())\n",
    "sse_seqs = {k:{} for k in all_keys}\n",
    "sse_extents = {k:{} for k in all_keys}\n",
    "unmatched = {k:0 for k in all_keys}\n",
    "unstructured = {k:0 for k in all_keys}\n",
    "\n",
    "for i, gain in enumerate(valid_collection.collection):\n",
    "        a_gesamt_file = f'../sda_template_aligned_files/sda_{i}.out'\n",
    "        b_gesamt_file = f'../sdb_template_aligned_files/sdb_{i}.out'\n",
    "\n",
    "        sda_matches = find_anchor_matches(a_gesamt_file, sda_centers, isTarget=False)\n",
    "        sdb_matches = find_anchor_matches(b_gesamt_file, sdb_centers, isTarget=False)\n",
    "        #print(sda_matches, sdb_matches)\n",
    "        hel_extents = np.full(shape = (gain.end-gain.start+1), fill_value=100)\n",
    "        she_extents = np.full(shape = (gain.end-gain.start+1), fill_value=100)\n",
    "        # Establish two matrices to match the respective residue to the index of its helix/sheet for easier matching\n",
    "        for i,element in enumerate(gain.sda_helices):\n",
    "            hel_extents[element[0]:element[1]] = i\n",
    "        for i,element in enumerate(gain.sdb_sheets):\n",
    "            she_extents[element[0]:element[1]] = i\n",
    "        # Match the corresponding closest residue to find the associated SSE with start, end and sequence\n",
    "        for sse, match in sda_matches.items():\n",
    "            if match[0] is None:\n",
    "                unmatched[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_index = hel_extents[match[0]-gain.start]\n",
    "\n",
    "            if sse_index == 100:\n",
    "                unstructured[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_extents[sse][gain.name] = gain.sda_helices[sse_index]\n",
    "            sse_seqs[sse][gain.name] = gain.sequence[gain.sda_helices[sse_index][0]:gain.sda_helices[sse_index][1]]\n",
    "        \n",
    "        for sse, match in sdb_matches.items():\n",
    "            if match[0] is None:\n",
    "                unmatched[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_index = she_extents[match[0]-gain.start]\n",
    "\n",
    "            if sse_index == 100:\n",
    "                unstructured[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_extents[sse][gain.name] = gain.sdb_sheets[sse_index]\n",
    "            sse_seqs[sse][gain.name] = gain.sequence[gain.sdb_sheets[sse_index][0]:gain.sdb_sheets[sse_index][1]]\n",
    "        \n",
    "for sse in all_keys:\n",
    "    with open(f'../sse_aln/{sse}.seqs.fa','w') as fa:\n",
    "        for name, seq in sse_seqs[sse].items():\n",
    "            fa.write(f'>{name}\\n{\"\".join(seq)}\\n')\n",
    "\n",
    "print(unmatched, '\\n', unstructured)\n",
    "#   Run MAFFT with each of the gathered sequences\n",
    "#   For each MAFFT\n",
    "#       Find the most conserved residue (Identity matrix)\n",
    "#       Set as new Anchor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for gain in valid_collection.collection:\n",
    "\n",
    "    if gain.name[:10] == 'A0A7K7IHI9': #SDA\n",
    "        sda_gain = gain\n",
    "    if gain.name[:10] =='A0A3P9I6M5': # SDB\n",
    "        sdb_gain = gain\n",
    "for i,k in enumerate(sda_centers.keys()):\n",
    "    kfile = glob.glob(f\"../sse_aln/{k}.aln.fa\")[0]\n",
    "    with open(kfile) as alnf:\n",
    "        x = alnf.readlines()[1].strip(\" \\n\")\n",
    "        kcutoff = len(x)\n",
    "    print(kcutoff)\n",
    "    aln = sse_func.read_alignment(kfile, cutoff=kcutoff)\n",
    "    h = sda_gain.sda_helices[i]\n",
    "    aln_matrix = np.array([list(seq) for seq in aln.values()])\n",
    "    kquality, kocc = calc_identity(aln_matrix)\n",
    "    '''    kquality = []\n",
    "    kocc = []\n",
    "    for col in range(aln_matrix.shape[1]):\n",
    "        chars, count = np.unique(aln_matrix[:,col], return_counts=True)\n",
    "        if chars[0] == '-':\n",
    "            q = count[1]\n",
    "        else:\n",
    "            q = count[0]\n",
    "        x = np.where(chars == '-')[0][0]\n",
    "        kocc.append(14435 - count[x])\n",
    "        kquality.append(q)'''\n",
    "    template_aln_seq = aln[sda_gain.name]\n",
    "    template_res_idx = np.argmax(kquality)\n",
    "    print(template_aln_seq, template_res_idx)\n",
    "    template_index = template_aln_seq[:template_res_idx+1]\n",
    "    t_res = template_aln_seq[template_res_idx]\n",
    "    print(template_index, t_res)\n",
    "    new = template_index\n",
    "\n",
    "    fig = plt.figure(figsize=[4,2], facecolor='w')\n",
    "    plt.bar(range(kcutoff), kquality)\n",
    "    plt.title(f'SDA TEMPLATE : {k}')\n",
    "    plt.xticks(ticks = range(kcutoff), labels=template_aln_seq, fontsize=5)\n",
    "    plt.savefig(f'../sse_aln/{k}.template1.png', dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "for i,k in enumerate(sdb_centers.keys()):\n",
    "    kfile = glob.glob(f\"../sse_aln/{k}.aln.fa\")[0]\n",
    "    with open(kfile) as alnf:\n",
    "        x = alnf.readlines()[1].strip(\" \\n\")\n",
    "        kcutoff = len(x)\n",
    "    print(kcutoff)\n",
    "    aln = sse_func.read_alignment(kfile, cutoff=kcutoff)\n",
    "    h = sdb_gain.sdb_sheets[i]\n",
    "    aln_matrix = np.array([list(seq) for seq in aln.values()])\n",
    "    kquality = []\n",
    "    kocc = []\n",
    "    kquality, kocc = calc_identity(aln_matrix)\n",
    "        \n",
    "    template_aln_seq = aln[sdb_gain.name]\n",
    "    template_res_idx = np.argmax(kquality)\n",
    "    print(template_aln_seq, template_res_idx)\n",
    "    template_index = template_aln_seq[:template_res_idx+1]\n",
    "    t_res = template_aln_seq[template_res_idx]\n",
    "    print(template_index, t_res)\n",
    "    new = template_index\n",
    "\n",
    "    fig = plt.figure(figsize=[4,2], facecolor='w')\n",
    "    plt.bar(range(kcutoff), kquality)\n",
    "    plt.title(f'SDB TEMPLATE : {k}')\n",
    "    plt.xticks(ticks = range(kcutoff), labels=template_aln_seq, fontsize=5)\n",
    "    plt.savefig(f'../sse_aln/{k}.template1.png', dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efcc3436bf700bf51081b251413b556e30c22be82f452601745119c8a669a2f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
