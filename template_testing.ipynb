{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "import glob, math, json, glob, re\n",
    "#from shutil import copyfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import logomaker\n",
    "# LOCAL IMPORTS\n",
    "#from indexing_classes import GPCRDBIndexing\n",
    "from gain_classes import GainDomain, GainCollection, Anchors, GPS\n",
    "import sse_func\n",
    "import matplotlib.pyplot as plt\n",
    "import template_finder as tf\n",
    "\n",
    "def calc_identity(aln_matrix):\n",
    "    # This takes an alignment matrix with shape=(n_columns, n_sequences) and generates counts based on the identity matrix.\n",
    "    # Returns the highest non \"-\" residue count as the most conserved residue and its occupancy based on count(\"-\") - n_struc\n",
    "    n_struc = aln_matrix.shape[0]\n",
    "    quality = []\n",
    "    occ = []\n",
    "    for col in range(aln_matrix.shape[1]):\n",
    "        chars, count = np.unique(aln_matrix[:,col], return_counts=True)\n",
    "        dtype = [('aa', 'S1'), ('counts', int)]\n",
    "        values = np.array(list(zip(chars,count)), dtype=dtype)\n",
    "        s_values = np.sort(values, order='counts')\n",
    "\n",
    "        if s_values[-1][0] == b'-':\n",
    "            q = s_values[-2][1]\n",
    "        else:\n",
    "            q = s_values[-1][1]\n",
    "        x = np.where(chars == '-')[0][0]\n",
    "        occ.append(n_struc - count[x])\n",
    "        quality.append(q)\n",
    "    return quality, occ\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize pre-calculated metrix for the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''valid_seqs = sse_func.read_multi_seq(\"/home/hildilab/projects/agpcr_nom/app_gain_gain.fa\")\n",
    "quality_file = \"/home/hildilab/projects/agpcr_nom/app_gain_gain.mafft.jal\"\n",
    "alignment_file = \"/home/hildilab/projects/agpcr_nom/app_gain_gain.mafft.fa\"\n",
    "stride_files = glob.glob(\"/home/hildilab/projects/agpcr_nom/sigmas/sigma_2/*\")\n",
    "# This only contains the sigma files for truncated (?) PDBs.\n",
    "#quality = sse_func.read_quality(quality_file)\n",
    "gps_minus_one = 6781 \n",
    "aln_cutoff = 6826 \n",
    "alignment_dict = sse_func.read_alignment(alignment_file, aln_cutoff)\n",
    "aln_matrix = np.array([list(seq) for seq in alignment_dict.values()])\n",
    "#print(aln_matrix.shape)\n",
    "quality, occ = calc_identity(aln_matrix)'''\n",
    "\n",
    "\"\"\"precalc_anchors = [ 662, 1194, 1912, 2490, 2848, 3011, 3073, 3260, #H1-H8\n",
    "            3455, 3607, 3998, 4279, 4850, 5339, #5341 S1-S6, S7 REMOVED!\n",
    "            5413, 5813, 6337, 6659, 6696, 6765, 6808] #S8-13\n",
    "precalc_anchor_occupation = [ 4594.,  6539., 11392., 13658.,  8862., 5092.,  3228., 14189., #H1-H8\n",
    "                      9413., 12760.,  9420., 11201., 12283., 3676.,#  4562. S1-S6, S7 REMOVED!\n",
    "                     13992., 12575., 13999., 14051., 14353., 9760., 14215.] #S8-13\n",
    "precalc_anchor_dict = sse_func.make_anchor_dict(precalc_anchors, 3425)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to the collection, we need the corresponding PDB files.\n",
    "valid_collection = pd.read_pickle(\"../valid_collection.pkl\")\n",
    "allpdbs = glob.glob('../all_pdbs/*.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Data for the initially selected templates\n",
    "def get_template_information(identifier, gain_collection, subdomain='a', threshold=3, no_input=True):\n",
    "    for gain in gain_collection.collection:\n",
    "        if identifier in gain.name:\n",
    "            print(gain.name, gain.start, gain.subdomain_boundary, gain.end, \"\\n\")\n",
    "\n",
    "            if subdomain.lower() == 'a':\n",
    "                sse = gain.sda_helices\n",
    "                d_string = \"HELIX \"\n",
    "                sse_type = \"H\"\n",
    "            elif subdomain.lower() == 'b':\n",
    "                sse = gain.sdb_sheets\n",
    "                d_string = \"STRAND\"\n",
    "                sse_type = \"S\"\n",
    "            else:\n",
    "                print(\"NO SUBDOMAIN specified. EXITING.\")\n",
    "        \n",
    "            #print(sse)\n",
    "            anchor_quality = {}\n",
    "            anchors = {}\n",
    "            counter = 1\n",
    "            aln_indices = []\n",
    "            for i,element in enumerate(sse):\n",
    "                if element[1]-element[0] <= threshold:\n",
    "                    print(\"Element length below threshold. Skipping.\", element)\n",
    "                    continue\n",
    "                if subdomain =='a' and gain.start+element[0] > gain.subdomain_boundary:\n",
    "                    print(\"Skipping Subdomain A Helix\", element)\n",
    "                    continue\n",
    "                label = f'{sse_type}{counter}'\n",
    "                q = [ gain.residue_quality[res] for res in range(element[0], element[1]+1)]\n",
    "                max_id = element[0]+np.argmax(q)\n",
    "                max_res = gain.sequence[max_id]\n",
    "                #aln_idx = gain.alignment_indices[max_id]\n",
    "                res_id = gain.start+max_id+1\n",
    "                print(f\"{d_string} #{i+1}: {max_res}{res_id} @ SSE residue {max_id-element[0]} | q = {np.max(q)} with res_idx {max_id} | {gain.start+element[0]}-{gain.start+element[1]}\")\n",
    "                if not no_input:\n",
    "                    confirm = input(f\"{d_string} #{i+1}: {max_res}{res_id} @ SSE re {max_id-element[0]} | q={np.max(q)} w res_idx {max_id} | {gain.start+element[0]}-{gain.start+element[1]}. Keep?\")\n",
    "                    if confirm.lower() != \"y\":\n",
    "                        print(\"Skipping this anchor.\");continue\n",
    "                anchor_quality[label] = np.max(q)\n",
    "                anchors[label] = max_id\n",
    "                aln_indices.append(gain.alignment_indices[max_id])\n",
    "                counter += 1\n",
    "            pdb_anchors = {v:k+gain.start+1 for v,k in anchors.items()}\n",
    "            print(\"__________\")\n",
    "            return anchors, anchor_quality, aln_indices, pdb_anchors\n",
    "_,_,_,_ = get_template_information('A0A6G1Q0B9', valid_collection, 'a')\n",
    "#get_template_information('A0A3P8S994', valid_collection, 'b')\n",
    "        ##_, cd, _, _ = gain.create_indexing(precalc_anchors, precalc_anchor_occupation, precalc_anchor_dict)\n",
    "        #_, cd, _, _ = gain.create_indexing(anchors, anchor_occupation, anchor_dict)\n",
    "        #ac =  {k[:-3]:gain.start+v for k,v in cd.items()}\n",
    "        #print(ac)\n",
    "        #print(gain.Anchors.alignment_indices )\n",
    "        #print(gain.Anchors.gain_residues)\n",
    "        #print(gain.start)\n",
    "        #qq =[ quality[v] for v in gain.alignment_indices[324-gain.start:331-gain.start]]\n",
    "        #print(qq)\n",
    "        #print(gain.alignment_indices[324-gain.start:331-gain.start])\n",
    "# pre:  'H1': 309, 'H2': 324, 'H3': 358, 'H4': 385, 'H5': 410, 'H6': 421, 'H8': 438\n",
    "# cons:            'H1': 317, 'H2': 358, 'H3': 377, 'H4': 415, 'H6': 425, 'H7': 443,\n",
    "\n",
    "# pre:  'S1': 621, 'S2': 628, 'S3': 643, 'S4': 669,                         'S7': 717,  'S8': 734,  'S9': 750, 'S10': 764, 'S11': 770, 'S12': 782, 'S13': 793\n",
    "# cons: 'S1': 621, 'S3': 629, 'S4': 644, 'S5': 671, 'S7': 697, 'S10': 712, 'S11': 717, 'S13': 736, 'S14': 753, 'S15': 764, 'S16': 770, 'S18': 784, 'S19': 793"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a GESAMT bashfile for pairwise aln to each subdomain. Output the resulting PDB into respective folder\n",
    "#SDB TEMPLATE\n",
    "import shutil\n",
    "sdb_template = 'A0A3P8S994-A0A3P8S994_AMPPE-AGRE5b,duplicate2-Amphiprion_percula'\n",
    "# SDA TEMPLATEs\n",
    "sda_templates = {'A': 'A0A2Y9F628-A0A2Y9F628_PHYMC-AGRA3isoformX1-Physeter_macrocephalus', \n",
    "                    'B': 'A0A4W6DVA0-A0A4W6DVA0_LATCA-AGRB1b-Lates_calcarifer', \n",
    "                    'C': 'A0A7K6E127-A0A7K6E127_9PASS-CELR3protein-Grantiella_picta.', \n",
    "                    'D': 'A0A1A7WJQ6-A0A1A7WJQ6_9TELE-GR144-Iconisemion_striatum.', \n",
    "                    'E': 'A0A3P8S994-A0A3P8S994_AMPPE-AGRE5b,duplicate2-Amphiprion_percula', \n",
    "                    'F': 'A0A452IH20-A0A452IH20_9SAUR-AGRF5-Gopherus_agassizii', \n",
    "                    'G': 'A0A1W4WJB1-A0A1W4WJB1_AGRPL-AGRG6-likeisoformX1-Agrilus_planipennis', #'A0A7K5TKG3-A0A7K5TKG3_9FRIN-AGRG6protein-Urocynchramus_pylzowi.', missing H1\n",
    "                    'L': 'A0A452HCU9-A0A452HCU9_9SAUR-AGRL3-Gopherus_agassizii', \n",
    "                    'V': 'A0A6Q2XYK2-A0A6Q2XYK2_ESOLU-AGRV1-Esox_lucius',\n",
    "                    'X': \"A0A6F9A857-A0A6F9A857_9TELE-Uncharacterizedprotein-Coregonus_sp._'balchen'.\"}\n",
    "def find_pdb(name, pdb_folder):\n",
    "    identifier = name.split(\"-\")[0]\n",
    "    target_pdb = glob.glob(f\"{pdb_folder}/*{identifier}*.pdb\")[0]\n",
    "    return target_pdb\n",
    "\n",
    "sdb_template_pdb = find_pdb(sdb_template, '../all_pdbs')\n",
    "\"\"\"tf.run_gesamt_execution(valid_collection.collection, \n",
    "                            outfolder=\"../A0A3P8S994_sdb\",\n",
    "                            pdb_folder='../all_pdbs', \n",
    "                            domain='sdb', \n",
    "                            n_threads=5, \n",
    "                            max_struc=len(valid_collection.collection), \n",
    "                            no_run=False,\n",
    "                            template=sdb_template_pdb)\"\"\"\n",
    "#shutil.copyfile(sdb_template_pdb, f'../calc_templates/sdb_A0A3P8S994.pdb')\n",
    "\n",
    "for fam, prot in sda_templates.items():\n",
    "    identifier = prot.split(\"-\")[0]\n",
    "    current_template = find_pdb(prot, '../all_pdbs')\n",
    "\n",
    "    shutil.copyfile(current_template, f'../calc_templates/{fam}_{identifier}.pdb')\n",
    "    gesamt_outfolder = f'../{identifier}_{fam}_sda'\n",
    "\n",
    "\"\"\"    tf.run_gesamt_execution(valid_collection.collection, \n",
    "                            outfolder=gesamt_outfolder, \n",
    "                            pdb_folder='../all_pdbs', \n",
    "                            domain='sda', \n",
    "                            n_threads=5, \n",
    "                            max_struc=len(valid_collection.collection), \n",
    "                            no_run=True,\n",
    "                            template=current_template)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {'sdb':['A0A3P8S994-A0A3P8S994_AMPPE-AGRE5b,duplicate2-Amphiprion_percula', 'b', '../A0A3P8S994_sdb'],\n",
    "                    'A': ['A0A2Y9F628-A0A2Y9F628_PHYMC-AGRA3isoformX1-Physeter_macrocephalus', 'a', '../A0A2Y9F628_A_sda'], \n",
    "                    'B': ['A0A4W6DVA0-A0A4W6DVA0_LATCA-AGRB1b-Lates_calcarifer', 'a', '../A0A4W6DVA0_B_sda'], \n",
    "                    'C': ['A0A7K6E127-A0A7K6E127_9PASS-CELR3protein-Grantiella_picta.', 'a', '../A0A7K6E127_C_sda'], \n",
    "                    'D': ['A0A1A7WJQ6-A0A1A7WJQ6_9TELE-GR144-Iconisemion_striatum.', 'a', '../A0A1A7WJQ6_D_sda'], \n",
    "                    'E': ['A0A3P8S994-A0A3P8S994_AMPPE-AGRE5b,duplicate2-Amphiprion_percula', 'a', '../A0A3P8S994_E_sda'], \n",
    "                    'F': ['A0A452IH20-A0A452IH20_9SAUR-AGRF5-Gopherus_agassizii', 'a', '../A0A452IH20_F_sda'], \n",
    "                    'G': ['A0A1W4WJB1-A0A1W4WJB1_AGRPL-AGRG6-likeisoformX1-Agrilus_planipennis', 'a', '../A0A1W4WJB1_G_sda'], \n",
    "                    'L': ['A0A452HCU9-A0A452HCU9_9SAUR-AGRL3-Gopherus_agassizii', 'a', '../A0A452HCU9_L_sda'], \n",
    "                    'V': ['A0A6Q2XYK2-A0A6Q2XYK2_ESOLU-AGRV1-Esox_lucius', 'a', '../A0A6Q2XYK2_V_sda'],\n",
    "                    'X': [\"A0A6F9A857-A0A6F9A857_9TELE-Uncharacterizedprotein-Coregonus_sp._'balchen'.\", 'a', '../A0A6F9A857_X_sda']}\n",
    "template_anchors = {}\n",
    "template_quality = {}\n",
    "template_indices = []\n",
    "\n",
    "all_indices = []\n",
    "for k, v in templates.items():\n",
    "    if k == 'sdb':\n",
    "        threshold = 2\n",
    "    else:\n",
    "        threshold = 4\n",
    "    raw_anchors, a_qual, indices, anchors = get_template_information(v[0].split(\"-\")[0], valid_collection, v[1], threshold=threshold)\n",
    "    template_anchors[k] = anchors\n",
    "    template_quality[k] = a_qual\n",
    "    if k == 'sdb':\n",
    "        continue\n",
    "    for i in indices:\n",
    "        template_indices.append(i)\n",
    "    all_indices.append(indices)\n",
    "\n",
    "print(template_anchors)\n",
    "print(template_quality)\n",
    "a,b  = np.unique(template_indices, return_counts=True)\n",
    "a_counts = dict(zip(a,b)) \n",
    "anchor_col = {x:i for i,x in enumerate(a)}\n",
    "print(anchor_col, len(a))\n",
    "hasAnchor = np.zeros(shape=(len(a), len(all_indices)))\n",
    "print(hasAnchor.shape)\n",
    "for fam_count, indices in enumerate(all_indices):\n",
    "    print(indices)\n",
    "    for v in indices:\n",
    "        print(v)\n",
    "        hasAnchor[anchor_col[v], fam_count] = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the pairwise GESAMT, we can use the resulting OUT and PDB files for analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=[4,4], facecolor='w')\n",
    "im = plt.imshow(hasAnchor.T, cmap='gray')\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(range(len(a)))\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_xticklabels(a, rotation=90)\n",
    "ax.set_yticklabels(list(\"ABCDEFGLVX\"))\n",
    "ax.set_xticks(np.arange(-.5, 11, 1), minor=True)\n",
    "ax.set_yticks(np.arange(-.5, 10, 1), minor=True)\n",
    "plt.xlabel(\"Helix Anchor column\")\n",
    "plt.ylabel(\"Subfamily SDA Template\")\n",
    "ax.grid(which='minor', linewidth=2)\n",
    "plt.savefig(\"../sda_template_anchors.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"UNUSED CODE FROM BEFORE TEMPLATE_FINDER.py\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def gain_set_to_template(list_of_gains, index_list, anchors, anchor_occupation, anchor_dict, mode='template', penalty=None, n_anch=23, print_matches=False):\n",
    "    distances = np.full(shape=(len(list_of_gains), n_anch), fill_value=penalty)\n",
    "    all_sda_anch = []\n",
    "    all_sdb_anch = []\n",
    "\n",
    "    for gain_idx, gain in enumerate(list_of_gains):\n",
    "        gain_distances, template_sda_anchors, template_sdb_anchors = match_to_template(index_list[gain_idx], \n",
    "                                                                                       gain, \n",
    "                                                                                       anchors=anchors, \n",
    "                                                                                       anchor_occupation=anchor_occupation, \n",
    "                                                                                       anchor_dict=anchor_dict,\n",
    "                                                                                       n_anch=n_anch,\n",
    "                                                                                       penalty=penalty, mode=mode)\n",
    "        distances[gain_idx,:] = gain_distances\n",
    "        all_sda_anch.append(template_sda_anchors)\n",
    "        all_sdb_anch.append(template_sdb_anchors)\n",
    "        if print_matches : print(template_sda_anchors, template_sdb_anchors)\n",
    "    return distances, all_sda_anch, all_sdb_anch\n",
    "\n",
    "def match_to_template(gain_idx, gain, anchors, anchor_occupation, anchor_dict, n_anch, penalty=None, mode='anchor-anchor'):\n",
    "    '''\n",
    "    Match a GAIN domain to a template PDB. This function has two modes. \n",
    "    In the 'template' mode, we match the closest residue to the template as the new anchor and output the residues and their distances.\n",
    "    In the 'anchor-anchor' mode, we also calculate anchor-anchor distances and output them.\n",
    "\n",
    "    gain_idx indicates the index of the domain used for locating GESAMT alignment files.\n",
    "    template PDBs are global, but not needed since we are handling only the aligned files / PDBs.\n",
    "\n",
    "    -GLOBALS\n",
    "       sda_anchors, sdb_anchors, sda_a_coords, sdb_a_coords'''\n",
    "    if gain.name.endswith('.'): # Adjust the name for finding the GESAMT aligned PDBs.\n",
    "\n",
    "        name = gain.name[:-1]\n",
    "    else:\n",
    "        name = gain.name\n",
    "    name = name.replace('(','-').replace(')','-').replace('\\'','')\n",
    "    \n",
    "    a_gesamt_file = f'../sda_template_aligned_files/sda_{gain_idx}.out'\n",
    "    b_gesamt_file = f'../sdb_template_aligned_files/sdb_{gain_idx}.out'\n",
    "\n",
    "    #a_matches = find_anchor_matches(a_gesamt_file, sda_gain_anchors, isTarget=True)\n",
    "    #b_matches = find_anchor_matches(b_gesamt_file, sdb_gain_anchors, isTarget=True)\n",
    "    sda_actual_anchors = find_anchor_matches(a_gesamt_file, sda_anchors)\n",
    "    sdb_actual_anchors = find_anchor_matches(b_gesamt_file, sdb_anchors)\n",
    "    #print('SDA MATCHES (mobile --> template_res):', matches, '\\n\\nACTUAL ANCHORS (template --> mobile_res)', actual_anchors)\n",
    "    #print('SDB MATCHES (mobile --> template_res):', matches, '\\n\\nACTUAL ANCHORS (template --> mobile_res)', actual_anchors)\n",
    "\n",
    "    if mode == 'anchor-anchor':\n",
    "        # Get the precalculated anchors from the GainDomain object, split into helices and strand dictionaries.\n",
    "        _, gain_anchor_dict, _, _ = gain.create_indexing(anchors, anchor_occupation, anchor_dict,split_mode='double', silent=True)\n",
    "        gain_anchors =  {k[:-3]:v+gain.start for k,v in gain_anchor_dict.items()} #modify X#.50 --> X#\n",
    "        sda_gain_anchors = {k:v for k,v in gain_anchors.items() if \"H\" in k and v < gain.subdomain_boundary}\n",
    "        sdb_gain_anchors = {k:v for k,v in gain_anchors.items() if \"S\" in k and v > gain.subdomain_boundary}\n",
    "        # Here, the pre-defined anchor-anchor distances will be calculated.\n",
    "        _, a_dist = calculate_anchor_distances(sda_a_coords, f'../sda_template_aligned_pdbs/{name}_sda.pdb', sda_gain_anchors)\n",
    "        _, b_dist = calculate_anchor_distances(sdb_a_coords, f'../sdb_template_aligned_pdbs/{name}_sdb.pdb', sdb_gain_anchors)\n",
    "        all_dist = {**a_dist, **b_dist}\n",
    "\n",
    "    elif mode == 'template': \n",
    "        all_dist = {k:v[1] for k,v in {**sda_actual_anchors, **sdb_actual_anchors}.items() } #{'H1': (653, 1.04) , ...}\n",
    "    # From the anchor-residue or anchor-anchor distances, construct a distance matrix where unmatched entried will be assigned a penalty value.\n",
    "    else: \n",
    "        print('Unknown mode. Just returning matched anchors.')\n",
    "        return None, sda_actual_anchors, sdb_actual_anchors\n",
    "    # Fill a matrix with the individual distances, assign unmatched anchors a pre-set penalty value\n",
    "    distances = np.full(shape=(n_anch), fill_value=penalty)\n",
    "    for i, sse in enumerate(anchor_dict.values()):\n",
    "        #print(i, sse)\n",
    "        if sse in all_dist.keys():\n",
    "            if all_dist[sse] is not None:\n",
    "                distances[i] = all_dist[sse]\n",
    "            #print(all_dist[sse])\n",
    "            for i,val in enumerate(distances):\n",
    "                if penalty is not None and val > penalty: \n",
    "                    distances[i] = penalty\n",
    "\n",
    "    return distances, sda_actual_anchors, sdb_actual_anchors\n",
    "\n",
    "def plot_pca(distance_matrix, cluster_intervals, n_components, name, plot3D=False, save=True):\n",
    "    colorlist = ['blue','red','green','yellow','orange','purple','forestgreen','limegreen','firebrick']\n",
    "    #X = anchor_distance_matrix\n",
    "    X = distance_matrix\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X)\n",
    "    print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "    #print(pca.singular_values_)\n",
    "    X_r = pca.fit(X).transform(X)\n",
    "    print(X_r.shape)\n",
    "\n",
    "    fig = plt.figure(figsize=[5,5])\n",
    "    fig.set_facecolor('w')\n",
    "    if plot3D:\n",
    "        ax = ax = fig.add_subplot(projection='3d')\n",
    "        for i, interval in enumerate(cluster_intervals):\n",
    "            ax.scatter(X_r[interval[0]:interval[1],0], X_r[interval[0]:interval[1],1], X_r[interval[0]:interval[1],2], marker='o', s=8, c=colorlist[i])\n",
    "    else:\n",
    "        ax = fig.add_subplot()\n",
    "        for i, interval in enumerate(cluster_intervals):\n",
    "            ax.scatter(X_r[interval[0]:interval[1],0], X_r[interval[0]:interval[1],1], marker='o', s=8, c=colorlist[i])\n",
    "    #cbar = plt.colorbar()\n",
    "    #plt.figure()\n",
    "    #plt.imshow(anchor_distance_matrix[::30])\n",
    "    #print(np.max(anchor_distance_matrix))\n",
    "    #plt.colorbar()\n",
    "    ax.set_title(f'PCA of MiniBatchKMeans - {name}')\n",
    "    ax.set_xlabel('PC 0')\n",
    "    ax.set_ylabel('PC 1')\n",
    "    if plot3D:\n",
    "        ax.set_zlabel('PC 2')\n",
    "    if save:\n",
    "        plt.savefig(f'{name}_pca.png', dpi=300)\n",
    "        \n",
    "def cluster_k_means(matrix, list_of_gain_obj, n_cluster=9):\n",
    "    struc_list = [gain.name for gain in list_of_gain_obj]\n",
    "    clust = MiniBatchKMeans(n_clusters=n_cluster,\n",
    "                            random_state=0,\n",
    "                            batch_size=6,\n",
    "                            max_iter=10,\n",
    "                            n_init=\"auto\").fit(matrix)\n",
    "    #clust = AgglomerativeClustering(n_clusters=n_cluster, metric='euclidean', \n",
    "    #                        memory=None, connectivity=None, compute_full_tree='auto', linkage='complete', distance_threshold=None, compute_distances=True).fit(anchor_distance_matrix)\n",
    "    clustering=clust.labels_\n",
    "    #print(np.unique(clustering, return_counts=True))\n",
    "    n_struc, n_distances = matrix.shape # 14432, 21\n",
    "    #print(clustering.labels_)\n",
    "    new_order = np.zeros(shape=(n_struc), dtype=int)\n",
    "    current_num = 0\n",
    "    for i in range(n_cluster):\n",
    "        for j, cluster_id in enumerate(clustering):\n",
    "            if cluster_id == i :\n",
    "                new_order[j] = current_num\n",
    "                current_num += 1\n",
    "    #print(new_order)\n",
    "    remap_dict = {old_idx:new_idx for old_idx, new_idx in enumerate(new_order)}\n",
    "    inv_remap_dict = {v:k for k,v in remap_dict.items()}\n",
    "    reordered_matrix = np.zeros(shape=(n_struc,n_distances))\n",
    "    for x in range(n_struc):\n",
    "        new_x = remap_dict[x]\n",
    "        reordered_matrix[new_x,:] = matrix[x,:]\n",
    "\n",
    "    #fig = plt.figure(figsize=[20,1])\n",
    "    #fig.set_facecolor('w')\n",
    "    #plt.imshow(ordered_distances.transpose(), cmap='Greys')\n",
    "    #cbar = plt.colorbar()\n",
    "    #cbar.set_label('RMSD [$\\AA$]')\n",
    "    #plt.savefig('../test_largedist.png',dpi=300)\n",
    "\n",
    "    reordered_names = [struc_list[inv_remap_dict[k]] for k in range(n_struc)]\n",
    "    reordered_clusters = [clustering[inv_remap_dict[k]] for k in range(n_struc)]\n",
    "\n",
    "    _, cluster_starts = np.unique(reordered_clusters, return_index = True)\n",
    "    cluster_intervals = [(cluster_starts[k], cluster_starts[k+1]) for k in range(n_cluster-1)]\n",
    "    cluster_intervals.append((cluster_starts[-1], n_struc))\n",
    "    #print(cluster_intervals)\n",
    "\n",
    "    return reordered_matrix, cluster_intervals, reordered_names\n",
    "\n",
    "def read_gesamt_pairs(gesamtfile):\n",
    "    with open(gesamtfile) as f:\n",
    "        pair_lines = [line for line in f.readlines() if line.startswith(\"|\")][2:]\n",
    "        #|H- A:LEU 720 | <**1.21**> |H- A:LEU 633 |\n",
    "        #| - A:ALA 721 | <..1.54..> | + A:GLN 634 |\n",
    "        #| + A:GLU 722 | <..1.75..> | + A:PRO 635 |\n",
    "        #| + A:GLU 723 | <==2.03==> | + A:GLN 636 |\n",
    "        #| + A:ASN 724 | <..1.91..> | - A:ALA 637 |\n",
    "        #|H+ A:ARG 725 | <..2.08..> |H- A:LEU 638 |\n",
    "    # construct a data structure with indices of both sides (fixed, mobile)\n",
    "    mobile_pairs = {}\n",
    "    template_pairs = {}\n",
    "    for pair in pair_lines:\n",
    "        template_str, distance_str, mobile_str = pair[9:13].strip(), pair[19:23].strip(), pair[36:40].strip()\n",
    "        #print(template_str, distance_str, mobile_str, \"\\n\", pair)\n",
    "        # If either residue is empty, let the pair point to None\n",
    "        if len(template_str) == 0:\n",
    "            mobile_pairs[int(mobile_str)] = (None, None)\n",
    "            continue\n",
    "        if len(mobile_str) == 0:\n",
    "            template_pairs[int(template_str)] = (None, None)\n",
    "            continue\n",
    "        if len(distance_str) == 0:\n",
    "            dist = None\n",
    "        else:\n",
    "            dist = float(distance_str)\n",
    "        template_pairs[int(template_str)] = (int(mobile_str), dist)\n",
    "        mobile_pairs[int(mobile_str)] = (int(template_str), dist)\n",
    "    \n",
    "    return template_pairs, mobile_pairs\n",
    "\n",
    "def find_anchor_matches(file, anchor_dict, isTarget=False):\n",
    "    # Takes a gesamt file and an anchor dictionary either of the target or the template and returns the matched other residue with the pairwise distance\n",
    "    # as a dictionary: {'H1': (653, 1.04) , ...}\n",
    "    template_pairs, mobile_pairs = read_gesamt_pairs(file)\n",
    "    # Find the closest residue to template anchors\n",
    "    matched_residues = {}\n",
    "    if not isTarget:\n",
    "        parsing_dict = template_pairs\n",
    "    else:\n",
    "        parsing_dict = mobile_pairs\n",
    "\n",
    "    start, end = min(parsing_dict.keys()), max(parsing_dict.keys())\n",
    "    for anchor_name, anchor_res in anchor_dict.items():\n",
    "        # If the anchor lies outside the aligned segments, pass empty match (None, None)\n",
    "        if anchor_res < start or anchor_res > end:\n",
    "            matched_residues[anchor_name] = (None, None)\n",
    "            continue\n",
    "        matched_residues[anchor_name] = parsing_dict[anchor_res]\n",
    "\n",
    "    return matched_residues\n",
    "\n",
    "def get_anchor_coords(pdbfile, anchor_dict, multistate=False):\n",
    "    # Find the CA coordinates of the anchor residue in the template PDB, return dictionary with the coords for each labeled anchor\n",
    "    with open(pdbfile) as p:\n",
    "        if multistate:\n",
    "            data = p.read().split(\"ENDMDL\")[1]\n",
    "        if not multistate:\n",
    "            data = p.read()\n",
    "        mdl2data = [l for l in data.split(\"\\n\") if l.startswith(\"ATOM\")]\n",
    "        ca_data = [atom for atom in mdl2data if \" CA \" in atom]\n",
    "        #print(anchor_dict, pdbfile)\n",
    "    # Find the PDB coords of the CA of each atom and get them as a dict with  {'myanchor':(x,y,z), ...}\n",
    "    res_ca_dict = {int(line[22:26]):(float(line[30:38]), float(line[38:46]), float(line[46:54])) for line in ca_data}\n",
    "    coord_dict={}\n",
    "    for anchor_name, anchor_res in anchor_dict.items():\n",
    "        if anchor_res in res_ca_dict.keys():\n",
    "            coord_dict[anchor_name]=res_ca_dict[anchor_res]\n",
    "        else:\n",
    "            \"FALLBACK. Eliminiating Residue Anchor.\"\n",
    "            coord_dict[anchor_name]=(1000.0, 1000.0, 1000.0)\n",
    "    #coord_dict = {anchor_name:res_ca_dict[anchor_res] for anchor_name, anchor_res in anchor_dict.items()}\n",
    "    \n",
    "    return coord_dict\n",
    "\n",
    "def space_distance(coords1, coords2):\n",
    "    # Pythagorean distance of two sets of coords\n",
    "    return round(math.sqrt(abs(coords1[0]-coords2[0])**2 + abs(coords1[1]-coords2[1])**2 + abs(coords1[2]-coords2[2])**2 ),3)\n",
    "\n",
    "def calculate_anchor_distances(template_anchor_coords, mobile_pdb, mobile_anchors, threshold=10):\n",
    "    # template_anchor_coords are a precalculated set of coords to save calc time.\n",
    "    # Matches always the closest Helix and sheet anchors, respectively, regardless of label.\n",
    "    mobile_anchor_coords = get_anchor_coords(mobile_pdb, mobile_anchors, multistate=True)\n",
    "    #distances = pd.DataFrame()\n",
    "    t_anchor_names = list(template_anchor_coords.keys())\n",
    "    t_coord = list(template_anchor_coords.values())\n",
    "    #print(t_coord)\n",
    "    anchor_occupied = np.zeros(shape=(len(template_anchor_coords.keys())), dtype=bool)\n",
    "    min_dists = []\n",
    "    matched_anchors = []\n",
    "    double_keys = []\n",
    "\n",
    "    mob_coords = list(mobile_anchor_coords.values())\n",
    "    n_template_keys = len(list(template_anchor_coords.keys()))\n",
    "\n",
    "    for m_coord in mob_coords:\n",
    "        distances = [space_distance(m_coord, coords) for coords in t_coord]\n",
    "        min_idx = np.argmin(distances)\n",
    "        if anchor_occupied[min_idx] == True:\n",
    "            double_keys.append(t_anchor_names[min_idx])\n",
    "        anchor_occupied[min_idx] = True\n",
    "        matched_anchors.append(t_anchor_names[min_idx])\n",
    "        min_dists.append(distances[min_idx])\n",
    "\n",
    "    if len(double_keys) == 0:\n",
    "        return dict(zip(mobile_anchors.keys(), matched_anchors)), dict(zip(mobile_anchors.keys(), min_dists))\n",
    "\n",
    "    # If anchors are present multiple times, delete the further distant entry and make None, None\n",
    "    for doublet in double_keys:\n",
    "        #print(f'{doublet = }, {matched_anchors = }, {type(matched_anchors) = }')\n",
    "        indices = [i for i,x in enumerate(matched_anchors) if doublet == x]\n",
    "        minimum = np.argmin(np.array(min_dists)[indices])\n",
    "        indices.remove(indices[minimum])\n",
    "        # Only the non-minimum values will remain in the indices list that need to be re-evaluated\n",
    "        newdists = np.empty(shape=(n_template_keys))\n",
    "        for idx in indices:\n",
    "\n",
    "            for i, coord in enumerate(t_coord):\n",
    "                if anchor_occupied[i] == True:\n",
    "                    newdists[i] = None\n",
    "                    continue\n",
    "                newdists[i] = space_distance(list(mobile_anchor_coords.values())[idx], coord)\n",
    "            newmindist = np.min(newdists)\n",
    "            if newmindist < threshold:\n",
    "                print('Found Alternative')\n",
    "                anchor_occupied[np.argmin(newdists)] = True\n",
    "                min_dists[idx] = newmindist\n",
    "                matched_anchors[idx] = list(mobile_anchor_coords.keys())[idx]\n",
    "            else: \n",
    "                #print('Removed double occurrence')\n",
    "                min_dists[idx] = None\n",
    "                matched_anchors[idx] = ''\n",
    "\n",
    "    return dict(zip(mobile_anchors.keys(), matched_anchors)), dict(zip(mobile_anchors.keys(), min_dists))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_family(name):\n",
    "    queries = [('AGR..', name, lambda x: x[-1][-2:]), #\n",
    "                ('ADGR..', name, lambda x: x[-1][-2:]), \n",
    "                ('cadher.*receptor.', name.lower(), lambda x: f\"C{x[-1][-1]}\"),\n",
    "                ('cels?r.', name.lower(), lambda x: f\"C{x[-1][-1]}\"), \n",
    "                ('latrophilin.*protein-?\\d', name.lower(), lambda x: f\"L{x[-1][-1]}\"),\n",
    "                ('latrophilin-?\\d', name.lower(), lambda x: f\"L{x[-1][-1]}\"),\n",
    "                ('GP?R133', name.upper(),lambda x: 'D1'),\n",
    "                ('GP?R126', name.upper(),lambda x: 'G6'),\n",
    "                ('GP?R?124', name.upper(),lambda x: 'A2'),\n",
    "                ('GP?R?125', name.upper(),lambda x: 'A3'),\n",
    "                ('GP?R112', name.upper(),lambda x: 'G4'),\n",
    "                ('GP?R116', name.upper(),lambda x: 'F5'),\n",
    "                ('GP?R144', name.upper(),lambda x: 'D2'),\n",
    "                ('ag-?.*-?coupled-?receptor-?.-?\\d', name.lower(),lambda x: x[-1].replace('-','')[-2:].upper()),\n",
    "                ('brain-?specific-?angiogenesis-?inhibitor-?\\d', name.lower(), lambda x: f\"B{x[-1][-1]}\"),\n",
    "                ('emr\\d', name.lower(), lambda x: f\"E{x[-1][-1]}\"),\n",
    "                ]\n",
    "    for pattern, searchstring, output in queries:\n",
    "        match = re.findall(pattern, searchstring)\n",
    "        if match != []:\n",
    "            #if output(match) == '': print(name)\n",
    "            return output(match)\n",
    "    return 'X'\n",
    "\n",
    "fam_list = [get_family(gain.name) for gain in valid_collection.collection]\n",
    "name_list = [gain.name for gain in valid_collection.collection]\n",
    "subfam_list = [x[0] for x in fam_list]\n",
    "receptors, counts  = np.unique(fam_list, return_counts=True)\n",
    "r_list = list(zip(receptors,counts))\n",
    "print(r_list)\n",
    "print(receptors)\n",
    "fam_counts = {}\n",
    "for prot in fam_list:\n",
    "    fam = prot[0]\n",
    "    if fam not in fam_counts.keys():\n",
    "        fam_counts[fam] = 0\n",
    "    fam_counts[fam] += 1\n",
    "\n",
    "print(fam_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match everything for each subfamily.\n",
    "#print(subfam_list)\n",
    "\n",
    "y = len(r_list)\n",
    "# famstring\n",
    "template_ids = list(templates.keys())\n",
    "\n",
    "t_occupancies = {}\n",
    "t_distances = {}\n",
    "unmatched = {}\n",
    "unmatched_counters = {}\n",
    "for t_id in template_ids[1:]:\n",
    "    t_anchors = template_anchors[t_id]\n",
    "    t_quality = template_quality[t_id]\n",
    "    t_folder = templates[t_id][-1]\n",
    "    n_anch = len(t_anchors.keys())\n",
    "    u_list = np.zeros(shape=(y), dtype=dict)\n",
    "    u_counters = np.zeros(shape=(y), dtype=int)\n",
    "    print(t_anchors)\n",
    "    anchor_index = {k:i for i, k in enumerate(t_anchors.keys())}\n",
    "    assigned_anchor_freq = np.zeros(shape=(len(receptors),n_anch))\n",
    "    all_anchor_averages = np.full(shape=(y,n_anch), fill_value=None)\n",
    "    all_anchor_occupancy = np.zeros(shape=(y,n_anch))\n",
    "\n",
    "    for fam_idx, r in enumerate(receptors[:1]):# in enumerate('ABCDEFGLVX'):\n",
    "        gain_subset = [ gain for i, gain in enumerate(valid_collection.collection) if fam_list[i] == r]#subfam_list[i]==r ]\n",
    "        gain_idx_list = [ i for i,gain in enumerate(fam_list) if gain == r ]\n",
    "        #print(r, len(gain_subset))\n",
    "\n",
    "        element_occupation = {k:0 for k in t_anchors.keys()}\n",
    "\n",
    "        for key, val in element_occupation.items():\n",
    "            assigned_anchor_freq[fam_idx, anchor_index[key]] = float(val)/len(gain_subset)\n",
    "        for gain in gain_subset: print(gain.name, [hel for hel in gain.sda_helices if hel[0] < gain.subdomain_boundary-gain.start])\n",
    "        fam_distances, fam_matched_anchors, unmatched_elements, unmatched_counter = tf.gain_set_to_template(gain_subset, \n",
    "                                                                                                            gain_idx_list, \n",
    "                                                                                                            t_anchors, \n",
    "                                                                                                            t_folder, \n",
    "                                                                                                            penalty=None,\n",
    "                                                                                                            subdomain='sda',\n",
    "                                                                                                            return_unmatched_mode='all', \n",
    "                                                                                                            debug=True)\n",
    "        #print(unmatched_elements)\n",
    "        mean_dist = np.empty(shape=(n_anch))\n",
    "        occ = np.zeros(shape=(n_anch))\n",
    "        \n",
    "        for j in range(n_anch):\n",
    "            occ_values = np.array([d for d in fam_distances[:,j] if d is not None])\n",
    "            if len(occ_values) != 0:\n",
    "                mean_dist[j] = round(np.mean(occ_values), 3)\n",
    "                occ[j] = round(np.count_nonzero(fam_distances[:,j])/len(gain_idx_list), 3)\n",
    "        all_anchor_averages[fam_idx,:] = mean_dist #np.mean(fam_distances, axis=0)\n",
    "        all_anchor_occupancy[fam_idx,:] = occ\n",
    "        u_counters[fam_idx] = unmatched_counter\n",
    "        u_list[fam_idx] = unmatched_elements\n",
    "        #print(all_anchor_averages)\n",
    "        #print(all_anchor_occupancy)\n",
    "    print(u_list.shape) # u_list is a list of dicts.\n",
    "    print(type(u_list[0]), type(u_list[1]))\n",
    "    print(f\"Done with Template {t_id}.\\n\", \"_\"*30)\n",
    "\n",
    "    t_distances[t_id] = all_anchor_averages\n",
    "    t_occupancies[t_id] = all_anchor_occupancy\n",
    "    unmatched[t_id] = u_list\n",
    "    unmatched_counters[t_id] = u_counters\n",
    "#print(unmatched_counters)\n",
    "#print(type(unmatched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fit.\n",
    "\"\"\"for t_id in template_ids:\n",
    "    t_anchors = template_anchors[t_id]\n",
    "    n_anch = len(t_anchors.keys())\n",
    "    t_anchor_freqs = t_occupancies[t_id]\n",
    "    u_counters = unmatched_counters[t_id]\n",
    "    fig = plt.figure(figsize=[6,10], facecolor='w')\n",
    "    #plt.yticks(ticks = range(ydim), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "    plt.title(f\"Template Match for : {t_id}\")\n",
    "    plt.yticks(ticks = range(len(r_list)), labels= [f'{i[0]}:{i[1]} (u:{u_counters[x]})' for x,i in enumerate(r_list)])\n",
    "    plt.xticks(ticks = range(n_anch), labels=t_anchors.keys(), rotation=90)\n",
    "    plt.imshow(t_anchor_freqs, cmap='summer')\n",
    "    cbar = plt.colorbar(shrink=0.5)\n",
    "    cbar.set_label('Relative Occupancy')\n",
    "    ydim = len(r_list)\n",
    "    for y in range(ydim):\n",
    "        for x in range(n_anch):\n",
    "            if t_anchor_freqs[y,x] > 0.001:\n",
    "                pass\n",
    "            else:\n",
    "                plt.text(x,y,'x', horizontalalignment='center', verticalalignment='center', fontsize=18,color='k')\n",
    "    plt.savefig(f'{t_id}_occupancy2.png', dpi=300)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"../unmatched_full.txt\", 'w')\n",
    "outfile.write(\"Temp  Grp   nGrp  alnIdx  nNoMat  avgLen  %unmat\\n\")\n",
    "#print(r_list, len(r_list))\n",
    "for t_id in template_ids[1:]:\n",
    "    u_list = unmatched[t_id]\n",
    "\n",
    "    #print(u_dict)\n",
    "    for rud_idx, receptor_unmatched_dict in enumerate(u_list[:1]):\n",
    "        e_length = []\n",
    "        e_res = []\n",
    "        res_len = {}\n",
    "        all_items = []\n",
    "        print(receptor_unmatched_dict)\n",
    "        for lst in receptor_unmatched_dict.values():\n",
    "            lengths = [int(i[2]) for i in lst]\n",
    "            e_length = e_length+lengths\n",
    "            e_res += [i[0] for i in lst]\n",
    "            \n",
    "            for i in lst:\n",
    "                if int(i[0]) not in res_len.keys():\n",
    "                    res_len[int(i[0])] = [i[2]]\n",
    "                else:\n",
    "                    res_len[int(i[0])].append(i[2])\n",
    "                all_items.append(i)\n",
    "        res_av_len = {k:np.average(v) for k,v in res_len.items()}\n",
    "\n",
    "        #print(np.average(e_length))\n",
    "        resid, ct = np.unique(e_res, return_counts=True)\n",
    "        where_many = {resid[k]:c for k,c in enumerate(ct) if c > 5}\n",
    "        #print(where_many)\n",
    "        #plt.bar(resid, ct)\n",
    "        \n",
    "        sel_length = r_list[rud_idx][1]\n",
    "        for idx, count in enumerate(ct):\n",
    "            if count > 0.1*sel_length and res_av_len[resid[idx]] > 3.5: # more than 10% of selection have this\n",
    "                outfile.write(f\"{t_id}{r_list[rud_idx][0].rjust(7)}{str(r_list[rud_idx][1]).rjust(8)}\")\n",
    "                outfile.write(f\"{str(resid[idx]).rjust(8)}{str(count).rjust(8)}{str(round(res_av_len[resid[idx]],1)).rjust(8)}{str(round(count*100/sel_length)).rjust(7)}%   \")\n",
    "                for value in all_items[idx]:\n",
    "                    outfile.write(str(value).rjust(8))#plt.bar(resid[idx], count)\n",
    "                outfile.write(\"\\n\")\n",
    "                #plt.annotate(f\"{round(res_av_len[resid[idx]],1)}\", (resid[idx],count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_anchor_averages.shape)\n",
    "#allsse =[f'H{a}' for a in range(1,9)]+[f'S{a}' for a in range(1,14)]\n",
    "allsse =[f'H{a}' for a in range(1,8)]+[f'S{a}' for a in range(1,20)]\n",
    "df = pd.DataFrame(data=all_anchor_averages, index = r_list, columns=allsse)\n",
    "print(df)\n",
    "docc = pd.DataFrame(data=all_anchor_occupancy, index = r_list, columns=allsse)\n",
    "print(docc)\n",
    "element_occupation = {k:0 for k in allsse}\n",
    "n_hels = {np.zeros(shape=14432)}\n",
    "n_sheets = np.zeros(shape=14432)\n",
    "for i, gain in enumerate(valid_collection.collection):\n",
    "    _, cd, _, _ = gain.create_indexing(anchors, anchor_occupation, anchor_dict,split_mode='double', silent=True)\n",
    "    h,s = 0,0\n",
    "    for k in cd.keys():\n",
    "        element_occupation[k[:-3]] += 1 \n",
    "        if k.startswith('H'): h += 1\n",
    "        elif k.startswith('S'): s +=1\n",
    "    n_hels[i] = h\n",
    "    n_sheets[i] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "for fam_idx, r in enumerate(receptors[:1]):# in enumerate('ABCDEFGLVX'):\n",
    "    gain_subset = [ gain for i, gain in enumerate(valid_collection.collection) if fam_list[i] == r]#subfam_list[i]==r ]\n",
    "    for gain in gain_subset:\n",
    "        id_list.append(gain.name.split(\"-\")[0])\n",
    "    file_str = [(find_pdb(i, '../all_pdbs/')) for i in id_list]\n",
    "print(\"pymol\",\" \".join(file_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ydim = 40\n",
    "fig = plt.figure(figsize=[8,ydim/3], facecolor='w')\n",
    "plt.imshow(docc, cmap='spring')\n",
    "#plt.yticks(ticks = range(ydim), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "plt.yticks(ticks = range(ydim), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "plt.xticks(ticks = range(n_anch), labels= allsse, rotation=90)\n",
    "\n",
    "all_anchor_averages # 10,21\n",
    "for y in range(ydim):\n",
    "    for x in range(n_anch):\n",
    "        if all_anchor_averages[y,x] > 0.0001:\n",
    "            plt.text(x,y, round(all_anchor_averages[y,x], 2), horizontalalignment='center', verticalalignment='center', fontsize=7,color='k', rotation=45)\n",
    "cbar = plt.colorbar(shrink=float(8/ydim))\n",
    "cbar.set_label('Relative Occupancy')\n",
    "plt.vlines(6.5,-0.5,ydim-0.5, color='k', linewidth=1.5)\n",
    "plt.savefig('identity_receptor_anchor_occupancy.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=[8,ydim/3], facecolor='w')\n",
    "#plt.imshow(df, cmap='summer')\n",
    "im_data = np.zeros(shape=(ydim, n_anch))\n",
    "\n",
    "print(all_anchor_averages.shape)\n",
    "#plt.yticks(ticks = range(10), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "plt.yticks(ticks = range(ydim), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "plt.xticks(ticks = range(n_anch), labels= allsse, rotation=90)\n",
    "#all_anchor_averages # 10,21\n",
    "for y in range(ydim):\n",
    "    for x in range(n_anch):\n",
    "        if all_anchor_averages[y,x] > 0.001:\n",
    "            im_data[y,x] = all_anchor_averages[y,x]\n",
    "        else:\n",
    "            plt.text(x,y,'x', horizontalalignment='center', verticalalignment='center', fontsize=20,color='k')\n",
    "            #patches.Rectangle((x,y), 1, 1, linewidth=0.5, edgecolor='k', facecolor='w')\n",
    "plt.imshow(im_data, cmap='summer', vmax=3)\n",
    "            #plt.text(x,y, round(all_anchor_averages[y,x], 2), horizontalalignment='center', verticalalignment='center', fontsize=7,color='k', rotation=45)\n",
    "cbar = plt.colorbar(shrink=float(8)/ydim)\n",
    "cbar.set_label(r'Closest Anchor Residue Distance [$\\AA$]')\n",
    "plt.vlines(6.5,-0.5,ydim-0.5, color='k', linewidth=1.5)\n",
    "plt.savefig('identity_receptor_anchor_distance.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydim = 40\n",
    "fig = plt.figure(figsize=[8,ydim/3], facecolor='w')\n",
    "\n",
    "#plt.yticks(ticks = range(ydim), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "plt.yticks(ticks = range(ydim), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "plt.xticks(ticks = range(n_anch), labels= allsse, rotation=90)\n",
    "occ_values = df.to_numpy()\n",
    "is_off = np.zeros(shape=(40,26))\n",
    "print(docc.shape, all_anchor_averages.shape)\n",
    "#all_anchor_averages # 10,21\n",
    "for y in range(ydim):\n",
    "    for x in range(n_anch):\n",
    "        if all_anchor_averages[y,x] is not None and all_anchor_averages[y,x] > 1.5 and occ_values[y,x] > 0.1:\n",
    "            is_off[y,x] = 1\n",
    "            plt.text(x,y, round(all_anchor_averages[y,x], 2), horizontalalignment='center', verticalalignment='center', fontsize=7,color='k', rotation=45)\n",
    "plt.imshow(is_off, cmap='spring')\n",
    "cbar = plt.colorbar(shrink=float(8/ydim))\n",
    "cbar.set_label('Relative Occupancy')\n",
    "plt.vlines(6.5,-0.5,ydim-0.5, color='k', linewidth=1.5)\n",
    "#plt.savefig('identity_receptor_anchor_occupancy.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_stats = np.zeros(shape = (n_anch, 2))\n",
    "\n",
    "for fam_idx, r in enumerate(receptors):# in enumerate('ABCDEFGLVX'):\n",
    "    print(r)\n",
    "    gain_subset = [ gain for i, gain in enumerate(valid_collection.collection) if fam_list[i] == r]#subfam_list[i]==r ]\n",
    "    gain_idx_list = [ i for i,gain in enumerate(fam_list) if gain == r ]\n",
    "    n_sse = [[len(gain.sda_helices), len(gain.sdb_sheets)] for gain in gain_subset] # (n_struc, 2)\n",
    "    n_strucs = np.mean(np.array(n_sse), axis=0)\n",
    "    print(r, round(n_strucs[0], 2), round(n_strucs[1],2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new Anchor management.\n",
    "\n",
    "#\n",
    "# Use the \"max\" SDA / SDB template for generating the new anchors \n",
    "# get the center residue index for each template SSE\n",
    "for gain in valid_collection.collection:\n",
    "    if gain.name[:10] == 'A0A7K7IHI9': #SDA\n",
    "        hel_centers = []\n",
    "        for hel in gain.sda_helices: # each hel is a tuple\n",
    "            hel_centers.append( gain.start + int((hel[0]+hel[1])/2) )\n",
    "        hel_keys = [f'H{i+1}' for i in range(len(hel_centers))]\n",
    "        sda_centers = dict(zip(hel_keys, hel_centers))\n",
    "    if gain.name[:10] =='A0A3P9I6M5':\n",
    "        sheet_centers = []\n",
    "        for sheet in gain.sdb_sheets: # each hel is a tuple\n",
    "            #if sheet[1] - sheet[0] < 3:\n",
    "            #    print(gain.start+sheet[0], gain.start+sheet[1])\n",
    "            sheet_centers.append( gain.start + int((sheet[0]+sheet[1])/2) )\n",
    "        sheet_keys = [f'S{i+1}' for i in range(len(sheet_centers))]\n",
    "        sdb_centers = dict(zip(sheet_keys, sheet_centers))\n",
    "# Manually curated the centers to exclude two small strands in the CD between S6/S7 @ 707-708 and 711-713, respectively (low pLDDT here).\n",
    "sda_centers = {'H1': 313, 'H2': 328, 'H3': 359, 'H4': 383, 'H5': 409, 'H6': 424, 'H7': 444}\n",
    "sdb_centers = {'S1': 622, 'S2': 631, 'S3': 645, 'S4': 658, 'S5': 670, 'S6': 695, 'S7': 719, 'S8': 736, 'S9': 752, 'S10': 765, 'S11': 771, 'S12': 782, 'S13': 793}\n",
    "\n",
    "# Find closest residue to the center (GESAMT), note down the sequence, start, end of the matched SSE; write to FASTA\n",
    "    # A dict of dicts --> for each key, there is a dictionary inside sse_seqs['H1'][gain.name]:'seqlist'\n",
    "all_keys = list({**sda_centers, **sdb_centers}.keys())\n",
    "sse_seqs = {k:{} for k in all_keys}\n",
    "sse_extents = {k:{} for k in all_keys}\n",
    "unmatched = {k:0 for k in all_keys}\n",
    "unstructured = {k:0 for k in all_keys}\n",
    "\n",
    "for i, gain in enumerate(valid_collection.collection):\n",
    "        a_gesamt_file = f'../sda_template_aligned_files/sda_{i}.out'\n",
    "        b_gesamt_file = f'../sdb_template_aligned_files/sdb_{i}.out'\n",
    "\n",
    "        sda_matches = find_anchor_matches(a_gesamt_file, sda_centers, isTarget=False)\n",
    "        sdb_matches = find_anchor_matches(b_gesamt_file, sdb_centers, isTarget=False)\n",
    "        #print(sda_matches, sdb_matches)\n",
    "        hel_extents = np.full(shape = (gain.end-gain.start+1), fill_value=100)\n",
    "        she_extents = np.full(shape = (gain.end-gain.start+1), fill_value=100)\n",
    "        # Establish two matrices to match the respective residue to the index of its helix/sheet for easier matching\n",
    "        for i,element in enumerate(gain.sda_helices):\n",
    "            hel_extents[element[0]:element[1]] = i\n",
    "        for i,element in enumerate(gain.sdb_sheets):\n",
    "            she_extents[element[0]:element[1]] = i\n",
    "        # Match the corresponding closest residue to find the associated SSE with start, end and sequence\n",
    "        for sse, match in sda_matches.items():\n",
    "            if match[0] is None:\n",
    "                unmatched[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_index = hel_extents[match[0]-gain.start]\n",
    "\n",
    "            if sse_index == 100:\n",
    "                unstructured[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_extents[sse][gain.name] = gain.sda_helices[sse_index]\n",
    "            sse_seqs[sse][gain.name] = gain.sequence[gain.sda_helices[sse_index][0]:gain.sda_helices[sse_index][1]]\n",
    "        \n",
    "        for sse, match in sdb_matches.items():\n",
    "            if match[0] is None:\n",
    "                unmatched[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_index = she_extents[match[0]-gain.start]\n",
    "\n",
    "            if sse_index == 100:\n",
    "                unstructured[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_extents[sse][gain.name] = gain.sdb_sheets[sse_index]\n",
    "            sse_seqs[sse][gain.name] = gain.sequence[gain.sdb_sheets[sse_index][0]:gain.sdb_sheets[sse_index][1]]\n",
    "        \n",
    "for sse in all_keys:\n",
    "    with open(f'../sse_aln/{sse}.seqs.fa','w') as fa:\n",
    "        for name, seq in sse_seqs[sse].items():\n",
    "            fa.write(f'>{name}\\n{\"\".join(seq)}\\n')\n",
    "\n",
    "print(unmatched, '\\n', unstructured)\n",
    "#   Run MAFFT with each of the gathered sequences\n",
    "#   For each MAFFT\n",
    "#       Find the most conserved residue (Identity matrix)\n",
    "#       Set as new Anchor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for gain in valid_collection.collection:\n",
    "\n",
    "    if gain.name[:10] == 'A0A7K7IHI9': #SDA\n",
    "        sda_gain = gain\n",
    "    if gain.name[:10] =='A0A3P9I6M5': # SDB\n",
    "        sdb_gain = gain\n",
    "for i,k in enumerate(sda_centers.keys()):\n",
    "    kfile = glob.glob(f\"../sse_aln/{k}.aln.fa\")[0]\n",
    "    with open(kfile) as alnf:\n",
    "        x = alnf.readlines()[1].strip(\" \\n\")\n",
    "        kcutoff = len(x)\n",
    "    print(kcutoff)\n",
    "    aln = sse_func.read_alignment(kfile, cutoff=kcutoff)\n",
    "    h = sda_gain.sda_helices[i]\n",
    "    aln_matrix = np.array([list(seq) for seq in aln.values()])\n",
    "    kquality, kocc = calc_identity(aln_matrix)\n",
    "    '''    kquality = []\n",
    "    kocc = []\n",
    "    for col in range(aln_matrix.shape[1]):\n",
    "        chars, count = np.unique(aln_matrix[:,col], return_counts=True)\n",
    "        if chars[0] == '-':\n",
    "            q = count[1]\n",
    "        else:\n",
    "            q = count[0]\n",
    "        x = np.where(chars == '-')[0][0]\n",
    "        kocc.append(14435 - count[x])\n",
    "        kquality.append(q)'''\n",
    "    template_aln_seq = aln[sda_gain.name]\n",
    "    template_res_idx = np.argmax(kquality)\n",
    "    print(template_aln_seq, template_res_idx)\n",
    "    template_index = template_aln_seq[:template_res_idx+1]\n",
    "    t_res = template_aln_seq[template_res_idx]\n",
    "    print(template_index, t_res)\n",
    "    new = template_index\n",
    "\n",
    "    fig = plt.figure(figsize=[4,2], facecolor='w')\n",
    "    plt.bar(range(kcutoff), kquality)\n",
    "    plt.title(f'SDA TEMPLATE : {k}')\n",
    "    plt.xticks(ticks = range(kcutoff), labels=template_aln_seq, fontsize=5)\n",
    "    plt.savefig(f'../sse_aln/{k}.template1.png', dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "for i,k in enumerate(sdb_centers.keys()):\n",
    "    kfile = glob.glob(f\"../sse_aln/{k}.aln.fa\")[0]\n",
    "    with open(kfile) as alnf:\n",
    "        x = alnf.readlines()[1].strip(\" \\n\")\n",
    "        kcutoff = len(x)\n",
    "    print(kcutoff)\n",
    "    aln = sse_func.read_alignment(kfile, cutoff=kcutoff)\n",
    "    h = sdb_gain.sdb_sheets[i]\n",
    "    aln_matrix = np.array([list(seq) for seq in aln.values()])\n",
    "    kquality = []\n",
    "    kocc = []\n",
    "    kquality, kocc = calc_identity(aln_matrix)\n",
    "        \n",
    "    template_aln_seq = aln[sdb_gain.name]\n",
    "    template_res_idx = np.argmax(kquality)\n",
    "    print(template_aln_seq, template_res_idx)\n",
    "    template_index = template_aln_seq[:template_res_idx+1]\n",
    "    t_res = template_aln_seq[template_res_idx]\n",
    "    print(template_index, t_res)\n",
    "    new = template_index\n",
    "\n",
    "    fig = plt.figure(figsize=[4,2], facecolor='w')\n",
    "    plt.bar(range(kcutoff), kquality)\n",
    "    plt.title(f'SDB TEMPLATE : {k}')\n",
    "    plt.xticks(ticks = range(kcutoff), labels=template_aln_seq, fontsize=5)\n",
    "    plt.savefig(f'../sse_aln/{k}.template1.png', dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efcc3436bf700bf51081b251413b556e30c22be82f452601745119c8a669a2f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
