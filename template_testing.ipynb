{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "import glob, math, json, glob, re\n",
    "#from shutil import copyfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import logomaker\n",
    "# LOCAL IMPORTS\n",
    "#from indexing_classes import GPCRDBIndexing\n",
    "from gain_classes import GainDomain, GainCollection, Anchors, GPS\n",
    "import sse_func\n",
    "import matplotlib.pyplot as plt\n",
    "import template_finder as tf\n",
    "\n",
    "def calc_identity(aln_matrix, return_best_aa=False):\n",
    "    # This takes an alignment matrix with shape=(n_columns, n_sequences) and generates counts based on the identity matrix.\n",
    "    # Returns the highest non \"-\" residue count as the most conserved residue and its occupancy based on count(\"-\") - n_struc\n",
    "    n_struc = aln_matrix.shape[0]\n",
    "    quality = []\n",
    "    occ = []\n",
    "    aa = []\n",
    "    for col in range(aln_matrix.shape[1]):\n",
    "        chars, count = np.unique(aln_matrix[:,col], return_counts=True)\n",
    "        dtype = [('aa', 'S1'), ('counts', int)]\n",
    "        values = np.array(list(zip(chars,count)), dtype=dtype)\n",
    "        s_values = np.sort(values, order='counts')\n",
    "\n",
    "        if s_values[-1][0] == b'-':\n",
    "            q = s_values[-2][1]\n",
    "            aa.append(s_values[-2][0])\n",
    "        else:\n",
    "            q = s_values[-1][1]\n",
    "            aa.append(s_values[-1][0])\n",
    "        x = np.where(chars == '-')[0][0]\n",
    "        occ.append(n_struc - count[x])\n",
    "        quality.append(q)\n",
    "    if not return_best_aa:\n",
    "        return quality, occ\n",
    "    if return_best_aa:\n",
    "        return quality, occ, aa\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the valid GAIN domain Dataset and specify the pdb path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to the collection, we need the corresponding PDB files.\n",
    "valid_collection = pd.read_pickle(\"../valid_collection.pkl\")\n",
    "allpdbs = '../all_pdbs/*.pdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Data for the initially selected templates\n",
    "def get_gain(identifier, a_gain_collection):\n",
    "    for gain in a_gain_collection:\n",
    "        if identifier in gain.name:\n",
    "            return gain\n",
    "        \n",
    "def get_struc_aln_anchors(gain, aln_dict, subdomain='a', threshold=3):\n",
    "    aln_matrix = np.array([list(seq) for seq in aln_dict.values()])\n",
    "    # Get the identity scores from the alignment\n",
    "    quality, occ, aa = calc_identity(aln_matrix, return_best_aa=True)\n",
    "    # The columns here exactly correspond to the template sequence order\n",
    "    if subdomain.lower() == 'a':\n",
    "        sse = gain.sda_helices\n",
    "        d_string = \"HELIX \"\n",
    "        sse_type = \"H\"\n",
    "    elif subdomain.lower() == 'b':\n",
    "        sse = gain.sdb_sheets\n",
    "        d_string = \"STRAND\"\n",
    "        sse_type = \"S\"\n",
    "    else:\n",
    "        print(\"NO SUBDOMAIN specified. EXITING.\")\n",
    "    \n",
    "    anchor_quality = {}\n",
    "    anchors = {}\n",
    "    counter = 1\n",
    "\n",
    "    for i,element in enumerate(sse):\n",
    "        if element[1]-element[0] <= threshold:\n",
    "            print(\"Element length below threshold. Skipping.\", element)\n",
    "            continue\n",
    "        if subdomain =='a' and gain.start+element[0] > gain.subdomain_boundary:\n",
    "            print(\"Skipping Subdomain A Helix\", element)\n",
    "            continue\n",
    "\n",
    "        q = quality[element[0]:element[1]+1]\n",
    "        label = f'{sse_type}{counter}'\n",
    "        max_id = element[0]+np.argmax(q)\n",
    "        max_res = gain.sequence[max_id]\n",
    "\n",
    "        res_id = gain.start+max_id+1\n",
    "\n",
    "        print(f\"{d_string} #{i+1}: {max_res}{res_id} @ SSE residue {max_id-element[0]} | q = {np.max(q)} with res_idx {max_id} | MOST CONSERVED: {aa[max_id]} | PDB-res {gain.start+element[0]+1}-{gain.start+element[1]+1}\")\n",
    "        anchor_quality[label] = np.max(q)\n",
    "        anchors[label] = max_id\n",
    "        counter += 1\n",
    "        pdb_anchors = {v:k+gain.start+1 for v,k in anchors.items()}\n",
    "    print(\"__________\")\n",
    "    return anchors, anchor_quality, pdb_anchors\n",
    "\n",
    "def get_template_information(identifier, gain_collection, subdomain='a', threshold=3, no_input=True):\n",
    "    for gain in gain_collection.collection:\n",
    "        if identifier in gain.name:\n",
    "            print(gain.name, gain.start, gain.subdomain_boundary, gain.end, \"\\n\")\n",
    "\n",
    "            if subdomain.lower() == 'a':\n",
    "                sse = gain.sda_helices\n",
    "                d_string = \"HELIX \"\n",
    "                sse_type = \"H\"\n",
    "            elif subdomain.lower() == 'b':\n",
    "                sse = gain.sdb_sheets\n",
    "                d_string = \"STRAND\"\n",
    "                sse_type = \"S\"\n",
    "            else:\n",
    "                print(\"NO SUBDOMAIN specified. EXITING.\")\n",
    "        \n",
    "            #print(sse)\n",
    "            anchor_quality = {}\n",
    "            anchors = {}\n",
    "            counter = 1\n",
    "            aln_indices = []\n",
    "            for i,element in enumerate(sse):\n",
    "                if element[1]-element[0] <= threshold:\n",
    "                    print(\"Element length below threshold. Skipping.\", element)\n",
    "                    continue\n",
    "                if subdomain =='a' and gain.start+element[0] > gain.subdomain_boundary:\n",
    "                    print(\"Skipping Subdomain A Helix\", element)\n",
    "                    continue\n",
    "                label = f'{sse_type}{counter}'\n",
    "                q = [ gain.residue_quality[res] for res in range(element[0], element[1]+1)]\n",
    "                max_id = element[0]+np.argmax(q)\n",
    "                max_res = gain.sequence[max_id]\n",
    "                #aln_idx = gain.alignment_indices[max_id]\n",
    "                res_id = gain.start+max_id+1\n",
    "                print(f\"{d_string} #{i+1}: {max_res}{res_id} @ SSE residue {max_id-element[0]} | q = {np.max(q)} with res_idx {max_id} | {q} | {gain.start+element[0]}-{gain.start+element[1]}\")\n",
    "                if not no_input:\n",
    "                    confirm = input(f\"{d_string} #{i+1}: {max_res}{res_id} @ SSE re {max_id-element[0]} | q={np.max(q)} w res_idx {max_id} | {gain.start+element[0]}-{gain.start+element[1]}. Keep?\")\n",
    "                    if confirm.lower() != \"y\":\n",
    "                        print(\"Skipping this anchor.\");continue\n",
    "                anchor_quality[label] = np.max(q)\n",
    "                anchors[label] = max_id\n",
    "                aln_indices.append(gain.alignment_indices[max_id])\n",
    "                counter += 1\n",
    "            pdb_anchors = {v:k+gain.start+1 for v,k in anchors.items()}\n",
    "            print(\"__________\")\n",
    "            return anchors, anchor_quality, aln_indices, pdb_anchors\n",
    "_,_,_,_ = get_template_information('A0A6G1Q0B9', valid_collection, 'a')\n",
    "#get_template_information('A0A3P8S994', valid_collection, 'b')\n",
    "        ##_, cd, _, _ = gain.create_indexing(precalc_anchors, precalc_anchor_occupation, precalc_anchor_dict)\n",
    "        #_, cd, _, _ = gain.create_indexing(anchors, anchor_occupation, anchor_dict)\n",
    "        #ac =  {k[:-3]:gain.start+v for k,v in cd.items()}\n",
    "        #print(ac)\n",
    "        #print(gain.Anchors.alignment_indices )\n",
    "        #print(gain.Anchors.gain_residues)\n",
    "        #print(gain.start)\n",
    "        #qq =[ quality[v] for v in gain.alignment_indices[324-gain.start:331-gain.start]]\n",
    "        #print(qq)\n",
    "        #print(gain.alignment_indices[324-gain.start:331-gain.start])\n",
    "# pre:  'H1': 309, 'H2': 324, 'H3': 358, 'H4': 385, 'H5': 410, 'H6': 421, 'H8': 438\n",
    "# cons:            'H1': 317, 'H2': 358, 'H3': 377, 'H4': 415, 'H6': 425, 'H7': 443,\n",
    "\n",
    "# pre:  'S1': 621, 'S2': 628, 'S3': 643, 'S4': 669,                         'S7': 717,  'S8': 734,  'S9': 750, 'S10': 764, 'S11': 770, 'S12': 782, 'S13': 793\n",
    "# cons: 'S1': 621, 'S3': 629, 'S4': 644, 'S5': 671, 'S7': 697, 'S10': 712, 'S11': 717, 'S13': 736, 'S14': 753, 'S15': 764, 'S16': 770, 'S18': 784, 'S19': 793"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a GESAMT bashfile for pairwise aln to each subdomain. Output the resulting PDB into respective folder\n",
    "#SDB TEMPLATE\n",
    "import shutil\n",
    "sdb_r1_template = {'E5b':'A0A3P8S994-A0A3P8S994_AMPPE-AGRE5b,duplicate2-Amphiprion_percula'}\n",
    "sdb_templates = {'E5b':'A0A3P8S994-A0A3P8S994_AMPPE-AGRE5b,duplicate2-Amphiprion_percula',\n",
    "                 #'G1b':'A0A7L3GD10-A0A7L3GD10_9AVES-AGRG1-Anhinga_rufa', \n",
    "                 #'G3b':'A0A3Q7QGV6-A0A3Q7QGV6_CALUR-AGRG3-likeisoformX3-Callorhinus_ursinus',\n",
    "                 'G5b':'A0A6J3IBI5-A0A6J3IBI5_SAPAP-AGRG5-Sapajus_apella'}\n",
    "# SDA TEMPLATEs\n",
    "sda_r1_templates = {'A': 'A0A2Y9F628-A0A2Y9F628_PHYMC-AGRA3isoformX1-Physeter_macrocephalus', \n",
    "                    'B': 'A0A4W6DVA0-A0A4W6DVA0_LATCA-AGRB1b-Lates_calcarifer', \n",
    "                    'C': 'A0A7K6E127-A0A7K6E127_9PASS-CELR3protein-Grantiella_picta.', \n",
    "                    'D': 'A0A1A7WJQ6-A0A1A7WJQ6_9TELE-GR144-Iconisemion_striatum.', \n",
    "                    'E': 'A0A3P8S994-A0A3P8S994_AMPPE-AGRE5b,duplicate2-Amphiprion_percula', \n",
    "                    'F': 'A0A452IH20-A0A452IH20_9SAUR-AGRF5-Gopherus_agassizii', \n",
    "                    'G': 'A0A1W4WJB1-A0A1W4WJB1_AGRPL-AGRG6-likeisoformX1-Agrilus_planipennis', \n",
    "                    'L': 'A0A452HCU9-A0A452HCU9_9SAUR-AGRL3-Gopherus_agassizii', \n",
    "                    'V': 'A0A6Q2XYK2-A0A6Q2XYK2_ESOLU-AGRV1-Esox_lucius',\n",
    "                    'X': \"A0A6F9A857-A0A6F9A857_9TELE-Uncharacterizedprotein-Coregonus_sp._'balchen'.\"}\n",
    "\n",
    "sda_r2_templates = {'G1': 'A0A7L3GD10-A0A7L3GD10_9AVES-AGRG1-Anhinga_rufa', \n",
    "                    'G2': 'A0A2K5MG19-A0A2K5MG19_CERAT-AGRG2-Cercocebus_atys', \n",
    "                    'G3': 'A0A3Q7QGV6-A0A3Q7QGV6_CALUR-AGRG3-likeisoformX3-Callorhinus_ursinus', \n",
    "                    'G4': 'A0A2I4CCH8-A0A2I4CCH8_9TELE-AGRG4-like-Austrofundulus_limnaeus.', \n",
    "                    'G5': 'A0A6J3IBI5-A0A6J3IBI5_SAPAP-AGRG5-Sapajus_apella', \n",
    "                    'G6': 'A0A6P7HB06-A0A6P7HB06_9TELE-AGRG6isoformX6-Parambassis_ranga', \n",
    "                    #'G6.1': 'F6QI92-F6QI92_CALJA-AGRG6-Callithrix_jacchus', \n",
    "                    #'G6.2':'A0A7J7WUN2-A0A7J7WUN2_MYOMY-AGRG6-Myotis_myotis',\n",
    "                    'G7': 'A0A2K5Y1I7-A0A2K5Y1I7_MANLE-AGRG7-Mandrillus_leucophaeus', \n",
    "                    'E1': 'A0A2I2YJG7-A0A2I2YJG7_GORGO-AGRE1-Gorilla_gorilla_gorilla', \n",
    "                    'E2': 'A0A2Y9QG39-A0A2Y9QG39_TRIMA-AGRE2isoformX3-Trichechus_manatus_latirostris', \n",
    "                    'E3': 'A0A2Y9M464-A0A2Y9M464_DELLE-AGRE3isoformX1-Delphinapterus_leucas', \n",
    "                    'E4': 'A0A6J3FRL0-A0A6J3FRL0_SAPAP-putativeAGRE4PisoformX1-Sapajus_apella', \n",
    "                    'E5': 'G1TKX5-G1TKX5_RABIT-AGRE5-Oryctolagus_cuniculus', \n",
    "                    #'E5.1': 'A0A2R9CT02-A0A2R9CT02_PANPA-AGRE5-Pan_paniscus',\n",
    "                    #'E5.2': 'F6PLI2-F6PLI2_CANLF-AGRE5-Canis_lupus_familiaris',\n",
    "                    'F2': 'A0A452SUX4-A0A452SUX4_URSAM-AGRF2-Ursus_americanus', \n",
    "                    #'F2.1':'A0A3Q0CU45-A0A3Q0CU45_MESAU-AGRF2-Mesocricetus_auratus',  \n",
    "                    #'F2.2':'E2RAG2-E2RAG2_CANLF-AGRF2-Canis_lupus_familiaris',\n",
    "                    'F4': 'W5PQ70-W5PQ70_SHEEP-AGRF4-Ovis_aries', \n",
    "                    'F5': 'A0A7L3N0A5-A0A7L3N0A5_9AVES-AGRF5protein-Oreotrochilus_melanogaster.', \n",
    "                    #'F5.1':'A0A1U7SCS2-A0A1U7SCS2_ALLSI-AGRF5isoformX1-Alligator_sinensis',\n",
    "                    #'F5.2':'A0A7K5GSD3-A0A7K5GSD3_9AVES-AGRF5protein-Chunga_burmeisteri',\n",
    "                    'D1': 'A0A369SLT5-A0A369SLT5_9METZ-AGRD1-Trichoplax_sp._H2.', \n",
    "                    'L4': 'A0A7L3KTA8-A0A7L3KTA8_9PASS-AGRL4protein-Drymodes_brunneopygia.'}\n",
    "\n",
    "sda_templates = {\n",
    "    'A': 'A0A2Y9F628-A0A2Y9F628_PHYMC-AGRA3isoformX1-Physeter_macrocephalus',\n",
    "    'C': 'A0A7K6E127-A0A7K6E127_9PASS-CELR3protein-Grantiella_picta.', \n",
    "    'D': 'A0A1A7WJQ6-A0A1A7WJQ6_9TELE-GR144-Iconisemion_striatum.', \n",
    "    'E1': 'A0A2I2YJG7-A0A2I2YJG7_GORGO-AGRE1-Gorilla_gorilla_gorilla', \n",
    "    'E5': 'G1TKX5-G1TKX5_RABIT-AGRE5-Oryctolagus_cuniculus', \n",
    "    'F2': 'A0A452SUX4-A0A452SUX4_URSAM-AGRF2-Ursus_americanus', \n",
    "    'F4': 'W5PQ70-W5PQ70_SHEEP-AGRF4-Ovis_aries', \n",
    "    'G7': 'A0A2K5Y1I7-A0A2K5Y1I7_MANLE-AGRG7-Mandrillus_leucophaeus', \n",
    "    'L': 'A0A452HCU9-A0A452HCU9_9SAUR-AGRL3-Gopherus_agassizii', \n",
    "    'L4': 'A0A7L3KTA8-A0A7L3KTA8_9PASS-AGRL4protein-Drymodes_brunneopygia.',\n",
    "    'V': 'A0A6Q2XYK2-A0A6Q2XYK2_ESOLU-AGRV1-Esox_lucius'\n",
    "}\n",
    "\n",
    "def find_pdb(name, pdb_folder):\n",
    "    identifier = name.split(\"-\")[0]\n",
    "    target_pdb = glob.glob(f\"{pdb_folder}/*{identifier}*.pdb\")[0]\n",
    "    return target_pdb\n",
    "\n",
    "for t_name, sdb_template in sdb_r1_template.items():\n",
    "    sdb_template_pdb = find_pdb(sdb_template, '../calc_templates')#'../r2_sdb_templates')\n",
    "    tf.run_gesamt_execution(valid_collection.collection, \n",
    "                                outfolder=f\"../{t_name}_{sdb_template.split('-')[0]}_sdb\",\n",
    "                                pdb_folder='../all_pdbs', \n",
    "                                domain='sdb', \n",
    "                                n_threads=6, \n",
    "                                max_struc=len(valid_collection.collection), \n",
    "                                no_run=True,\n",
    "                                template=sdb_template_pdb)\n",
    "\n",
    "\"\"\"for fam, prot in sda_templates.items():\n",
    "    identifier = prot.split(\"-\")[0]\n",
    "    current_template = find_pdb(prot, '../r2_sda_templates')\n",
    "\n",
    "    gesamt_outfolder = f'../{fam}_{identifier}_sda'\n",
    "\n",
    "    tf.run_gesamt_execution(valid_collection.collection, \n",
    "                            outfolder=gesamt_outfolder, \n",
    "                            pdb_folder='../all_pdbs', \n",
    "                            domain='sda', \n",
    "                            n_threads=6, \n",
    "                            max_struc=len(valid_collection.collection), \n",
    "                            no_run=False,\n",
    "                            template=current_template)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_templates = {'sdb':['A0A3P8S994-A0A3P8S994_AMPPE-AGRE5b,duplicate2-Amphiprion_percula', 'b', '../A0A3P8S994_sdb'],\n",
    "                    'A': ['A0A2Y9F628-A0A2Y9F628_PHYMC-AGRA3isoformX1-Physeter_macrocephalus', 'a', '../A0A2Y9F628_A_sda'], \n",
    "                    'B': ['A0A4W6DVA0-A0A4W6DVA0_LATCA-AGRB1b-Lates_calcarifer', 'a', '../A0A4W6DVA0_B_sda'], \n",
    "                    'C': ['A0A7K6E127-A0A7K6E127_9PASS-CELR3protein-Grantiella_picta.', 'a', '../A0A7K6E127_C_sda'], \n",
    "                    'D': ['A0A1A7WJQ6-A0A1A7WJQ6_9TELE-GR144-Iconisemion_striatum.', 'a', '../A0A1A7WJQ6_D_sda'], \n",
    "                    'E': ['A0A3P8S994-A0A3P8S994_AMPPE-AGRE5b,duplicate2-Amphiprion_percula', 'a', '../A0A3P8S994_E_sda'], \n",
    "                    'F': ['A0A452IH20-A0A452IH20_9SAUR-AGRF5-Gopherus_agassizii', 'a', '../A0A452IH20_F_sda'], \n",
    "                    'G': ['A0A1W4WJB1-A0A1W4WJB1_AGRPL-AGRG6-likeisoformX1-Agrilus_planipennis', 'a', '../A0A1W4WJB1_G_sda'], \n",
    "                    'L': ['A0A452HCU9-A0A452HCU9_9SAUR-AGRL3-Gopherus_agassizii', 'a', '../A0A452HCU9_L_sda'], \n",
    "                    'V': ['A0A6Q2XYK2-A0A6Q2XYK2_ESOLU-AGRV1-Esox_lucius', 'a', '../A0A6Q2XYK2_V_sda'],\n",
    "                    'X': [\"A0A6F9A857-A0A6F9A857_9TELE-Uncharacterizedprotein-Coregonus_sp._'balchen'.\", 'a', '../A0A6F9A857_X_sda']}\n",
    "\n",
    "# fuse sda_templates and sdb_templates together to be in the form of:\n",
    "#    'A': ['A0A2Y9F628-A0A2Y9F628_PHYMC-AGRA3isoformX1-Physeter_macrocephalus', 'a', '../A0A2Y9F628_A_sda'],\n",
    "templates = {}\n",
    "for t_id, t_name in {**sdb_templates, **sda_templates}.items():\n",
    "    if \"b\" in t_id: sd = 'b'\n",
    "    else:           sd = 'a'\n",
    "    \n",
    "    folder_string = f\"../{t_id}_{t_name.split('-')[0]}_sd{sd}\"\n",
    "    templates[t_id] = [t_name, sd, folder_string]\n",
    "\n",
    "print(\"Fused SDA and SDB templates into a single dictionary:\", templates)\n",
    "\n",
    "template_anchors = {}\n",
    "template_quality = {}\n",
    "template_indices = []\n",
    "\n",
    "all_indices = []\n",
    "for k, v in templates.items():\n",
    "    if 'b' in k:\n",
    "        threshold = 1\n",
    "    else:\n",
    "        threshold = 4\n",
    "    \n",
    "    template_gain = get_gain(v[0].split(\"-\")[0], valid_collection.collection)\n",
    "    #raw_anchors, a_qual, indices, anchors = get_template_information(v[0].split(\"-\")[0], valid_collection, v[1], threshold=threshold)\n",
    "    structural_alignment = tf.construct_structural_alignment(template_gain_domain=template_gain,\n",
    "                                                             list_of_gain_obj=valid_collection.collection,\n",
    "                                                             gain_indices=range(len(valid_collection.collection)),\n",
    "                                                             gesamt_folder=v[2],\n",
    "                                                             outfile=f'../{v[0].split(\"-\")[0]}_{k}.struc_aln.fa'\n",
    "                                                             )\n",
    "    #print(structural_alignment)\n",
    "    a_qual, indices, anchors = get_struc_aln_anchors(gain=template_gain,\n",
    "                                                     aln_dict=structural_alignment,\n",
    "                                                     subdomain=v[1],\n",
    "                                                     threshold=threshold)\n",
    "    print(a_qual, indices, anchors)\n",
    "    template_anchors[k] = anchors\n",
    "    template_quality[k] = a_qual\n",
    "    if 'b' in k:\n",
    "        continue\n",
    "    for i in indices:\n",
    "        template_indices.append(i)\n",
    "    all_indices.append(indices)\n",
    "\n",
    "print(template_anchors)\n",
    "print(template_quality)\n",
    "a,b  = np.unique(template_indices, return_counts=True)\n",
    "a_counts = dict(zip(a,b)) \n",
    "anchor_col = {x:i for i,x in enumerate(a)}\n",
    "print(anchor_col, len(a))\n",
    "hasAnchor = np.zeros(shape=(len(a), len(all_indices)))\n",
    "print(hasAnchor.shape)\n",
    "for fam_count, indices in enumerate(all_indices):\n",
    "    print(indices)\n",
    "    for v in indices:\n",
    "        print(v)\n",
    "        hasAnchor[anchor_col[v], fam_count] = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the pairwise GESAMT, we can use the resulting OUT and PDB files for analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a,b)\n",
    "# Plot the anchors with their respective alignment columns\n",
    "# This won't work with the StAl-based anchors, since we have one alignment for each template and the index is simply the residue index\n",
    "fig = plt.figure(figsize=[4,4], facecolor='w')\n",
    "im = plt.imshow(hasAnchor.T, cmap='gray')\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(range(len(a)))\n",
    "ax.set_yticks(range(len(templates.keys())))\n",
    "ax.set_xticklabels(a, rotation=90)\n",
    "ax.set_yticklabels(templates.keys())\n",
    "ax.set_xticks(np.arange(-.5, len(a), 1), minor=True)\n",
    "ax.set_yticks(np.arange(-.5, len(templates.keys()), 1), minor=True)\n",
    "plt.xlabel(\"Helix Anchor column\")\n",
    "plt.ylabel(\"Subfamily SDA Template\")\n",
    "ax.grid(which='minor', linewidth=2)\n",
    "plt.savefig(\"../r2_StAl_sda_template_anchors.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_list = [tf.get_agpcr_type(gain.name) for gain in valid_collection.collection]\n",
    "name_list = [gain.name for gain in valid_collection.collection]\n",
    "subfam_list = [x[0] for x in fam_list]\n",
    "receptors, counts  = np.unique(fam_list, return_counts=True)\n",
    "r_list = list(zip(receptors,counts))\n",
    "print(r_list)\n",
    "print(receptors)\n",
    "fam_counts = {}\n",
    "for prot in fam_list:\n",
    "    fam = prot[0]\n",
    "    if fam not in fam_counts.keys():\n",
    "        fam_counts[fam] = 0\n",
    "    fam_counts[fam] += 1\n",
    "\n",
    "print(fam_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match everything for each subfamily.\n",
    "#print(subfam_list)\n",
    "\n",
    "y = len(r_list)\n",
    "# famstring\n",
    "template_ids = list(templates.keys())\n",
    "\n",
    "t_occupancies = {}\n",
    "t_distances = {}\n",
    "unmatched = {}\n",
    "unmatched_counters = {}\n",
    "for t_id in template_ids:\n",
    "    t_anchors = template_anchors[t_id]\n",
    "    # SDB modified. t_anchors = {'S1': 324, 'S2': 335, 'S3': 353, 'S4': 359, 'S5': 381, 'S6': 409, 'S7': 414, 'S8': 436, 'S9': 453, 'S10': 459, 'S11': 470, 'S12': 478, 'S13': 487}\n",
    "    #t_anchors = {'S1': 324, 'S2': 335, 'S3': 349, 'S4': 359, 'S5': 381, 'S6': 409, 'S7': 414, 'S8': 436, 'S9': 453, 'S10': 459, 'S11': 466, 'S12': 478, 'S13': 487}\n",
    "    t_quality = template_quality[t_id]\n",
    "    t_folder = templates[t_id][-1]\n",
    "    n_anch = len(t_anchors.keys())\n",
    "    u_list = np.zeros(shape=(y), dtype=dict)\n",
    "    u_counters = np.zeros(shape=(y), dtype=int)\n",
    "    print(t_anchors)\n",
    "    anchor_index = {k:i for i, k in enumerate(t_anchors.keys())}\n",
    "    assigned_anchor_freq = np.zeros(shape=(len(receptors),n_anch))\n",
    "    all_anchor_averages = np.full(shape=(y,n_anch), fill_value=None)\n",
    "    all_anchor_occupancy = np.zeros(shape=(y,n_anch))\n",
    "\n",
    "    if 'b' in t_id: sd_string = 'sdb'\n",
    "    else: sd_string = 'sda'\n",
    "\n",
    "    for fam_idx, r in enumerate(receptors):# in enumerate('ABCDEFGLVX'):\n",
    "        gain_subset = [ gain for i, gain in enumerate(valid_collection.collection) if fam_list[i] == r ]#subfam_list[i]==r ]\n",
    "        gain_idx_list = [ i for i,gain in enumerate(fam_list) if gain == r ]\n",
    "        #print(r, len(gain_subset))\n",
    "\n",
    "        element_occupation = {k:0 for k in t_anchors.keys()}\n",
    "\n",
    "        for key, val in element_occupation.items():\n",
    "            assigned_anchor_freq[fam_idx, anchor_index[key]] = float(val)/len(gain_subset)\n",
    "        #DEBUG:\n",
    "        #for gain in gain_subset: \n",
    "        #    print(gain.name, [hel for hel in gain.sda_helices if hel[0] < gain.subdomain_boundary-gain.start])\n",
    "        \n",
    "        fam_distances, fam_matched_anchors, unmatched_elements, unmatched_counter = tf.gain_set_to_template(gain_subset, \n",
    "                                                                                                            gain_idx_list, \n",
    "                                                                                                            t_anchors, \n",
    "                                                                                                            t_folder, \n",
    "                                                                                                            penalty=None,\n",
    "                                                                                                            subdomain=sd_string,\n",
    "                                                                                                            return_unmatched_mode='all', \n",
    "                                                                                                            debug=False)\n",
    "        #print(\"DEBUG: ROOT\" , unmatched_elements)\n",
    "        mean_dist = np.empty(shape=(n_anch))\n",
    "        occ = np.zeros(shape=(n_anch))\n",
    "        \n",
    "        for j in range(n_anch):\n",
    "            occ_values = np.array([d for d in fam_distances[:,j] if d is not None])\n",
    "            if len(occ_values) != 0:\n",
    "                mean_dist[j] = round(np.mean(occ_values), 3)\n",
    "                occ[j] = round(np.count_nonzero(fam_distances[:,j])/len(gain_idx_list), 3)\n",
    "        all_anchor_averages[fam_idx,:] = mean_dist #np.mean(fam_distances, axis=0)\n",
    "        all_anchor_occupancy[fam_idx,:] = occ\n",
    "        u_counters[fam_idx] = unmatched_counter\n",
    "        u_list[fam_idx] = unmatched_elements\n",
    "        #print(all_anchor_averages)\n",
    "        #print(all_anchor_occupancy)\n",
    "    print(u_list.shape) # u_list is a list of dicts.\n",
    "    print(type(u_list[0]), type(u_list[1]))\n",
    "    print(f\"Done with Template {t_id}.\\n\", \"_\"*30)\n",
    "\n",
    "    t_distances[t_id] = all_anchor_averages\n",
    "    t_occupancies[t_id] = all_anchor_occupancy\n",
    "    unmatched[t_id] = u_list\n",
    "    unmatched_counters[t_id] = u_counters\n",
    "#print(unmatched_counters)\n",
    "#print(type(unmatched))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the OCCUPANCY\n",
    "for t_id in template_ids:\n",
    "    t_anchors = template_anchors[t_id]\n",
    "    # SDB modfied anchors : t_anchors = {'S1': 324, 'S2': 335, 'S3': 353, 'S4': 359, 'S5': 381, 'S6': 409, 'S7': 414, 'S8': 436, 'S9': 453, 'S10': 459, 'S11': 470, 'S12': 478, 'S13': 487}\n",
    "    #t_anchors = {'S1': 324, 'S2': 335, 'S3': 349, 'S4': 359, 'S5': 381, 'S6': 409, 'S7': 414, 'S8': 436, 'S9': 453, 'S10': 459, 'S11': 466, 'S12': 478, 'S13': 487}\n",
    "    n_anch = len(t_anchors.keys())\n",
    "    t_anchor_freqs = t_occupancies[t_id]\n",
    "    u_counters = unmatched_counters[t_id]\n",
    "    fig = plt.figure(figsize=[6,10], facecolor='w')\n",
    "    #plt.yticks(ticks = range(ydim), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "    plt.title(f\"Template Match for : {t_id}\")\n",
    "    plt.yticks(ticks = range(len(r_list)), labels= [f'{i[0]}:{i[1]} (u:{u_counters[x]})' for x,i in enumerate(r_list)])\n",
    "    #plt.yticks(ticks = range(len(r_list)), labels= [f'{i[0]}:{i[1]}' for x,i in enumerate(r_list)])\n",
    "    plt.xticks(ticks = range(n_anch), labels=t_anchors.keys(), rotation=90)\n",
    "    #plt.imshow(t_anchor_freqs, cmap='summer')\n",
    "    distances = np.zeros(shape=(len(r_list), n_anch), dtype=float)\n",
    "    #t_dists = t_distances[t_id]\n",
    "    #for i,l in enumerate(t_dists):\n",
    "    #    distances[i,:] = l\n",
    "    #plt.imshow(distances, cmap='spring')\n",
    "    plt.imshow(t_anchor_freqs, cmap='summer')\n",
    "    cbar = plt.colorbar(shrink=0.5)\n",
    "    cbar.set_label('Relative Occupancy')\n",
    "    #cbar.set_label('AA-Distance')\n",
    "    ydim = len(r_list)\n",
    "    for y in range(ydim):\n",
    "        for x in range(n_anch):\n",
    "            if t_anchor_freqs[y,x] > 0.001:\n",
    "                pass\n",
    "            else:\n",
    "                plt.text(x,y,'x', horizontalalignment='center', verticalalignment='center', fontsize=18,color='k')\n",
    "    plt.savefig(f'r2_StAl{t_id}_occ.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the DISTANCES\n",
    "for t_id in template_ids[-1:]:\n",
    "    t_anchors = template_anchors[t_id]\n",
    "    # SDB modfied anchors : t_anchors = {'S1': 324, 'S2': 335, 'S3': 353, 'S4': 359, 'S5': 381, 'S6': 409, 'S7': 414, 'S8': 436, 'S9': 453, 'S10': 459, 'S11': 470, 'S12': 478, 'S13': 487}\n",
    "    #t_anchors = {'S1': 324, 'S2': 335, 'S3': 349, 'S4': 359, 'S5': 381, 'S6': 409, 'S7': 414, 'S8': 436, 'S9': 453, 'S10': 459, 'S11': 466, 'S12': 478, 'S13': 487}\n",
    "    n_anch = len(t_anchors.keys())\n",
    "    t_anchor_freqs = t_occupancies[t_id]\n",
    "    u_counters = unmatched_counters[t_id]\n",
    "    fig = plt.figure(figsize=[6,10], facecolor='w')\n",
    "    #plt.yticks(ticks = range(ydim), labels= [f'ADGR{f}' for f in r_list])\n",
    "    plt.title(f\"Template Match for : {t_id}\")\n",
    "    #plt.yticks(ticks = range(len(r_list)), labels= [f'{i[0]}:{i[1]} (u:{u_counters[x]})' for x,i in enumerate(r_list)])\n",
    "    plt.yticks(ticks = range(len(r_list)), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "    plt.xticks(ticks = range(n_anch), labels=t_anchors.keys(), rotation=90)\n",
    "    #plt.imshow(t_anchor_freqs, cmap='summer')\n",
    "    distances = np.zeros(shape=(len(r_list), n_anch), dtype=float)\n",
    "    t_dists = t_distances[t_id]\n",
    "    for i,l in enumerate(t_dists):\n",
    "        distances[i,:] = l\n",
    "    plt.imshow(distances, cmap='spring')\n",
    "    #plt.imshow(t_anchor_freqs, cmap='summer')\n",
    "    cbar = plt.colorbar(shrink=0.5)\n",
    "    #cbar.set_label('Relative Occupancy')\n",
    "    cbar.set_label('AA-Distance')\n",
    "    ydim = len(r_list)\n",
    "    for y in range(ydim):\n",
    "        for x in range(n_anch):\n",
    "            if t_anchor_freqs[y,x] > 0.001:\n",
    "                pass\n",
    "            else:\n",
    "                plt.text(x,y,'x', horizontalalignment='center', verticalalignment='center', fontsize=18,color='k')\n",
    "    plt.savefig(f'r2_StAl{t_id}_dist.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"../r2_StAl_unmatched_full.txt\", 'w')\n",
    "outfile.write(\"Temp  Grp   nGrp  alnIdx  nNoMat  avgLen  %unmat\\n\")\n",
    "adress_matrix = [] # (my_template, col_names[id], value)\n",
    "col_names = {} # A1-1094: 0\n",
    "skip = 3\n",
    "\n",
    "#print(r_list, len(r_list))\n",
    "for t_index, t_id in enumerate(template_ids[skip:]):\n",
    "    u_list = unmatched[t_id]\n",
    "\n",
    "    #print(u_dict)\n",
    "    for rud_idx, receptor_unmatched_dict in enumerate(u_list):\n",
    "        \n",
    "        e_length = []\n",
    "        e_res = []\n",
    "        res_len = {}\n",
    "        all_items = []\n",
    "        #print(receptor_unmatched_dict)\n",
    "        for lst in receptor_unmatched_dict.values():\n",
    "            lengths = [int(i[2]) for i in lst]\n",
    "            e_length = e_length+lengths\n",
    "            e_res += [i[0] for i in lst]\n",
    "            \n",
    "            for i in lst:\n",
    "                if int(i[0]) not in res_len.keys():\n",
    "                    res_len[int(i[0])] = [i[2]]\n",
    "                else:\n",
    "                    res_len[int(i[0])].append(i[2])\n",
    "                all_items.append(i)\n",
    "        res_av_len = {k:np.average(v) for k,v in res_len.items()}\n",
    "\n",
    "        #print(np.average(e_length))\n",
    "        resid, ct = np.unique(e_res, return_counts=True)\n",
    "        where_many = {resid[k]:c for k,c in enumerate(ct) if c > 5}\n",
    "        #print(where_many)\n",
    "        #plt.bar(resid, ct)\n",
    "        \n",
    "        sel_length = r_list[rud_idx][1]\n",
    "        receptor_name = r_list[rud_idx][0]\n",
    "        for idx, count in enumerate(ct):\n",
    "            if count > 0.1*sel_length and res_av_len[resid[idx]] > 3.5: # more than 10% of selection have this\n",
    "\n",
    "                unindexed_freq = count/sel_length\n",
    "                column_name = f\"{receptor_name}-{str(resid[idx]).ljust(4)}\"\n",
    "                if column_name not in col_names.keys(): \n",
    "                    name_idx = len(col_names.keys())\n",
    "                    col_names[column_name] = name_idx\n",
    "                    \n",
    "                else:\n",
    "                    name_idx = col_names[column_name]\n",
    "                adress_matrix.append( (t_index, name_idx, unindexed_freq) )\n",
    "\n",
    "                outfile.write(f\"{t_id}{receptor_name.rjust(7)}{str(sel_length).rjust(8)}\")\n",
    "                outfile.write(f\"{str(resid[idx]).rjust(8)}{str(count).rjust(8)}{str(round(res_av_len[resid[idx]],1)).rjust(8)}{str(round(count*100/sel_length)).rjust(7)}%   \")\n",
    "                for value in all_items[idx]:\n",
    "                    outfile.write(str(value).rjust(8))#plt.bar(resid[idx], count)\n",
    "                outfile.write(\"\\n\")\n",
    "                #plt.annotate(f\"{round(res_av_len[resid[idx]],1)}\", (resid[idx],count))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can construct a DataFrame from this shizzle\n",
    "unmatched_matrix = np.zeros(shape=(len(template_ids)-skip, len(col_names.keys())), dtype=float)\n",
    "for item in adress_matrix:\n",
    "    unmatched_matrix[item[0],item[1]] = item[2]\n",
    "# Sort the matrix to match the receptor order in $receptors\n",
    "sorted_unmatched_matrix = np.zeros(shape=(len(template_ids)-skip, len(col_names.keys())), dtype=float)\n",
    "new_order = sorted(range(len(col_names.keys())), key=lambda k: list(col_names.keys())[k])\n",
    "#print(new_order)\n",
    "for data_col in range(len(col_names.keys())):\n",
    "    sorted_unmatched_matrix[:,data_col] = unmatched_matrix[:,new_order[data_col]]\n",
    "\n",
    "fig = plt.figure(figsize=[12,4], facecolor='w')\n",
    "ax = plt.gca()\n",
    "ax.imshow(sorted_unmatched_matrix, cmap='binary')\n",
    "plt.xticks(ticks = range(len(col_names.keys())), labels = sorted(col_names.keys()) ,rotation=90, fontsize=4, verticalalignment='top')\n",
    "plt.yticks(ticks = range(len(template_ids)-skip), labels=template_ids[skip:], fontsize=4)\n",
    "stored_ki = 'A1'\n",
    "r_bounds = []\n",
    "for i, ki in enumerate(sorted(col_names.keys())):\n",
    "    r = ki.split(\"-\")[0]\n",
    "    if r != stored_ki:\n",
    "        stored_ki = r\n",
    "        r_bounds.append(i)\n",
    "for b in r_bounds:\n",
    "    plt.vlines(b-0.5, -0.5, len(template_ids)-skip-0.5, color='r', linewidth=0.5)\n",
    "plt.savefig(\"../r2_StAl_template_match1.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[4,12], facecolor='w')\n",
    "plt.imshow(sorted_unmatched_matrix)\n",
    "stored_ki = 'A1'\n",
    "r_bounds = []\n",
    "for i, ki in enumerate(sorted(col_names.keys())):\n",
    "    r = ki.split(\"-\")[0]\n",
    "    if r != stored_ki:\n",
    "        stored_ki = r\n",
    "        r_bounds.append(i)\n",
    "\n",
    "#for b in r_bounds:\n",
    "#    plt.vlines(b, 0, len(template_ids)-1, color='w', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel2pymol(receptor, target_folder, find=False, stride=1):\n",
    "    id_list = []\n",
    "    for r in receptors:# in enumerate('ABCDEFGLVX'):\n",
    "        if r != receptor:\n",
    "            continue\n",
    "        gain_subset = [ gain for i, gain in enumerate(valid_collection.collection) if fam_list[i] == r]#subfam_list[i]==r ]\n",
    "        for gain in gain_subset:\n",
    "            id_list.append(gain.name.split(\"-\")[0])\n",
    "        file_str = [(find_pdb(i, target_folder)) for i in id_list]\n",
    "        print(\"pymol\",\" \".join(file_str))\n",
    "    if find:\n",
    "        for identifier in id_list[::stride]:\n",
    "            print(f'find . -name \\\"*{identifier}*rank_1*pdb\\\" | tee -a {receptor}_found.txt')\n",
    "\n",
    "sel2pymol('F2', '../all_pdbs/', True, stride=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ydim = 40\n",
    "fig = plt.figure(figsize=[8,ydim/3], facecolor='w')\n",
    "plt.imshow(docc, cmap='spring')\n",
    "#plt.yticks(ticks = range(ydim), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "plt.yticks(ticks = range(ydim), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "plt.xticks(ticks = range(n_anch), labels= allsse, rotation=90)\n",
    "\n",
    "all_anchor_averages # 10,21\n",
    "for y in range(ydim):\n",
    "    for x in range(n_anch):\n",
    "        if all_anchor_averages[y,x] > 0.0001:\n",
    "            plt.text(x,y, round(all_anchor_averages[y,x], 2), horizontalalignment='center', verticalalignment='center', fontsize=7,color='k', rotation=45)\n",
    "cbar = plt.colorbar(shrink=float(8/ydim))\n",
    "cbar.set_label('Relative Occupancy')\n",
    "plt.vlines(6.5,-0.5,ydim-0.5, color='k', linewidth=1.5)\n",
    "plt.savefig('identity_receptor_anchor_occupancy.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=[8,ydim/3], facecolor='w')\n",
    "#plt.imshow(df, cmap='summer')\n",
    "im_data = np.zeros(shape=(ydim, n_anch))\n",
    "\n",
    "print(all_anchor_averages.shape)\n",
    "#plt.yticks(ticks = range(10), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "plt.yticks(ticks = range(ydim), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "plt.xticks(ticks = range(n_anch), labels= allsse, rotation=90)\n",
    "#all_anchor_averages # 10,21\n",
    "for y in range(ydim):\n",
    "    for x in range(n_anch):\n",
    "        if all_anchor_averages[y,x] > 0.001:\n",
    "            im_data[y,x] = all_anchor_averages[y,x]\n",
    "        else:\n",
    "            plt.text(x,y,'x', horizontalalignment='center', verticalalignment='center', fontsize=20,color='k')\n",
    "            #patches.Rectangle((x,y), 1, 1, linewidth=0.5, edgecolor='k', facecolor='w')\n",
    "plt.imshow(im_data, cmap='summer', vmax=3)\n",
    "            #plt.text(x,y, round(all_anchor_averages[y,x], 2), horizontalalignment='center', verticalalignment='center', fontsize=7,color='k', rotation=45)\n",
    "cbar = plt.colorbar(shrink=float(8)/ydim)\n",
    "cbar.set_label(r'Closest Anchor Residue Distance [$\\AA$]')\n",
    "plt.vlines(6.5,-0.5,ydim-0.5, color='k', linewidth=1.5)\n",
    "plt.savefig('identity_receptor_anchor_distance.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydim = 40\n",
    "fig = plt.figure(figsize=[8,ydim/3], facecolor='w')\n",
    "\n",
    "#plt.yticks(ticks = range(ydim), labels= [f'ADGR{f}' for f in 'ABCDEFGLVX'])\n",
    "plt.yticks(ticks = range(ydim), labels= [f'{i[0]}:{i[1]}' for i in r_list])\n",
    "plt.xticks(ticks = range(n_anch), labels= allsse, rotation=90)\n",
    "occ_values = df.to_numpy()\n",
    "is_off = np.zeros(shape=(40,26))\n",
    "print(docc.shape, all_anchor_averages.shape)\n",
    "#all_anchor_averages # 10,21\n",
    "for y in range(ydim):\n",
    "    for x in range(n_anch):\n",
    "        if all_anchor_averages[y,x] is not None and all_anchor_averages[y,x] > 1.5 and occ_values[y,x] > 0.1:\n",
    "            is_off[y,x] = 1\n",
    "            plt.text(x,y, round(all_anchor_averages[y,x], 2), horizontalalignment='center', verticalalignment='center', fontsize=7,color='k', rotation=45)\n",
    "plt.imshow(is_off, cmap='spring')\n",
    "cbar = plt.colorbar(shrink=float(8/ydim))\n",
    "cbar.set_label('Relative Occupancy')\n",
    "plt.vlines(6.5,-0.5,ydim-0.5, color='k', linewidth=1.5)\n",
    "#plt.savefig('identity_receptor_anchor_occupancy.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_stats = np.zeros(shape = (n_anch, 2))\n",
    "\n",
    "for fam_idx, r in enumerate(receptors):# in enumerate('ABCDEFGLVX'):\n",
    "    print(r)\n",
    "    gain_subset = [ gain for i, gain in enumerate(valid_collection.collection) if fam_list[i] == r]#subfam_list[i]==r ]\n",
    "    gain_idx_list = [ i for i,gain in enumerate(fam_list) if gain == r ]\n",
    "    n_sse = [[len(gain.sda_helices), len(gain.sdb_sheets)] for gain in gain_subset] # (n_struc, 2)\n",
    "    n_strucs = np.mean(np.array(n_sse), axis=0)\n",
    "    print(r, round(n_strucs[0], 2), round(n_strucs[1],2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new Anchor management.\n",
    "\n",
    "#\n",
    "# Use the \"max\" SDA / SDB template for generating the new anchors \n",
    "# get the center residue index for each template SSE\n",
    "for gain in valid_collection.collection:\n",
    "    if gain.name[:10] == 'A0A7K7IHI9': #SDA\n",
    "        hel_centers = []\n",
    "        for hel in gain.sda_helices: # each hel is a tuple\n",
    "            hel_centers.append( gain.start + int((hel[0]+hel[1])/2) )\n",
    "        hel_keys = [f'H{i+1}' for i in range(len(hel_centers))]\n",
    "        sda_centers = dict(zip(hel_keys, hel_centers))\n",
    "    if gain.name[:10] =='A0A3P9I6M5':\n",
    "        sheet_centers = []\n",
    "        for sheet in gain.sdb_sheets: # each hel is a tuple\n",
    "            #if sheet[1] - sheet[0] < 3:\n",
    "            #    print(gain.start+sheet[0], gain.start+sheet[1])\n",
    "            sheet_centers.append( gain.start + int((sheet[0]+sheet[1])/2) )\n",
    "        sheet_keys = [f'S{i+1}' for i in range(len(sheet_centers))]\n",
    "        sdb_centers = dict(zip(sheet_keys, sheet_centers))\n",
    "# Manually curated the centers to exclude two small strands in the CD between S6/S7 @ 707-708 and 711-713, respectively (low pLDDT here).\n",
    "sda_centers = {'H1': 313, 'H2': 328, 'H3': 359, 'H4': 383, 'H5': 409, 'H6': 424, 'H7': 444}\n",
    "sdb_centers = {'S1': 622, 'S2': 631, 'S3': 645, 'S4': 658, 'S5': 670, 'S6': 695, 'S7': 719, 'S8': 736, 'S9': 752, 'S10': 765, 'S11': 771, 'S12': 782, 'S13': 793}\n",
    "\n",
    "# Find closest residue to the center (GESAMT), note down the sequence, start, end of the matched SSE; write to FASTA\n",
    "    # A dict of dicts --> for each key, there is a dictionary inside sse_seqs['H1'][gain.name]:'seqlist'\n",
    "all_keys = list({**sda_centers, **sdb_centers}.keys())\n",
    "sse_seqs = {k:{} for k in all_keys}\n",
    "sse_extents = {k:{} for k in all_keys}\n",
    "unmatched = {k:0 for k in all_keys}\n",
    "unstructured = {k:0 for k in all_keys}\n",
    "\n",
    "for i, gain in enumerate(valid_collection.collection):\n",
    "        a_gesamt_file = f'../sda_template_aligned_files/sda_{i}.out'\n",
    "        b_gesamt_file = f'../sdb_template_aligned_files/sdb_{i}.out'\n",
    "\n",
    "        sda_matches = find_anchor_matches(a_gesamt_file, sda_centers, isTarget=False)\n",
    "        sdb_matches = find_anchor_matches(b_gesamt_file, sdb_centers, isTarget=False)\n",
    "        #print(sda_matches, sdb_matches)\n",
    "        hel_extents = np.full(shape = (gain.end-gain.start+1), fill_value=100)\n",
    "        she_extents = np.full(shape = (gain.end-gain.start+1), fill_value=100)\n",
    "        # Establish two matrices to match the respective residue to the index of its helix/sheet for easier matching\n",
    "        for i,element in enumerate(gain.sda_helices):\n",
    "            hel_extents[element[0]:element[1]] = i\n",
    "        for i,element in enumerate(gain.sdb_sheets):\n",
    "            she_extents[element[0]:element[1]] = i\n",
    "        # Match the corresponding closest residue to find the associated SSE with start, end and sequence\n",
    "        for sse, match in sda_matches.items():\n",
    "            if match[0] is None:\n",
    "                unmatched[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_index = hel_extents[match[0]-gain.start]\n",
    "\n",
    "            if sse_index == 100:\n",
    "                unstructured[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_extents[sse][gain.name] = gain.sda_helices[sse_index]\n",
    "            sse_seqs[sse][gain.name] = gain.sequence[gain.sda_helices[sse_index][0]:gain.sda_helices[sse_index][1]]\n",
    "        \n",
    "        for sse, match in sdb_matches.items():\n",
    "            if match[0] is None:\n",
    "                unmatched[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_index = she_extents[match[0]-gain.start]\n",
    "\n",
    "            if sse_index == 100:\n",
    "                unstructured[sse] += 1\n",
    "                continue\n",
    "\n",
    "            sse_extents[sse][gain.name] = gain.sdb_sheets[sse_index]\n",
    "            sse_seqs[sse][gain.name] = gain.sequence[gain.sdb_sheets[sse_index][0]:gain.sdb_sheets[sse_index][1]]\n",
    "        \n",
    "for sse in all_keys:\n",
    "    with open(f'../sse_aln/{sse}.seqs.fa','w') as fa:\n",
    "        for name, seq in sse_seqs[sse].items():\n",
    "            fa.write(f'>{name}\\n{\"\".join(seq)}\\n')\n",
    "\n",
    "print(unmatched, '\\n', unstructured)\n",
    "#   Run MAFFT with each of the gathered sequences\n",
    "#   For each MAFFT\n",
    "#       Find the most conserved residue (Identity matrix)\n",
    "#       Set as new Anchor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for gain in valid_collection.collection:\n",
    "\n",
    "    if gain.name[:10] == 'A0A7K7IHI9': #SDA\n",
    "        sda_gain = gain\n",
    "    if gain.name[:10] =='A0A3P9I6M5': # SDB\n",
    "        sdb_gain = gain\n",
    "for i,k in enumerate(sda_centers.keys()):\n",
    "    kfile = glob.glob(f\"../sse_aln/{k}.aln.fa\")[0]\n",
    "    with open(kfile) as alnf:\n",
    "        x = alnf.readlines()[1].strip(\" \\n\")\n",
    "        kcutoff = len(x)\n",
    "    print(kcutoff)\n",
    "    aln = sse_func.read_alignment(kfile, cutoff=kcutoff)\n",
    "    h = sda_gain.sda_helices[i]\n",
    "    aln_matrix = np.array([list(seq) for seq in aln.values()])\n",
    "    kquality, kocc = calc_identity(aln_matrix)\n",
    "    '''    kquality = []\n",
    "    kocc = []\n",
    "    for col in range(aln_matrix.shape[1]):\n",
    "        chars, count = np.unique(aln_matrix[:,col], return_counts=True)\n",
    "        if chars[0] == '-':\n",
    "            q = count[1]\n",
    "        else:\n",
    "            q = count[0]\n",
    "        x = np.where(chars == '-')[0][0]\n",
    "        kocc.append(14435 - count[x])\n",
    "        kquality.append(q)'''\n",
    "    template_aln_seq = aln[sda_gain.name]\n",
    "    template_res_idx = np.argmax(kquality)\n",
    "    print(template_aln_seq, template_res_idx)\n",
    "    template_index = template_aln_seq[:template_res_idx+1]\n",
    "    t_res = template_aln_seq[template_res_idx]\n",
    "    print(template_index, t_res)\n",
    "    new = template_index\n",
    "\n",
    "    fig = plt.figure(figsize=[4,2], facecolor='w')\n",
    "    plt.bar(range(kcutoff), kquality)\n",
    "    plt.title(f'SDA TEMPLATE : {k}')\n",
    "    plt.xticks(ticks = range(kcutoff), labels=template_aln_seq, fontsize=5)\n",
    "    plt.savefig(f'../sse_aln/{k}.template1.png', dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "for i,k in enumerate(sdb_centers.keys()):\n",
    "    kfile = glob.glob(f\"../sse_aln/{k}.aln.fa\")[0]\n",
    "    with open(kfile) as alnf:\n",
    "        x = alnf.readlines()[1].strip(\" \\n\")\n",
    "        kcutoff = len(x)\n",
    "    print(kcutoff)\n",
    "    aln = sse_func.read_alignment(kfile, cutoff=kcutoff)\n",
    "    h = sdb_gain.sdb_sheets[i]\n",
    "    aln_matrix = np.array([list(seq) for seq in aln.values()])\n",
    "    kquality = []\n",
    "    kocc = []\n",
    "    kquality, kocc = calc_identity(aln_matrix)\n",
    "        \n",
    "    template_aln_seq = aln[sdb_gain.name]\n",
    "    template_res_idx = np.argmax(kquality)\n",
    "    print(template_aln_seq, template_res_idx)\n",
    "    template_index = template_aln_seq[:template_res_idx+1]\n",
    "    t_res = template_aln_seq[template_res_idx]\n",
    "    print(template_index, t_res)\n",
    "    new = template_index\n",
    "\n",
    "    fig = plt.figure(figsize=[4,2], facecolor='w')\n",
    "    plt.bar(range(kcutoff), kquality)\n",
    "    plt.title(f'SDB TEMPLATE : {k}')\n",
    "    plt.xticks(ticks = range(kcutoff), labels=template_aln_seq, fontsize=5)\n",
    "    plt.savefig(f'../sse_aln/{k}.template1.png', dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor2pdb(input_pdb, output_pdb, metric):\n",
    "    # line [61:66] = xx.xx b factor\n",
    "    # set to zero if not in metric, set to val otherwise\n",
    "    with open(input_pdb) as ipdb:\n",
    "        data = ipdb.readlines()\n",
    "    newdata = []\n",
    "    for l in data:\n",
    "        if not l.startswith(\"ATOM\"):\n",
    "            newdata.append(l)\n",
    "            continue\n",
    "        \n",
    "        resid = int(l[22:26])\n",
    "\n",
    "        if resid not in metric.keys():\n",
    "            l = l[:61]+\"00.00\"+l[66:]\n",
    "            newdata.append(l)\n",
    "            continue\n",
    "\n",
    "        l = l[:61]+f'{metric[resid]:5.2f}'+l[66:]\n",
    "        newdata.append(l)\n",
    "\n",
    "    with open(output_pdb, 'w') as opdb:\n",
    "        opdb.write(\"\".join(newdata))\n",
    "    \n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(template_anchors)\n",
    "template_ids = templates.keys()\n",
    "print(templates)\n",
    "target_folder = '../r2_f_templates/'\n",
    "outstr = []\n",
    "for t_id in template_ids:\n",
    "    print(t_id)\n",
    "    t_anchors = template_anchors[t_id]\n",
    "    t_pdb = find_pdb(templates[t_id][0], '../all_pdbs/')\n",
    "\n",
    "    t_metric = {v:1 for v in t_anchors.values()}\n",
    "    outpdb = target_folder+t_id+\".b.pdb\"\n",
    "\n",
    "    factor2pdb(t_pdb, outpdb, t_metric)\n",
    "    outstr.append(outpdb)\n",
    "\n",
    "print(\" \".join(outstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare this to the classical anchor representation using the CONSERVATION QUALITY from MAFFT\n",
    "\"\"\"alignment_file = \"/home/hildilab/projects/agpcr_nom/app_gain_gain.mafft.fa\"\n",
    "# This only contains the sigma files for truncated (?) PDBs.\n",
    "#quality = sse_func.read_quality(quality_file)\n",
    "gps_minus_one = 6781 \n",
    "aln_cutoff = 6826 \n",
    "alignment_dict = sse_func.read_alignment(alignment_file, aln_cutoff)\n",
    "aln_matrix = np.array([list(seq) for seq in alignment_dict.values()])\n",
    "#print(aln_matrix.shape)\n",
    "quality, occ = calc_identity(aln_matrix)\"\"\"\n",
    "precalc_anchors = [ 662, 1194, 1912, 2490, 2848, 3011, 3073, 3260, #H1-H8\n",
    "            3455, 3607, 3998, 4279, 4850, 5339, #5341 S1-S6, S7 REMOVED!\n",
    "            5413, 5813, 6337, 6659, 6696, 6765, 6808] #S8-13\n",
    "precalc_anchor_occupation = [ 4594.,  6539., 11392., 13658.,  8862., 5092.,  3228., 14189., #H1-H8\n",
    "                      9413., 12760.,  9420., 11201., 12283., 3676.,#  4562. S1-S6, S7 REMOVED!\n",
    "                     13992., 12575., 13999., 14051., 14353., 9760., 14215.] #S8-13\n",
    "precalc_anchor_dict = sse_func.make_anchor_dict(precalc_anchors, 3425)\n",
    "\n",
    "print(templates)\n",
    "target_folder = '../r2_f_templates/'\n",
    "outstr = []\n",
    "for t_id in template_ids:\n",
    "    print(t_id)\n",
    "    t_anchors = template_anchors[t_id]\n",
    "    t_pdb = find_pdb(templates[t_id][0], '../all_pdbs/')\n",
    "\n",
    "    # Find the template in the valid_collection\n",
    "    for gain in valid_collection.collection:\n",
    "        if gain.name.split(\"-\")[0] == templates[t_id][0].split(\"-\")[0]:\n",
    "            _,centers,_,_ = gain.create_indexing(precalc_anchors, precalc_anchor_occupation, precalc_anchor_dict, \n",
    "                                            outdir=None, offset=0, silent=True, split_mode='single',debug=False)\n",
    "            break\n",
    "    #print(centers)\n",
    "    \n",
    "    t_metric = {v+gain.start+1:1 for v in centers.values()}\n",
    "    outpdb = target_folder+t_id+\".p.pdb\"\n",
    "\n",
    "    factor2pdb(t_pdb, outpdb, t_metric)\n",
    "    outstr.append(outpdb)\n",
    "\n",
    "print(\" \".join(outstr))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "''' \n",
    "        Makes the indexing list, this is NOT automatically generated, since we do not need this for the base dataset\n",
    "        Prints out the final list and writes it to file if outdir is specified\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        anchors : list, required\n",
    "            List of anchors with each corresponding to an alignment index\n",
    "        anchor_occupation : list, required\n",
    "            List of Occupation values corresponding to each anchor for resolving ambiguity conflicts\n",
    "        anchor_dict : dict, required\n",
    "            Dictionary where each anchor index is assigned a name (H or S followed by a greek letter for enumeration)\n",
    "        offfset : int,  optional (default = 0)\n",
    "            An offsset to be incorporated (i.e. for offsetting model PDBs against UniProt entries)\n",
    "        silent : bool, optional (default = False)\n",
    "            opt in to run wihtout so much info.\n",
    "        outdir : str, optional\n",
    "            Output directory where the output TXT is going to be written as {self.name}.txt\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        indexing_dir : dict\n",
    "            A dictionary containing the residue indices for each assigned SSE and the GPS\n",
    "        indexing_centers : dict\n",
    "            A dictionary containing the XX.50 Residue for each assigned SSE\n",
    "        named_residue_dir : dict\n",
    "            A dictionary mapping individual consensus labels to their respective position.\n",
    "        unindexed : list\n",
    "            A list of detected SSE that remain unindexed.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efcc3436bf700bf51081b251413b556e30c22be82f452601745119c8a669a2f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
